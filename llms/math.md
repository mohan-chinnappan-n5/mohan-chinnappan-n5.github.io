
## Embeddings

- words --> numbers

## Math


### Attentions

- Context
```
Please buy an apple and an orange 
- we are talking about fruits
    - apple <---> orange
```



```
apple release m4 laptops

- we are talking about Brand apple
    - apple <---> laptop
```

#### Multi-head attention

![multi-head attention](img/mutli-head-attention-1.png)

- Math ops


### Similarity
![dot product](img/dot-product-1.png)
![cosine](img/cosine-1.png)
![cosine 2](img/cosine-2.png)
![dot cosine ](img/dot-cosine-1.png)
![Scaled dot product](img/scaled-dot-product-1.png)
![cosine-2](img/cs-1.png)

### Word Match
![word math](img/word-math-1.png)
![word math2](img/word-math-2.png)

### Normalize
![normalize](img/normalize-2.png)

### SoftMax
![Softmax](img/softmax-1.png)
![Softmax2](img/softmax-2.png)

### Key Query Value (KQV)
-    Finding similarities

![kqv-1](img/kqv-1.png)
![kqv-11](img/kqv-11.png)
![kqv-2](img/kqv-2.png)

![Transformed](img/simalirty-transformed-1.png)

### Value Matrix
- Finding next word
![Value mat](img/value-matrix-1.png)
![Value mat2](img/value-matrix-2.png)
![Value mat3](img/value-matrix-3.png)

### Single and multi-head
![Single](img/self-attention-11.png)
![Multihead](img/muliti-head-00.png)
![Multihead concat](img/multi-head-concat-1.png)

![Multihead](img/multi-head-11.png)

### Transformer Layers

- ![Next word predication](img/trans-next-word.png)
- ![Layers](img/transformer-laryers.png)

- Perceptron
![perceptron](img/perceptron-2.png)

- 3 outputs

![3 outputs](img/3outputs.png)

- Predict next word
![predict next word](img/predict-next-word-1.png)


## References

- [https://www.youtube.com/watch?v=UPtG_38Oq8o](The math behind Attention: Keys, Queries, and Values matrices)
- [LLM University!](https://docs.cohere.com/docs/llmu)
- [Stanford CS324](https://stanford-cs324.github.io/winter2022/lectures/introduction/)
