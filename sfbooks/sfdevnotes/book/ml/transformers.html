<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Transformers - Salesforce Development Notes</title>


        <!-- Custom HTML head -->

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="../custom.css">

        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="../intro.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="../dg/dg.html"><strong aria-hidden="true">2.</strong> Data Governance</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../dg/changeManagement.html"><strong aria-hidden="true">2.1.</strong> Change Management</a></li></ol></li><li class="chapter-item expanded "><a href="../dm/datamodel.html"><strong aria-hidden="true">3.</strong> Data Model</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../dm/fsc-dm.html"><strong aria-hidden="true">3.1.</strong> Financial Services Cloud</a></li><li class="chapter-item expanded "><a href="../dm/salescloud.html"><strong aria-hidden="true">3.2.</strong> Sales Cloud</a></li></ol></li><li class="chapter-item expanded "><a href="../tools/aboutTools.html"><strong aria-hidden="true">4.</strong> Tools</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../tools/tools-org.html"><strong aria-hidden="true">4.1.</strong> Org Tools</a></li><li class="chapter-item expanded "><a href="../tools/tools-data-bulkapi2.html"><strong aria-hidden="true">4.2.</strong> Data Loading BulkAPI2</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../tools/userloading.html"><strong aria-hidden="true">4.2.1.</strong> User Loading</a></li><li class="chapter-item expanded "><a href="../tools/product2loading.html"><strong aria-hidden="true">4.2.2.</strong> Product2 Loading and Update</a></li><li class="chapter-item expanded "><a href="../tools/flowinterview.html"><strong aria-hidden="true">4.2.3.</strong> FlowInterview Query and Deletion</a></li></ol></li><li class="chapter-item expanded "><a href="../tools/dataTreeImport.html"><strong aria-hidden="true">4.3.</strong> Prepare and Load for data:tree:import</a></li><li class="chapter-item expanded "><a href="../tools/tools-scpl.html"><strong aria-hidden="true">4.4.</strong> Loading State and Country Picklist</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../tools/tools-scpl-update.html"><strong aria-hidden="true">4.4.1.</strong> Updating State and Country Picklist </a></li></ol></li><li class="chapter-item expanded "><a href="../tools/project-management/gantt.html"><strong aria-hidden="true">4.5.</strong> Project Management</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../project-management/pert.html"><strong aria-hidden="true">4.5.1.</strong> PERT</a></li></ol></li><li class="chapter-item expanded "><a href="../tools/em.html"><strong aria-hidden="true">4.6.</strong> Event Monitoring</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../monitoring/proactive-monitoring.html"><strong aria-hidden="true">4.6.1.</strong> Proactive Monitoring</a></li><li class="chapter-item expanded "><a href="../monitoring/realtime-event-monitoring.html"><strong aria-hidden="true">4.6.2.</strong> Real-Time Event Monitoring</a></li><li class="chapter-item expanded "><a href="../tools/scripts.html"><strong aria-hidden="true">4.6.3.</strong> Auditing Scripts</a></li><li class="chapter-item expanded "><a href="../tools/compare.html"><strong aria-hidden="true">4.6.4.</strong> Comparison</a></li></ol></li><li class="chapter-item expanded "><a href="../tools/instance.html"><strong aria-hidden="true">4.7.</strong> Instance Related</a></li><li class="chapter-item expanded "><a href="../tools/oms.html"><strong aria-hidden="true">4.8.</strong> Org Metadata Snapshot</a></li></ol></li><li class="chapter-item expanded "><a href="../tools/scripts.html"><strong aria-hidden="true">5.</strong> Auditing Scripts</a></li><li class="chapter-item expanded "><a href="../custom-metadata-types.html"><strong aria-hidden="true">6.</strong> Custom Metadata Types</a></li><li class="chapter-item expanded "><a href="../tools/uuid.html"><strong aria-hidden="true">7.</strong> UUID</a></li><li class="chapter-item expanded "><a href="../cq/cq.html"><strong aria-hidden="true">8.</strong> Code Quality</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../cq/pmd.html"><strong aria-hidden="true">8.1.</strong> Code Scan PMD</a></li><li class="chapter-item expanded "><a href="../cq/comments.html"><strong aria-hidden="true">8.2.</strong> Comments Checking</a></li><li class="chapter-item expanded "><a href="../cq/test-coverage.html"><strong aria-hidden="true">8.3.</strong> Apex Code Test Coverage</a></li><li class="chapter-item expanded "><a href="../cq/lwc-linting.html"><strong aria-hidden="true">8.4.</strong> LWC Linting</a></li><li class="chapter-item expanded "><a href="../cq/eslint-custom-rules.html"><strong aria-hidden="true">8.5.</strong> eslint custom rules</a></li></ol></li><li class="chapter-item expanded "><a href="../deployments/deployments.html"><strong aria-hidden="true">9.</strong> Deployments</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../deployments/delta.html"><strong aria-hidden="true">9.1.</strong> Delta Deployments</a></li><li class="chapter-item expanded "><a href="../deployments/gha.html"><strong aria-hidden="true">9.2.</strong> Github Actions</a></li><li class="chapter-item expanded "><a href="../deployments/validate.html"><strong aria-hidden="true">9.3.</strong> Validating changes</a></li><li class="chapter-item expanded "><a href="../deployments/admin-changes.html"><strong aria-hidden="true">9.4.</strong> Taking care of Admin changes</a></li><li class="chapter-item expanded "><a href="../deployments/devOpsCenter.html"><strong aria-hidden="true">9.5.</strong> DevOps Center</a></li><li class="chapter-item expanded "><a href="../deployments/jenkins.html"><strong aria-hidden="true">9.6.</strong> Jenkins</a></li><li class="chapter-item expanded "><a href="../deployments/git.html"><strong aria-hidden="true">9.7.</strong> git</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../deployments/git-internals.html"><strong aria-hidden="true">9.7.1.</strong> git internals</a></li><li class="chapter-item expanded "><a href="../deployments/git-hooks.html"><strong aria-hidden="true">9.7.2.</strong> git hooks</a></li><li class="chapter-item expanded "><a href="../deployments/git-ops.html"><strong aria-hidden="true">9.7.3.</strong> git operations</a></li><li class="chapter-item expanded "><a href="../deployments/pr-template.html"><strong aria-hidden="true">9.7.4.</strong> pull request template</a></li><li class="chapter-item expanded "><a href="../deployments/git-diff.html"><strong aria-hidden="true">9.7.5.</strong> git diff</a></li><li class="chapter-item expanded "><a href="../deployments/git-merge.html"><strong aria-hidden="true">9.7.6.</strong> git merge</a></li><li class="chapter-item expanded "><a href="../deployments/git-log.html"><strong aria-hidden="true">9.7.7.</strong> git log</a></li><li class="chapter-item expanded "><a href="../deployments/git-stash.html"><strong aria-hidden="true">9.7.8.</strong> git stash</a></li><li class="chapter-item expanded "><a href="../deployments/git-config.html"><strong aria-hidden="true">9.7.9.</strong> git config</a></li><li class="chapter-item expanded "><a href="../deployments/git-revert.html"><strong aria-hidden="true">9.7.10.</strong> git revert</a></li><li class="chapter-item expanded "><a href="../deployments/git-delete-branch.html"><strong aria-hidden="true">9.7.11.</strong> git deleting branch</a></li></ol></li><li class="chapter-item expanded "><a href="../deployments/app-exchange.html"><strong aria-hidden="true">9.8.</strong> Apps from App Exchange </a></li><li class="chapter-item expanded "><a href="../deployments/sourceTracking.html"><strong aria-hidden="true">9.9.</strong> Source Tracking</a></li><li class="chapter-item expanded "><a href="../deployments/dangling.html"><strong aria-hidden="true">9.10.</strong> Dangling Fields Finder</a></li><li class="chapter-item expanded "><a href="../deployments/sbx.html"><strong aria-hidden="true">9.11.</strong> Sandbox Management</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../deployments/sandbox-preview-guide.html"><strong aria-hidden="true">9.11.1.</strong> Sandbox Preview Guide</a></li></ol></li><li class="chapter-item expanded "><a href="../deployments/MetadataComponentDependency.html"><strong aria-hidden="true">9.12.</strong> MetadataComponentDependency</a></li><li class="chapter-item expanded "><a href="../deployments/yaml.html"><strong aria-hidden="true">9.13.</strong> YAML</a></li><li class="chapter-item expanded "><a href="../deployments/packages.html"><strong aria-hidden="true">9.14.</strong> Installed Packages</a></li></ol></li><li class="chapter-item expanded "><a href="../crma/tools.html"><strong aria-hidden="true">10.</strong> CRMA tools</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../crma/load-export.html"><strong aria-hidden="true">10.1.</strong> Dataset Load and Export</a></li><li class="chapter-item expanded "><a href="../crma/analysis.html"><strong aria-hidden="true">10.2.</strong> Analysis</a></li><li class="chapter-item expanded "><a href="../crma/binding.html"><strong aria-hidden="true">10.3.</strong> Binding</a></li><li class="chapter-item expanded "><a href="../crma/deployment.html"><strong aria-hidden="true">10.4.</strong> Deployment</a></li></ol></li><li class="chapter-item expanded "><a href="../perf/perf.html"><strong aria-hidden="true">11.</strong> Performance</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../perf/lex-perf.html"><strong aria-hidden="true">11.1.</strong> Lex Performance</a></li><li class="chapter-item expanded "><a href="../perf/perf-testing.html"><strong aria-hidden="true">11.2.</strong> Performance testing</a></li><li class="chapter-item expanded "><a href="../perf/perf-pw.html"><strong aria-hidden="true">11.3.</strong> Testing with Playwright</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../e2e/playwright.html"><strong aria-hidden="true">11.3.1.</strong> More with Playwright</a></li></ol></li><li class="chapter-item expanded "><a href="../perf/edge.html"><strong aria-hidden="true">11.4.</strong> Salesforce Edge Network</a></li><li class="chapter-item expanded "><a href="../perf/lex-query.html"><strong aria-hidden="true">11.5.</strong> Lightning Experience Queries</a></li><li class="chapter-item expanded "><a href="../perf/rum.html"><strong aria-hidden="true">11.6.</strong> Real User Monitoring (RUM)</a></li></ol></li><li class="chapter-item expanded "><a href="../dwg/dwg-tools.html"><strong aria-hidden="true">12.</strong> Diagram tools</a></li><li class="chapter-item expanded "><a href="../xml/xml-tools.html"><strong aria-hidden="true">13.</strong> XML tools</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../xml/xlst-transformer.html"><strong aria-hidden="true">13.1.</strong> Transformer</a></li><li class="chapter-item expanded "><a href="../xml/xml-delta.html"><strong aria-hidden="true">13.2.</strong> XML Delta Util</a></li></ol></li><li class="chapter-item expanded "><a href="../dp/dp.html"><strong aria-hidden="true">14.</strong> Data Processing tools</a></li><li class="chapter-item expanded "><a href="../i18n/i18n.html"><strong aria-hidden="true">15.</strong> Translation Tools</a></li><li class="chapter-item expanded "><a href="../media/recording.html"><strong aria-hidden="true">16.</strong> Media - Recording tools</a></li><li class="chapter-item expanded "><a href="../media/image.html"><strong aria-hidden="true">17.</strong> Image Processing</a></li><li class="chapter-item expanded "><a href="../codegen/codegen.html"><strong aria-hidden="true">18.</strong> Code Generators</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../codegen/lwc.html"><strong aria-hidden="true">18.1.</strong> LWC Code Generators</a></li><li class="chapter-item expanded "><a href="../ml/nlp.html"><strong aria-hidden="true">18.2.</strong> Apex Code Generators</a></li></ol></li><li class="chapter-item expanded "><a href="../ml/ml.html"><strong aria-hidden="true">19.</strong> Machine Learning</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../ml/sdcg.html"><strong aria-hidden="true">19.1.</strong> Semantic Doc Compare and Group</a></li><li class="chapter-item expanded "><a href="../ml/datacatalog.html"><strong aria-hidden="true">19.2.</strong> Data Catalog for ML Work</a></li><li class="chapter-item expanded "><a href="../ml/chatgpt.html"><strong aria-hidden="true">19.3.</strong> openAI - ChatGPT</a></li><li class="chapter-item expanded "><a href="../ml/gpt3codex.html"><strong aria-hidden="true">19.4.</strong> openAI - GPT3 Codex</a></li><li class="chapter-item expanded "><a href="../ml/einsteinbot.html"><strong aria-hidden="true">19.5.</strong> Einstein Bot</a></li><li class="chapter-item expanded "><a href="../ml/dialogflow.html"><strong aria-hidden="true">19.6.</strong> Dialogflow - Knowledge Base</a></li><li class="chapter-item expanded "><a href="../ml/nlp.html"><strong aria-hidden="true">19.7.</strong> NLP</a></li><li class="chapter-item expanded "><a href="../ml/transformers.html" class="active"><strong aria-hidden="true">19.8.</strong> Transformers</a></li><li class="chapter-item expanded "><a href="../ml/llm.html"><strong aria-hidden="true">19.9.</strong> Large Language Models</a></li><li class="chapter-item expanded "><a href="../ml/team-productivity.html"><strong aria-hidden="true">19.10.</strong> Using LLM to improve team productivity</a></li></ol></li><li class="chapter-item expanded "><a href="../docmgmt/docmgmt.html"><strong aria-hidden="true">20.</strong> Document Management tools</a></li><li class="chapter-item expanded "><a href="../design/design-notes.html"><strong aria-hidden="true">21.</strong> Design Principles</a></li><li class="chapter-item expanded "><a href="../chromeext/ext.html"><strong aria-hidden="true">22.</strong> Chrome Extensions</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../chromeext/onnu.html"><strong aria-hidden="true">22.1.</strong> ONNU</a></li></ol></li><li class="chapter-item expanded "><a href="../vscode/ext.html"><strong aria-hidden="true">23.</strong> VS Code Extensions</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../vscode/sfland.html"><strong aria-hidden="true">23.1.</strong> SF Land</a></li><li class="chapter-item expanded "><a href="../vscode/pmd.html"><strong aria-hidden="true">23.2.</strong> VSCode Apex PMD</a></li><li class="chapter-item expanded "><a href="../vscode/vsc-tips.html"><strong aria-hidden="true">23.3.</strong> VSCode tips</a></li></ol></li><li class="chapter-item expanded "><a href="../dviz/dviz.html"><strong aria-hidden="true">24.</strong> Data Viz</a></li><li class="chapter-item expanded "><a href="../releases.html"><strong aria-hidden="true">25.</strong> Releases</a></li><li class="chapter-item expanded "><a href="../rest/rest_tester.html"><strong aria-hidden="true">26.</strong> REST Tester</a></li><li class="chapter-item expanded "><a href="../security/security.html"><strong aria-hidden="true">27.</strong> Security</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../security/exploit-prevention.html"><strong aria-hidden="true">27.1.</strong> Exploitation and prevention</a></li><li class="chapter-item expanded "><a href="../security/sd.html"><strong aria-hidden="true">27.2.</strong> Sharing Debugger</a></li><li class="chapter-item expanded "><a href="../security/sharing-recalc.html"><strong aria-hidden="true">27.3.</strong> Sharing Recalculation</a></li><li class="chapter-item expanded "><a href="../security/rh.html"><strong aria-hidden="true">27.4.</strong> Role Hierarchy</a></li><li class="chapter-item expanded "><a href="../security/shield.html"><strong aria-hidden="true">27.5.</strong> Shield</a></li><li class="chapter-item expanded "><a href="../security/profile-ps.html"><strong aria-hidden="true">27.6.</strong> Profile, PermissionSet</a></li><li class="chapter-item expanded "><a href="../security/setupAuditTrail.html"><strong aria-hidden="true">27.7.</strong> SetupAuditTrail</a></li><li class="chapter-item expanded "><a href="../security/datamask.html"><strong aria-hidden="true">27.8.</strong> Data Mask</a></li><li class="chapter-item expanded "><a href="../security/saml.html"><strong aria-hidden="true">27.9.</strong> SAML</a></li><li class="chapter-item expanded "><a href="../security/openIDConnectConcepts.html"><strong aria-hidden="true">27.10.</strong> OpenID Connect</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../security/openIDConnect.html"><strong aria-hidden="true">27.10.1.</strong> OIDC Google</a></li><li class="chapter-item expanded "><a href="../security/openIDConnectProvider.html"><strong aria-hidden="true">27.10.2.</strong> OIDC Provider</a></li><li class="chapter-item expanded "><a href="../security/jwt.html"><strong aria-hidden="true">27.10.3.</strong> JWT</a></li></ol></li><li class="chapter-item expanded "><a href="../security/openssl.html"><strong aria-hidden="true">27.11.</strong> OpenSSL</a></li><li class="chapter-item expanded "><a href="../security/pen-test.html"><strong aria-hidden="true">27.12.</strong> Penetration Testing</a></li></ol></li><li class="chapter-item expanded "><a href="../mobile/mobile.html"><strong aria-hidden="true">28.</strong> Mobile</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../mobile/ios-sim.html"><strong aria-hidden="true">28.1.</strong> Simulator</a></li></ol></li><li class="chapter-item expanded "><a href="../rd/rd.html"><strong aria-hidden="true">29.</strong> Reports and Dashboards</a></li><li class="chapter-item expanded "><a href="../customLabels/cl.html"><strong aria-hidden="true">30.</strong> Custom Labels</a></li><li class="chapter-item expanded "><a href="../accessibility/508.html"><strong aria-hidden="true">31.</strong> 508 - Accessibility</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../accessibility/sally.html"><strong aria-hidden="true">31.1.</strong> Sally</a></li></ol></li><li class="chapter-item expanded "><a href="../process-automation/processAutomation.html"><strong aria-hidden="true">32.</strong> Process Automation</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../process-automation/flow.html"><strong aria-hidden="true">32.1.</strong> Flow</a></li></ol></li><li class="chapter-item expanded "><a href="../great/folks.html"><strong aria-hidden="true">33.</strong> Great Folks</a></li><li class="chapter-item expanded "><a href="../editor/editor-tips.html"><strong aria-hidden="true">34.</strong> Editor tips</a></li><li class="chapter-item expanded "><a href="../bash/tips.html"><strong aria-hidden="true">35.</strong> Bash tips</a></li><li class="chapter-item expanded "><a href="../language-design/ld.html"><strong aria-hidden="true">36.</strong> Language Design</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../language-design/apex.html"><strong aria-hidden="true">36.1.</strong> Apex</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../language-design/apex-grammar.html"><strong aria-hidden="true">36.1.1.</strong> Grammar</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="../cal-integration/cal.html"><strong aria-hidden="true">37.</strong> Calendar Integrations</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../cal-integration/ical.html"><strong aria-hidden="true">37.1.</strong> iCal</a></li></ol></li><li class="chapter-item expanded "><a href="../tips.html"><strong aria-hidden="true">38.</strong> Tips</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Salesforce Development Notes</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="transformers"><a class="header" href="#transformers"><a href="https://arxiv.org/abs/1706.03762">Transformers</a></a></h1>
<pre><code>Model success depends on how fast you can build it and how good it is in reducing the losses!
</code></pre>
<ul>
<li>
<p>It is a general-purpose <a href="https://twitter.com/karpathy/status/1582807367988654081?lang=en">differentiable computer</a>. </p>
</li>
<li>
<p>A neural network architecture developed at Google that relies on self-attention mechanisms to transform a <strong>sequence of input embeddings</strong> into a <strong>sequence of output embeddings</strong> without relying on convolutions or recurrent neural networks. </p>
</li>
<li>
<p>A <a href="https://developers.google.com/machine-learning/glossary#Transformer">Transformer</a> can be viewed as a stack of self-attention layers.</p>
</li>
<li>
<p>Based on a self-attention mechanisms unlike traditional convolutional (CNN) or recurrent neural networks (RNN)</p>
</li>
<li>
<p>Transformer outperforms both recurrent and convolutional models in language translation benchmarks</p>
</li>
</ul>
<hr />
<h3 id="resilient"><a class="header" href="#resilient">Resilient</a></h3>
<p>Remarkably, the Transformer architecture published by <a href="https://arxiv.org/pdf/1706.03762.pdf">Google team</a> is very resilient over time. 
The Transformer that came out in <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">2017</a> is basically the same as today except 
that we reshuffle some of the layer normalizations. </p>
<p>Researchers are making training datasets much larger, evaluation much larger,
but they are keeping the architecture untouched, which is remarkable!</p>
<hr />
<p><img src="https://4.bp.blogspot.com/-ovHQGevt5Ks/WaiCfS0OPUI/AAAAAAAAB_U/nEqsh9fgecM1v98NAvGp8Zgr5BwBbOGBQCEwYBhgL/s640/image4.png" alt="Perf in lang modeling" /></p>
<ul>
<li>they aggregate information from <strong>surrounding words</strong> to determine the meaning of a given bit of language in context. </li>
</ul>
<hr />
<p><strong>A man/word is known by the company she/he/word keeps means that a person/words is similar to the people/words she/
he/word chooses to spend time with!</strong></p>
<p>Example: </p>
<ul>
<li>deciding on the most likely meaning and appropriate representation of the word <strong>bank</strong> in the sentence :</li>
</ul>
<p><strong>I arrived at the bank after crossing the…</strong> requires knowing if the sentence ends in <strong>“... road.” or “... river.”</strong></p>
<hr />
<table><thead><tr><th>Architecture</th><th>Notes</th></tr></thead><tbody>
<tr><td>RNN</td><td>Can determine that “bank” is likely to refer to the bank of a <strong>river</strong> after reading each word between “bank” and “river” step by step. <strong>More</strong> the steps decisions require, the <strong>harder</strong> it is for a RNN to learn how to make those decisions.The sequential nature of RNNs also makes it more difficult to fully take advantage of modern fast computing devices such as <a href="https://arxiv.org/abs/1704.04760">TPUs</a> and <a href="https://www.intel.com/content/www/us/en/products/docs/processors/what-is-a-gpu.html">GPUs</a>, which excel at parallel and not sequential processing.</td></tr>
<tr><td>CNN</td><td>Less sequential than RNNs, the number of steps required to combine information from distant parts of the input <strong>still grows with increasing distance</strong></td></tr>
<tr><td>Transformer</td><td>Performs a small, constant number of steps (chosen empirically). In each step, it applies a <strong>self-attention</strong> mechanism which directly models <strong>relationships between all words in a sentence</strong>, regardless of their respective position.</td></tr>
</tbody></table>
<hr />
<table><thead><tr><th>Feature</th><th>Comments</th></tr></thead><tbody>
<tr><td>Expressive</td><td>With forward pass. It nodes store <strong>vectors</strong> and these nodes look at each other and they can see what's important for their computation in the other nodes. Message-passing-like architecture is general (i.e. completeness) and powerful (i.e. efficiency), able to cover many real-world algorithms and in a small number of compute steps; an an empirical finding.</td></tr>
<tr><td>Optimizable</td><td>With <a href="https://developers.google.com/machine-learning/glossary#b">Back-Propagation</a> and <a href="https://developers.google.com/machine-learning/glossary#gradient_descent">Gradient Descent</a>. With  residual connections, layer normalization and <a href="https://developers.google.com/machine-learning/glossary#softmax">softmax</a> attention. Residual connections support a kind of ability to learn short algorithms (think low LOC) fast and first, then gradually extend them longer during training.</td></tr>
<tr><td>Efficient</td><td>High parallelism compute graph. Can efficiently run on modern hardware (GPUs). Compute graph is shallow and wide, mapping significantly better to our <a href="https://arxiv.org/abs/1511.08228">high-parallelism compute architectures</a></td></tr>
</tbody></table>
<hr />
<ul>
<li>
<p>It can become a single neural network architecture for multi-modal inputs (text, image, video, audio), and thus converging all the other architectures into one.</p>
</li>
<li>
<p>Transformers combine some of the benefits traditionally seen with convolutional neural networks (CNNs) and recurrent neural networks (RNNs).</p>
</li>
</ul>
<table><thead><tr><th>Architecture</th><th>Feature</th><th>Transformer</th></tr></thead><tbody>
<tr><td>CNN</td><td>learn structured representations of images</td><td>ability to process input in a <strong>hierarchical</strong> manner, with each layer of the model learning increasingly complex features</td></tr>
<tr><td>CNN</td><td>process input in a fixed-length window</td><td>use self-attention mechanisms that allow the model to directly relate different input elements to each other, regardless of their distance in the sequence. Capture long-range dependencies between input elements,</td></tr>
<tr><td>RNN</td><td>Cyclic. Ability to capture dependencies between elements in a <strong>sequence</strong></td><td></td></tr>
</tbody></table>
<hr />
<h2 id="model-architecture"><a class="header" href="#model-architecture">Model Architecture</a></h2>
<p><img src="img/transformer-1.png" alt="Transformer Model" /> </p>
<ul>
<li>The encoder and decoder are represented by <strong>six layers</strong> (the number can be any, for example, in BERT there are 24 encoder blocks)</li>
</ul>
<pre class="mermaid">graph LR;
Inputs--&gt;Embedding--&gt;SelfAttention--&gt;FeedForwardNN--&gt;Outputs
</pre>
<table><thead><tr><th>Steps</th><th>Description</th></tr></thead><tbody>
<tr><td>1</td><td>The input data of the encoder first passes through the Self-Attention layer. This layer helps the encoder look at other words in the input sentence when it encodes a particular word.</td></tr>
<tr><td>2</td><td>The results of this Self-Attention layer are fed to the FeedForward NN</td></tr>
<tr><td>3</td><td>As the model processes each token (each word in the input sequence, say <em>w</em>), the Self-Attention layer allows it to search for hints in the other tokens (words) in the input sequence that may help improve the <strong>encoding</strong> of that word (<em>w</em>).</td></tr>
<tr><td>4</td><td>FeedForward NN has <strong>no interaction with other words</strong>, and therefore <strong>various chains</strong> can be executed in parallel as they pass through this layer. This allows transformer to process all words in the input text in <strong>parallel</strong>.</td></tr>
</tbody></table>
<p>Model  constantly cares about what “she”, “it” , “the”, or “that” refers to (pronouns).</p>
<pre><code>I never been to Mars. But it is on wish list
</code></pre>
<p><strong>it</strong>  here refers to <strong>Mars</strong></p>
<h4 id="querykeyvalue-vectors----retrieval-system"><a class="header" href="#querykeyvalue-vectors----retrieval-system">Query/Key/Value vectors -  retrieval system</a></h4>
<p>\( \int x dx = \frac{x^2}{2} + C \)</p>
<pre class="mermaid">graph LR;

Query--&gt;Keys--&gt;ToGetValues

</pre>
<ul>
<li>we want to compute the self-attention for the word <strong>it</strong> in above sentence </li>
<li>creating matrices Q, K, and V for this input sentence
<ul>
<li>where q (row in matrix Q) represents the query vector for the word <strong>it</strong>
<ul>
<li>K represents the key   matrix for all the words in the sentence</li>
<li>V represents the value matrix for all the words in the sentence.</li>
</ul>
</li>
</ul>
</li>
<li>The self-attention for the word <strong>it</strong> would then be computed as:
<ul>
<li>the dot product of Q and K, divided by the square root of the length of K, followed by a dot product with V.</li>
</ul>
</li>
</ul>
<p>\( \frac {Q \cdot K}{\sqrt(dim(K))} \)</p>
<hr />
<p>The matrices  \( (W^Q, W^K, W^V) \) are initially randomly initialized weight matrices.</p>
<ul>
<li>The vector \( (x_n) \)in this case would be a word embedding of the input token (or output of the previous attention layer). </li>
</ul>
<h5 id="example"><a class="header" href="#example">Example</a></h5>
<pre><code>Hello World
</code></pre>
<ul>
<li>Embeddings
<ul>
<li>\( x_0 \) : Hello</li>
<li>\( x_1 \) : World</li>
</ul>
</li>
<li>We have vectors \( q, k,  v \) for each of these two words. </li>
<li>These vectors are generated by multiplying embedding vector of the word and the weight matrix \( W \)</li>
</ul>
<h3 id="encoder-and-decoder"><a class="header" href="#encoder-and-decoder">Encoder and Decoder</a></h3>
<p>An encoder transforms a sequence of <a href="">embeddings</a> into a new sequence of the same length. </p>
<ul>
<li>
<p>Includes N identical layers (Nx), each of which contains <strong>two sub-layers</strong>. </p>
<ul>
<li>These two sub-layers are applied at each position of the input embedding sequence, transforming each element of the sequence into a new embedding. 
<ul>
<li>The first encoder sub-layer <strong>aggregates</strong> information from across the input sequence.  - aggregation </li>
<li>The second encoder sub-layer <strong>transforms</strong> the aggregated information into an output embedding. - transformation</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Each encoder consists of two layers: Self-Attention and Feed Forward Neural Network.</p>
</li>
</ul>
<pre class="mermaid">graph LR;
  inputSequence --&gt; Aggregate --&gt; Transform --&gt; outputSequence
</pre>
<p>A decoder transforms a sequence of input embeddings into a sequence of output embeddings, possibly with a different length.</p>
<ul>
<li>Includes N identical layers with <strong>three sub-layers</strong></li>
<li>First and second of which are similar to the encoder sub-layers. </li>
<li>The third decoder sub-layer takes the output of the encoder and applies the self-attention mechanism to gather information from it</li>
</ul>
<pre class="mermaid">graph LR;
  inputSequence --&gt; Aggregate --&gt; Transform --&gt; ApplySelfAttention
</pre>
<h3 id="self-attention"><a class="header" href="#self-attention">Self-attention</a></h3>
<p>For this sentence:</p>
<pre><code> The animal didn't cross the street because it was too tired.  
 
</code></pre>
<ul>
<li>self-attention layer's attention pattern for the pronoun <strong>it</strong>:  (with the darkness of each line indicating how much each word contributes to the representation:)
<img src="https://developers.google.com/static/machine-learning/glossary/images/self-attention.png" alt="representation" /></li>
<li>The self-attention layer highlights words that are relevant to <strong>it</strong>. In this case, the attention layer has learned to highlight words that it might refer to, assigning the highest weight to animal.</li>
<li>For a sequence of n tokens, self-attention transforms a sequence of embeddings n separate times, once at each position in the sequence.</li>
</ul>
<p>In the  example:</p>
<pre><code>I arrived at the bank after crossing the river
</code></pre>
<ul>
<li>
<p>to determine that the word <strong>bank</strong> refers to the <em>shore of a river</em> and not a financial institution</p>
<ul>
<li>Transformer can learn to immediately attend to the word <strong>river</strong> and make this decision in a single step</li>
</ul>
</li>
<li>
<p>To compute the next representation for a given word - <strong>bank</strong> for example - the Transformer compares it to <strong>every other word</strong> in the sentence</p>
<ul>
<li>The result of these comparisons is an <strong>attention score</strong> for every other word in the sentence. </li>
<li>These <strong>attention scores</strong> determine how much each of the other words should <strong>contribute</strong> to the next representation of <strong>bank</strong>
<ul>
<li>The disambiguating <strong>river</strong> could receive a <strong>high attention score</strong> when computing a new representation for <strong>bank</strong> </li>
<li>The attention scores are then used as weights for a weighted average of all words’ representations which is fed into a fully-connected network to generate a new representation for <strong>bank</strong>, reflecting that the sentence is <strong>talking about a river bank</strong>.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://3.bp.blogspot.com/-aZ3zvPiCoXM/WaiKQO7KRnI/AAAAAAAAB_8/7a1CYjp40nUg4lKpW7covGZJQAySxlg8QCLcBGAs/s640/transform20fps.gif" alt="Demo" /></p>
<p>Neural networks for machine translation typically contain </p>
<ul>
<li>
<p>an <strong>encoder</strong> reading the input sentence and generating a representation of it</p>
</li>
<li>
<p>a decoder then generates the output sentence word by word while <strong>consulting</strong> the representation generated by the encoder</p>
</li>
<li>
<p>Transformer starts by generating initial representations, or embeddings, for each word. </p>
<ul>
<li>These are represented by the unfilled circles</li>
<li>Then, using self-attention, it aggregates information from all of the other words, generating a new representation per word informed by the entire context, represented by the filled balls.</li>
<li>This step is then repeated multiple times in <strong>parallel for all words</strong>, successively generating new representations.</li>
</ul>
</li>
<li>
<p>The <strong>decoder</strong> operates similarly, but generates one word at a time, from left to right. It attends not only to the other previously generated words, but also to the final representations generated by the encoder.</p>
</li>
<li>
<p>We can visualize what other parts of a sentence the network <strong>attends</strong> to when processing or translating a given word, thus gaining insights into how information travels through the network.</p>
</li>
</ul>
<p><img src="https://2.bp.blogspot.com/-2yoN-i1w6M4/WaiBaF3cEOI/AAAAAAAAB_I/okirZsIxxeQe9Rzh7cl3INMpdqgzvSPXQCLcBGAs/s400/Screen%2BShot%2B2017-08-31%2Bat%2B2.35.34%2BPM.png" alt="Example2" /></p>
<ul>
<li>In the first sentence pair <strong>it</strong> refers to the animal, and in the second to the street. </li>
<li>In one of its steps, the Transformer clearly identified the two nouns <strong>it</strong> could refer to and the respective amount of attention reflects its choice in the different contexts.</li>
</ul>
<p><img src="https://1.bp.blogspot.com/-AVGK0ApREtk/WaiAuzddKVI/AAAAAAAAB_A/WPV5ropBU-cxrcMpqJBFHg73K9NX4vywwCLcBGAs/s640/image2.png" alt="Contexts" /></p>
<p>Given this insight, it might not be that surprising that the Transformer also performs very well on the classic language analysis task of syntactic constituency parsing, a task the natural language processing community has attacked with highly specialized systems for decades.</p>
<ul>
<li><a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb">Sample work</a></li>
</ul>
<p><img src="img/transformer-1.webm.gif" alt="tensor2tensor" /></p>
<ul>
<li><a href="https://github.com/tensorflow/tensor2tensor/blob/master/README.md#walkthrough">Walkthrough</a></li>
<li><a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/Transformer_translate.ipynb">Notebook</a></li>
</ul>
<hr />
<h2 id="terms"><a class="header" href="#terms">Terms</a></h2>
<table><thead><tr><th>Term</th><th>Description</th></tr></thead><tbody>
<tr><td>Gradient descent</td><td>A mathematical technique to <strong>minimize loss</strong>. <a href="https://developers.google.com/machine-learning/glossary#gradient_descent">Gradient descent</a> iteratively adjusts <strong>weights and biases</strong>, gradually finding the best combination to minimize loss</td></tr>
<tr><td>Loss</td><td>The difference between the prediction and the label value is the loss</td></tr>
<tr><td>Forward Pass</td><td>System processes a batch of <strong>examples</strong> to yield prediction(s). The system compares each prediction to each label value. The difference between the prediction and the label value is the loss for that example. The system aggregates the losses for all the examples to compute the total loss for the current batch.</td></tr>
<tr><td>Backward pass (backpropagation)</td><td>System reduces loss by adjusting the weights of all the neurons in all the hidden layer(s).Backpropagation determines whether to increase or decrease the weights applied to particular neurons.</td></tr>
<tr><td><a href="https://developers.google.com/machine-learning/glossary#hidden_layer">Hidden layer</a></td><td>A layer in a neural network between the input layer (the features) and the output layer (the prediction). Each <img src="https://developers.google.com/static/machine-learning/glossary/images/HiddenLayerBigPicture.png" alt="hidden layer" /> consists of one or more neurons.</td></tr>
<tr><td>Learning Rate</td><td>Multiplier that controls the degree to which each backward pass increases or decreases each weight. A large learning rate will increase or decrease each weight more than a small learning rate</td></tr>
<tr><td><a href="https://en.wikipedia.org/wiki/Chain_rule">Chain rule</a></td><td>expresses the derivative of the composition of two differentiable functions f and g in terms of the derivatives of f and g</td></tr>
<tr><td><a href="https://en.wikipedia.org/wiki/Notation_for_differentiation#Lagrange&#x27;s_notation">Lagrange's notation</a></td><td>a prime mark denotes a derivative. If f is a function, then its derivative evaluated at x is written f`(x)</td></tr>
<tr><td><a href="https://developers.google.com/machine-learning/glossary#e">Encode</a></td><td>In sequence-to-sequence tasks, encoder takes an input sequence and returns an internal state (a vector). Then, the decoder uses that internal state to predict the next sequence.</td></tr>
<tr><td>Tensor</td><td>N-dimensional (where N could be very large) data structures, most commonly scalars, vectors, or matrices. The elements of a Tensor can hold integer, floating-point, or string values</td></tr>
<tr><td>language understanding tasks</td><td>language modeling, machine translation and question answering</td></tr>
<tr><td>Attention</td><td>Weighted <strong>sum over a set of inputs</strong>, where the weight for each input is computed by another part of the neural network</td></tr>
<tr><td>self-attention</td><td>Each embedding in the output sequence is constructed by integrating information from the elements of the input sequence through an attention mechanism.The <strong>self</strong> part of <strong>self-attention</strong> refers to the <strong>sequence attending to itself</strong> rather than to some other context. A self-attention layer starts with a sequence of input representations, one for each word. For each word in an input sequence, the network scores the relevance of the word to every element in the whole sequence of words. The relevance scores determine how much the word's final representation incorporates the representations of other words.Self-attention is one of the main building blocks for Transformers and uses dictionary lookup terminology, such as “query”, “key”, and “value”. Self-attention and multi-head self-attention, which are the building blocks of Transformers.</td></tr>
<tr><td>Masked language modelling</td><td>Encoder only architectures (like BERT) solve the task of predicting the masked word in a sentence. So the attention can see all the words before and after this masked word</td></tr>
<tr><td>Predict next token</td><td>Predict the next token or set of tokens, i.e. given tokens [0, ..., n-1] predict n</td></tr>
</tbody></table>
<h2 id="sentence-completion-task"><a class="header" href="#sentence-completion-task">Sentence completion task</a></h2>
<ul>
<li>This requires decoder portion of the transformer architecture</li>
<li>For the translation tasks we need to <strong>encode</strong> the input text first 
<ul>
<li>have a cross-attention layer with the translated text</li>
</ul>
</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../ml/nlp.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next" href="../ml/llm.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../ml/nlp.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next" href="../ml/llm.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script type="text/javascript">
            var socket = new WebSocket("ws://localhost:3000/__livereload");
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>



        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript" src="../mermaid.min.js"></script>
        <script type="text/javascript" src="../mermaid-init.js"></script>


    </body>
</html>
