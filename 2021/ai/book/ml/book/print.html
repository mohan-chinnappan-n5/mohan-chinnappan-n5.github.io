<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Machine Learning for Everyone!</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="chapter_1.html"><strong aria-hidden="true">1.</strong> Quick Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="sup.html"><strong aria-hidden="true">1.1.</strong> Supervised Learning</a></li><li class="chapter-item expanded "><a href="unsup.html"><strong aria-hidden="true">1.2.</strong> Unsupervised Learning</a></li></ol></li><li class="chapter-item expanded "><a href="chapter_2.html"><strong aria-hidden="true">2.</strong> Basic Linear Algebra needed</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="eigen.html"><strong aria-hidden="true">2.1.</strong> Eigenvectors </a></li><li class="chapter-item expanded "><a href="chapter_2-ref.html"><strong aria-hidden="true">2.2.</strong> References</a></li></ol></li><li class="chapter-item expanded "><a href="ml_models.html"><strong aria-hidden="true">3.</strong> ML Models</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="word2vec.html"><strong aria-hidden="true">3.1.</strong> Word2Vec</a></li><li class="chapter-item expanded "><a href="glove.html"><strong aria-hidden="true">3.2.</strong> GloVe</a></li></ol></li><li class="chapter-item expanded "><a href="dl.html"><strong aria-hidden="true">4.</strong> Deep Learning</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="nn.html"><strong aria-hidden="true">4.1.</strong> Neural Network</a></li><li class="chapter-item expanded "><a href="grad_descent.html"><strong aria-hidden="true">4.2.</strong> Gradient descent</a></li><li class="chapter-item expanded "><a href="back_propagation.html"><strong aria-hidden="true">4.3.</strong> Back Propagation</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="bp_calc.html"><strong aria-hidden="true">4.3.1.</strong> Calculus</a></li></ol></li><li class="chapter-item expanded "><a href="act_func.html"><strong aria-hidden="true">4.4.</strong> Activation Function</a></li><li class="chapter-item expanded "><a href="cnn.html"><strong aria-hidden="true">4.5.</strong> Convolutional Neural Networks</a></li><li class="chapter-item expanded "><a href="rnn.html"><strong aria-hidden="true">4.6.</strong> Recurrent Neural Networks</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="mnist.html"><strong aria-hidden="true">4.6.1.</strong> MNIST</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="tf.html"><strong aria-hidden="true">5.</strong> Tensorflow</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="tfjs.html"><strong aria-hidden="true">5.1.</strong> TensorFlow.js</a></li></ol></li><li class="chapter-item expanded "><a href="torch.html"><strong aria-hidden="true">6.</strong> PyTorch</a></li><li class="chapter-item expanded "><a href="chapter_7.html"><strong aria-hidden="true">7.</strong> Transformers</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="bert.html"><strong aria-hidden="true">7.1.</strong> BERT</a></li><li class="chapter-item expanded "><a href="GPT.html"><strong aria-hidden="true">7.2.</strong> GPT</a></li><li class="chapter-item expanded "><a href="T5.html"><strong aria-hidden="true">7.3.</strong> T5</a></li><li class="chapter-item expanded "><a href="copilot.html"><strong aria-hidden="true">7.4.</strong> GitHub Copilot</a></li><li class="chapter-item expanded "><a href="chapter_7_ref.html"><strong aria-hidden="true">7.5.</strong> References</a></li></ol></li><li class="chapter-item expanded "><a href="sf_e.html"><strong aria-hidden="true">8.</strong> Salesforce Einstein</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="sf_ml.html"><strong aria-hidden="true">8.1.</strong> Machine Learning</a></li><li class="chapter-item expanded "><a href="sf_nlp.html"><strong aria-hidden="true">8.2.</strong> Natural Language Processing</a></li><li class="chapter-item expanded "><a href="sf_cv.html"><strong aria-hidden="true">8.3.</strong> Computer Vision</a></li></ol></li><li class="chapter-item expanded "><a href="gcp.html"><strong aria-hidden="true">9.</strong> Google Cloud Platform</a></li><li class="chapter-item expanded "><a href="pu.html"><strong aria-hidden="true">10.</strong> Processing Units</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="cpu.html"><strong aria-hidden="true">10.1.</strong> CPU</a></li><li class="chapter-item expanded "><a href="gpu.html"><strong aria-hidden="true">10.2.</strong> GPU</a></li><li class="chapter-item expanded "><a href="tpu.html"><strong aria-hidden="true">10.3.</strong> TPU</a></li></ol></li><li class="chapter-item expanded "><a href="ml_pl.html"><strong aria-hidden="true">11.</strong> ML Pipelines</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="ml-ops.html"><strong aria-hidden="true">11.1.</strong> ML ops</a></li><li class="chapter-item expanded "><a href="tfs.html"><strong aria-hidden="true">11.2.</strong> TensorFlow Serving</a></li><li class="chapter-item expanded "><a href="tfx.html"><strong aria-hidden="true">11.3.</strong> TensorFlow Extended</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="apache_airflow.html"><strong aria-hidden="true">11.3.1.</strong> Apache Airflow</a></li><li class="chapter-item expanded "><a href="apache_beam.html"><strong aria-hidden="true">11.3.2.</strong> Apache Beam</a></li><li class="chapter-item expanded "><a href="kubeflow.html"><strong aria-hidden="true">11.3.3.</strong> Kubeflow</a></li></ol></li><li class="chapter-item expanded "><a href="automl.html"><strong aria-hidden="true">11.4.</strong> AutoML</a></li><li class="chapter-item expanded "><a href="kubernetes.html"><strong aria-hidden="true">11.5.</strong> Kubernetes</a></li></ol></li><li class="chapter-item expanded "><a href="speedup.html"><strong aria-hidden="true">12.</strong> Speedup</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="jax.html"><strong aria-hidden="true">12.1.</strong> JAX</a></li><li class="chapter-item expanded "><a href="closure.html"><strong aria-hidden="true">12.2.</strong> Closures and Decorators</a></li><li class="chapter-item expanded "><a href="jax_ref.html"><strong aria-hidden="true">12.3.</strong> References</a></li></ol></li><li class="chapter-item expanded "><a href="openai.html"><strong aria-hidden="true">13.</strong> OpenAI</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="openai-api.html"><strong aria-hidden="true">13.1.</strong> API</a></li><li class="chapter-item expanded "><a href="openai-chat.html"><strong aria-hidden="true">13.2.</strong> Chat</a></li><li class="chapter-item expanded "><a href="openai-summarize.html"><strong aria-hidden="true">13.3.</strong> Summarize</a></li><li class="chapter-item expanded "><a href="openai-tldr.html"><strong aria-hidden="true">13.4.</strong> TLDR</a></li><li class="chapter-item expanded "><a href="open-translate.html"><strong aria-hidden="true">13.5.</strong> Translate</a></li><li class="chapter-item expanded "><a href="openai-codex.html"><strong aria-hidden="true">13.6.</strong> Codex</a></li></ol></li><li class="chapter-item expanded "><a href="inspire.html"><strong aria-hidden="true">14.</strong> Inspirations</a></li><li class="chapter-item expanded "><a href="datasets.html"><strong aria-hidden="true">15.</strong> Datasets</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="datasets/boston.html"><strong aria-hidden="true">15.1.</strong> Boston Housing</a></li></ol></li><li class="chapter-item expanded "><a href="industry/ml-industry.html"><strong aria-hidden="true">16.</strong> Building ML for Industries</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="industry/lfitem-magmt.html"><strong aria-hidden="true">16.1.</strong> Lost-Found Item Management</a></li></ol></li><li class="chapter-item expanded "><a href="hardware/hardware.html"><strong aria-hidden="true">17.</strong> Hardware</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="hardware/raspi.html"><strong aria-hidden="true">17.1.</strong> Raspberry Pi</a></li></ol></li><li class="chapter-item expanded "><a href="conversational-ai.html"><strong aria-hidden="true">18.</strong> Conversational-AI</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="chatbots.html"><strong aria-hidden="true">18.1.</strong> Chatbots</a></li><li class="chapter-item expanded "><a href="ebot.html"><strong aria-hidden="true">18.2.</strong> Einstein Bots</a></li><li class="chapter-item expanded "><a href="DeepPavlov.html"><strong aria-hidden="true">18.3.</strong> DeepPavlov</a></li><li class="chapter-item expanded "><a href="dialogflow.html"><strong aria-hidden="true">18.4.</strong> Dialogflow</a></li><li class="chapter-item expanded "><a href="rasa.html"><strong aria-hidden="true">18.5.</strong> Rasa</a></li></ol></li><li class="chapter-item expanded "><a href="transformers.html"><strong aria-hidden="true">19.</strong> Transformers</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="transformers/gpt.html"><strong aria-hidden="true">19.1.</strong> GPT</a></li><li class="chapter-item expanded "><a href="transformers/build-gpt.html"><strong aria-hidden="true">19.2.</strong> Building GPT</a></li></ol></li><li class="chapter-item expanded "><a href="tools.html"><strong aria-hidden="true">20.</strong> Tools</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="iac.html"><strong aria-hidden="true">20.1.</strong> Infrastructure as code</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Machine Learning for Everyone!</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="chapter-1--quick-introduction"><a class="header" href="#chapter-1--quick-introduction">Chapter 1 : Quick Introduction</a></h1>
<blockquote>
<p>Select few pilots (say 2) in parallel, not in mission-critical areas but in smaller ones, in your specific industry, where a win is likely in a few months time, and where <em>successes will win over
skeptics</em>.  - <strong>Andrew Ng</strong> in How to Choose Your First AI Project</p>
</blockquote>
<p><img src="industry/img/valley.jpg" alt="Valley" />
<sup>Image credit: Chromecast</sup></p>
<ul>
<li>In traditional programming we start with <strong>data</strong> and <strong>hard-coded rules</strong> to apply on the data to get <strong>answers</strong>. </li>
<li>This style of programming can't bring  answers <strong>easily</strong> for problems like:
<ul>
<li>predicting a <strong>type of a cat</strong> in the given animal picture</li>
</ul>
</li>
</ul>
<p>Assume you need to write a program find out the given animal is cat or dog. Traditional way will be something like this:</p>
<pre><code class="language-py">def detect_colors(image):
# lots of code

def detect_edges(image):
# lots of code

def analyze_shapes(image):
# lots of code

def guess_texture(image):
# lots of code

def define_animal():
# lots of code

def handle_probability():
# lots of code
</code></pre>
<p>So, we will be writing lot of <strong>hard-corded</strong> rules!</p>
<p>It would be great if write a algorithm (say <strong>Classifier</strong>) which can figure out rules for us based on the data we provided (<strong>data-driven rules</strong>), so we <strong>do not have</strong> to write those rules by hand.</p>
<p>So it is trained on the Data and rules are written based on the provided data</p>
<p>So when we provide:</p>
<ul>
<li>
<p>Input: Data (say cat's image) : <em>cat image</em></p>
</li>
<li>
<p>The program takes in the given the cat image</p>
</li>
<li>
<p>Output: Predicted animal name - <em>Persian Cat with probability 89.178%</em></p>
</li>
</ul>
<p>As shown in the demo below, the user provides a image of the cat, the application predicts type of the cat in that image with a confidence (probability) with help of a Machine Learning Model. </p>
<p>Cool?</p>
<p><img src="img/1/img-rec-1.mov.webm.gif" alt="Image Recognition - 1 " /></p>
<p>To build this kind of solution using traditional programming, we may have to write too many rules or sometimes this problem is not easily solvable by our traditional programming. Here comes our hero <strong>Machine Learning</strong> to our rescue us!</p>
<h2 id="what-is-special-about-machine-learning-"><a class="header" href="#what-is-special-about-machine-learning-">What is special about Machine Learning ?</a></h2>
<p>How long it will take to write the code based on hard-coded rules for this task:</p>
<ul>
<li><strong>Solving Rubik’s Cube with a single Robot Hand</strong> using our traditional programming? </li>
</ul>
<iframe width="720" height="480" src="https://www.youtube.com/embed/kVmp0uGtShk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<h2 id="steps-in-the-ml"><a class="header" href="#steps-in-the-ml">Steps in the ML</a></h2>
<p><strong>Goal:  Create an accurate Model that answers our questions most of time</strong></p>
<h3 id="step-1---gathering-data"><a class="header" href="#step-1---gathering-data">Step-1 - Gathering Data</a></h3>
<ul>
<li>To train a ML Model we need to:
<ul>
<li>Collect data to train on</li>
</ul>
</li>
</ul>
<h3 id="step-2---data-preparation"><a class="header" href="#step-2---data-preparation">Step-2 - Data Preparation</a></h3>
<ul>
<li>Load the data and visualize it</li>
<li>Check for data errors and data imbalances</li>
<li>Split the data into 2 parts
<ul>
<li>
<ol>
<li>Training Data (80%)</li>
</ol>
</li>
<li>
<ol start="2">
<li>Testing Data (20%)</li>
</ol>
</li>
</ul>
</li>
</ul>
<h3 id="step-3---choosing-a-model"><a class="header" href="#step-3---choosing-a-model">Step-3 - Choosing a Model</a></h3>
<p>In our case, we can use a linear model.</p>
<h3 id="step-4---training-the-model"><a class="header" href="#step-4---training-the-model">Step-4 - Training the Model</a></h3>
<p>Model </p>
<ul>
<li>
<p>\(y = mx + b\)</p>
</li>
<li>
<p>\(x\) is the input</p>
</li>
<li>
<p>\(y\) is the output (prediction)</p>
</li>
</ul>
<p>The values we are to going to adjust the training are:</p>
<ul>
<li>
<p>\(m\) (weight) and \(b\) (bias)</p>
</li>
<li>
<p>Start the training by initializing  \(m\) (weight) and \(b\) (bias) with some random values</p>
</li>
<li>
<p>At the beginning, the Model will perform very poorly</p>
<ul>
<li>We compare the model's output \(y\) with what it should have produced (target value of y)</li>
<li>We will adjust values of \(m\) (weight) and \(b\) (bias) so that we get more accurate predictions on the next time around</li>
<li>This error correction repeats...
<ul>
<li>Each iteration updates \(m\) (weight) and \(b\) (bias) - called <em>one training step</em></li>
</ul>
</li>
<li>We will stop the training once we got the good accuracy (low error)</li>
</ul>
</li>
</ul>
<h3 id="step-5---evaluating-the-model"><a class="header" href="#step-5---evaluating-the-model">Step-5 - Evaluating the Model</a></h3>
<ul>
<li>We can check the fitness of our Model using Evaluation</li>
<li>We test our Model against the <strong>Testing Data</strong> we created in Step-2
<ul>
<li>We are testing the model against data the Model has not seen yet (simulating the real-world situation)</li>
</ul>
</li>
</ul>
<h3 id="step-6---parameter-tuning"><a class="header" href="#step-6---parameter-tuning">Step-6 - Parameter Tuning</a></h3>
<p>Parameters (AKA hyper-parameters) we can tune:</p>
<ul>
<li>How many times we run through the training dataset?</li>
<li>Learning Rate
<ul>
<li>How far we did the error correction based on the information from the previous training step</li>
</ul>
</li>
</ul>
<p>These parameters determine:</p>
<ul>
<li>Accuracy of our Model</li>
<li>How long it takes to train the Model</li>
</ul>
<p>How we initialized the Model affects the Model training time
- Random values or
- Zero values</p>
<h3 id="step-7---predicationinference"><a class="header" href="#step-7---predicationinference">Step-7 - Predication/Inference</a></h3>
<p>We can use our Model to predict the values for the given input.
Power here is we can predict the values for the given input with our Model
- not by human judgement and manual rules</p>
<h2 id="videos"><a class="header" href="#videos">Videos</a></h2>
<iframe width="720" height="480" src="https://www.youtube.com/embed/cKxRvEZd3Mw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="780" height="420" src="https://www.youtube.com/embed/nKW8Ndu7Mjw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="780" height="420" src="https://www.youtube.com/embed/h0e2HAPTGF4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<h2 id="references"><a class="header" href="#references">References</a></h2>
<ul>
<li><a href="https://hbr.org/2019/02/how-to-choose-your-first-ai-project">How to Choose Your First AI Project </a></li>
<li><a href="https://drive.google.com/file/d/13j4E5pDyygeK7f9cDz4VmGtuxQ2YN1Cc/view">A Very Quick Primer for Curious Executives</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="supervised-learning"><a class="header" href="#supervised-learning">Supervised Learning</a></h1>
<ul>
<li>Given a set of feature/label pairs</li>
<li>Find a model predicts the label associated with a previously unseen input</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="unsupervised-learning"><a class="header" href="#unsupervised-learning">Unsupervised Learning</a></h1>
<ul>
<li>Given a set of feature vectors (without labels)</li>
<li>Group them into <strong>natural clusters</strong> or create labels for groups</li>
</ul>
<p>Here are some data on the New England Patriots and let us see how we can use clustering to create groups.</p>
<pre><code>

Features:
 Name, height, weight
 
 Labeled by type of position

Receivers: (label)
    edelman = ['edelman’, 70, 200]  &lt;---- Feature vector
    hogan = ['hogan', 73, 210]
    gronkowski = ['gronkowski', 78, 265]
    amendola = ['amendola’, 71, 190]
    bennett = ['bennett’, 78, 275]

Linemen: (label)
    cannon = ['cannon’, 77, 335]
    solder = ['solder', 80, 325]
    mason = ['mason’, 73, 310]
    thuney = ['thuney', 77, 305]
    karras = ['karras', 76, 305]

</code></pre>
<pre><code class="language-py">
# r: receiver, l: linemen
ne_fb_players = [[&quot;edelman&quot;, 70, 200, &quot;r&quot;],
                 [&quot;hogan&quot;, 73, 210, &quot;r&quot;],  
                 [&quot;gronkowski&quot;, 78, 265, &quot;r&quot;], 
                 [&quot;amendola&quot;, 71, 190, &quot;r&quot;], 
                 [&quot;bennett&quot;, 78, 275, &quot;r&quot;],

                 [&quot;cannon&quot;, 77, 335, &quot;l&quot;],
                 [&quot;solder&quot;, 80, 325, &quot;l&quot;],
                 [&quot;mason&quot;, 73, 310, &quot;l&quot;],
                 [&quot;thuney&quot;, 77, 305, &quot;l&quot;],
                 [&quot;karras&quot;, 76, 305, &quot;l&quot;]
                 
                 ]

import numpy as np
import matplotlib.pyplot as plt
nep_dataset = np.array(ne_fb_players)


plt.scatter( nep_dataset[:, 1], nep_dataset[:, 2])

plt.xlabel(&quot;Height&quot;)
plt.ylabel(&quot;Weight&quot;)

plt.grid()
plt.show()




</code></pre>
<p><img src="img/1/nep_data-1.png" alt="nep dataset plot" /></p>
<pre><code class="language-py">
X = nep_dataset[:, 1:3]
print (X)
kmeans = KMeans(n_clusters=2, random_state=0).fit(X)
kmeans.labels_

</code></pre>
<pre><code>
[['70' '200']
 ['73' '210']
 ['78' '265']
 ['71' '190']
 ['78' '275']
 ['77' '335']
 ['80' '325']
 ['73' '310']
 ['77' '305']
 ['76' '305']]
array([1, 1, 0, 1, 0, 0, 0, 0, 0, 0], dtype=int32)

</code></pre>
<p>As we the first 2 items and 4th item are in one cluster while all others in the second cluster</p>
<pre><code class="language-py">plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')

# colormap viridis: https://matplotlib.org/stable/tutorials/colors/colormaps.html
 
plt.grid()
plt.show()

</code></pre>
<p><img src="img/1/kmeans-nep-1.png" alt="KMeans with 2 clusters" /></p>
<p>If we want to group them into 3 clusters, we need to provide n_clusters=3 as shown below:</p>
<pre><code>kmeans = KMeans(n_clusters=3, random_state=0).fit(X)

</code></pre>
<p><img src="img/1/kmeans-nep-2.png" alt="KMeans with 2 clusters" /></p>
<h3 id="k-means"><a class="header" href="#k-means">K-Means</a></h3>
<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.11-K-Means.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory"> - Introducing k-Means</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-2---basic-linear-algebra-needed-for-ml"><a class="header" href="#chapter-2---basic-linear-algebra-needed-for-ml">Chapter 2 - Basic Linear Algebra needed for ML</a></h1>
<blockquote>
<p>DO NOT WORRY ABOUT YOUR DIFFICULTIES IN MATHEMATICS. I CAN ASSURE YOU MINE ARE STILL GREATER - Alert Einstein</p>
</blockquote>
<blockquote>
<p>“Last time I asked: 'What does mathematics mean to you?', and some people answered: &quot;The manipulation of numbers, the manipulation of structures.' And if I had asked what music means to you, would you have answered: 'The manipulation of notes?”
― <a href="https://en.wikipedia.org/wiki/Serge_Lang">Serge Lang</a>, The Beauty of Doing Mathematics </p>
</blockquote>
<p>As we know computer are comfortable in dealing with numbers and perform fast operations on those numbers to provide us the results we are interested in. But in our real world we deal with <strong>things</strong> like <em>words, sentences and images</em>. This creates an <em>impedance mismatch</em>. So obvious solution will be to solve this mismatch is by representing our <strong>things</strong> in numbers, let us call this as <strong>Data Representations</strong>.</p>
<p><strong>Linear Algebra</strong> what we learned in our high school math class comes to save us here!</p>
<p>In this chapter, we will have a friendly introduction to <strong>Linear Algebra</strong>. </p>
<p>If you did not have a chance to learn <strong>Linear Algebra</strong> in your high school, do not worry, I will try to explain in a <em>simplest possible way</em> to understand the <em>Data Representations</em> concepts so we can do Machine Learning work.</p>
<hr />
<p><em>&quot;Simplicity is the ultimate sophistication&quot;</em> - <a href="https://en.wikipedia.org/wiki/Leonardo_da_Vinci">Leonardo da Vinci</a></p>
<hr />
<h2 id="let-us-use-vectors-for-data-representations"><a class="header" href="#let-us-use-vectors-for-data-representations">Let us use Vectors for Data Representations</a></h2>
<p>Ok, what is a <strong>Vector</strong>?</p>
<ul>
<li>Vector is one Dimensional Array of numbers</li>
<li>It has magnitude (value) and direction</li>
</ul>
<p><img src="img/2/vector.png" alt="vector" /></p>
<ul>
<li>Example vector with 3 entries</li>
</ul>
<pre><code>v1 = [1, 2, 3]

</code></pre>
<p>When we say <em>n-dimensional vector space</em> we mean this space consists of all vector with <em>n</em> entries.
In our vector with 3 entries, <em>3-dimensional vector space</em> will consist of all the vectors with <em>3</em> entries.</p>
<p>Another name vector space is <strong>feature</strong>, let me explain that in few moments...</p>
<h2 id="how-you-can-draw-a-point-3d-space"><a class="header" href="#how-you-can-draw-a-point-3d-space">How you can draw a point 3D space</a></h2>
<pre><code class="language-py">from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt

# width by height here are 10 inches by  10 inches
fig = plt.figure(figsize=(10,10))

# 3d projection 
# with position (pos) of subplot as num-of-rows:1, num-of-cols:1, index-of-subplot:1
# If no positional arguments are passed, defaults to (1, 1, 1).

ax = fig.add_subplot(111, projection='3d')

# plot a point
ax.scatter(2,3,4)
plt.show()
</code></pre>
<p><img src="img/2/3d-plot.png" alt="3d plot" /></p>
<h2 id="what-is-a-feature-vector"><a class="header" href="#what-is-a-feature-vector">What is a feature Vector?</a></h2>
<p>Entries of the feature vectors represent <strong>features</strong> of the thing (object) this vector is used to represent.</p>
<p>Example:
Assume the thing (object) has 3 features: <em>Color, Heaviness and Shape</em>:</p>
<ol>
<li><strong>Color</strong> = 2
<ul>
<li>say number 1 means it is Red </li>
<li>say number 2 means it is Green</li>
<li>say number 3 means it is Blue</li>
</ul>
</li>
<li><strong>Heaviness</strong> : 2
<ul>
<li>say number 1 means it is light</li>
<li>say number 2 means it is medium</li>
<li>say number 3 means it is heavy</li>
<li>say number 4 means it is super heavy</li>
</ul>
</li>
<li><strong>Shape</strong> : 1
<ul>
<li>say number 1 means it is circle</li>
<li>say number 2 means it is rectangle</li>
<li>say number 3 means it is square</li>
<li>say number 4 means it is cube</li>
</ul>
</li>
</ol>
<p>So this object with <em>Color: Green, Heaviness: medium and Shape: circle</em> is represented (Data Representation) by this feature vector whose entries are:</p>
<pre><code> fv = [2,2,1]

</code></pre>
<p>Another example:</p>
<p>The object here is a <em>Patient</em> with:</p>
<ul>
<li>height: 64 inches,</li>
<li>weight: 131 pounds, </li>
<li>age: 23 years</li>
</ul>
<p>The <em>patient</em> vector <strong>p</strong>:</p>
<pre><code>p = [64, 131, 23]
</code></pre>
<p><img src="img/2/patient-vector.png" alt="Patient Vector" /></p>
<p>Now we understand how we can provide Data Representation using <strong>Feature Vectors</strong>.</p>
<h3 id="the-object-i-have-is-an-image-how-to-do-the-data-representations-for-this"><a class="header" href="#the-object-i-have-is-an-image-how-to-do-the-data-representations-for-this">The Object I have is an Image, how to do the Data Representations for this?</a></h3>
<ul>
<li>Black and White Images
<ul>
<li>Black: 0</li>
<li>White: 1</li>
<li>Gray: 0 to 255</li>
</ul>
</li>
</ul>
<p><img src="img/2/bw-img-2.png" alt="black and white image" /></p>
<h3 id="now-we-have-words-in-say-english-dictionary-how-to-do-the-data-representations-for-these-words"><a class="header" href="#now-we-have-words-in-say-english-dictionary-how-to-do-the-data-representations-for-these-words">Now we have words in say English dictionary, how to do the Data Representations for these words</a></h3>
<ul>
<li>
<p>Naive way:</p>
<ul>
<li>words are <strong>discrete</strong> and <strong>independent</strong> tokens</li>
<li>Build Dictionary or tokens </li>
</ul>
<pre><code class="language-py">['aardvark', ... 'king', ..., 'queen', ...]
</code></pre>
<ul>
<li>Since we need to convert words to numbers as part of our Data Representations, we can assign numbers to these words</li>
</ul>
<pre><code>[0, ...  11000, ... 12000, ...]

</code></pre>
<p>We will find out these large number are not well suited for ML. We can solve this by using a concept called <strong>one-hot-vector</strong></p>
<pre><code> [ [1,0,0,0,...], ... [0,0,0,0…1,0,0…], ... [0,0,0,,0,0…1,0,0…] ]      ]
</code></pre>
<p>These vectors have <strong>same dimensionality</strong> as the <strong>number-of-words in the dictionary</strong>.</p>
<p>Suppose the English has 100,000 word, the dimension of the one-hot-vector for each word will be 100,000. In that one-hot-vector only one entry in this vector will be <strong>1</strong> and all other entries will be <strong>0</strong>.</p>
<p><img src="img/2/one-hot-vector.png" alt="one-hot-vector" /></p>
<h4 id="disadvantages-with-one-hot-vector-way-of-data-representation"><a class="header" href="#disadvantages-with-one-hot-vector-way-of-data-representation">Disadvantages with one-hot-vector way of Data Representation</a></h4>
<ul>
<li>Very high dimensionality </li>
<li>Do not capture any world knowledge (like: Gender, Part-of-Speech...) about the words
<ul>
<li>example: <strong>king</strong> and <strong>queen</strong> are more in common with each other than <strong>aardvark</strong>
<ul>
<li>all of these token have <strong>90 degrees</strong> angles between them</li>
</ul>
</li>
</ul>
</li>
<li>Let us take a dictionary with only these 3 words</li>
</ul>
<pre><code>['aardvark', 'king', 'queen'] 

</code></pre>
<p><img src="img/2/word-rep-1.png" alt="word rep" /></p>
<ul>
<li>
<p>These 3 words in vectors in above 3-dimensional space</p>
</li>
<li>
<p>They are unit vectors aligned to axes</p>
</li>
<li>
<p>We need to find a way the words to occupy the entire this 3-dimensional space instead of perfectly aligning to the axes.</p>
</li>
<li>
<p>More useful Data Representation of words will be continuous vectors in the n-dimensional space</p>
<ul>
<li>This will allow <strong>aardvark</strong>, <strong>king</strong> and <strong>queen</strong> to flow anywhere in this 3-dimensional space. So their representations will be real-values like <strong>0.3</strong>, <strong>1.9</strong>, <strong>-0.4</strong> for <strong>aardvark</strong></li>
</ul>
<p><img src="img/2/num-rep-cont-1.png" alt="Word Continuous Representation" /></p>
</li>
<li>
<p>Representing world knowledge (like: Gender, Part-of-Speech...)</p>
</li>
<li>
<p>For example for <strong>queen</strong> [0.1, -0.3, 1.2,     -0.4, 0.02, 1.1,    -0.25, ... ]</p>
<ul>
<li>First 3 entires can represent the aspect of <strong>Gender</strong> for example</li>
<li>Next 3 entires can represent the aspect of <strong>Part-of-speech</strong> for example </li>
</ul>
</li>
<li>
<p>This mechanism will help us to express <strong>relationships between the words</strong> as equal to relative <strong>vector distance</strong></p>
</li>
</ul>
<p><img src="img/2/word-embedding-1.png" alt="word embedding" /></p>
<ul>
<li>For example, for Gender Dimension, <strong>king and queen</strong> should be far part as <strong>man and woman</strong>
<ul>
<li>In case of Part-of-speech dimensions, all these words should clustered together at a distance zero, since all of them are <strong>nouns</strong></li>
<li><strong>play</strong> (verb) and <strong>playful</strong> (adjective) should be at the same distance as <strong>joy</strong> and <strong>joyful</strong></li>
</ul>
</li>
</ul>
<h4 id="how-we-can-learn-useful-embedding-data-representations"><a class="header" href="#how-we-can-learn-useful-embedding-data-representations">How we can learn useful embedding (Data Representations)?</a></h4>
<ul>
<li>
<p><strong>Wikipedia</strong> comes to rescue us here! It is a reliable source of information we can use to learn the useful embeddings:</p>
<ul>
<li>has 28 billion words</li>
<li>309 languages</li>
</ul>
</li>
<li>
<p>If we look at the <strong>king</strong> and <strong>queen</strong> in Wikipedia, both them have lot of commonality</p>
<ul>
<li>They reference each other, includes common words like monarch</li>
<li>So Wikipedia has word knowledge we can extract to learn the useful embedding for us</li>
</ul>
</li>
<li>
<p>Wikipedia is freeform text</p>
<ul>
<li>Common practice
<ul>
<li>Unsupervised text data ---&gt; supervised task</li>
<li>we can ask the ML model to fill in the gaps and predict the next word as shown below:</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code>King is the title given to male ______

monarch : 70%
body : 20%
dog: 10%

</code></pre>
<ul>
<li>The ML model could produce a probability distribution over the word in the vocabulary indicating which ones are more likely to follow the word given so far, in our case it is <strong>monarch</strong></li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="eigenvectors"><a class="header" href="#eigenvectors">Eigenvectors</a></h1>
<h2 id="key-points"><a class="header" href="#key-points">Key points</a></h2>
<ul>
<li>Matrices as linear transformations</li>
<li>Determinants </li>
<li>Linear Systems</li>
<li>Change of Bias</li>
</ul>
<h3 id="scalar"><a class="header" href="#scalar">Scalar</a></h3>
<ul>
<li>A scalar is a number, like :
<ul>
<li>\( 2, -5, 0.368 \)</li>
</ul>
</li>
</ul>
<h3 id="vector"><a class="header" href="#vector">Vector</a></h3>
<ul>
<li>Vector can be thought of as a <strong>list numbers</strong> (can be in a row or column)
<ul>
<li>
<p>has rows <strong>OR</strong> columns</p>
</li>
<li>
<p>2 numbers for 2D space, such as \( (2,4) \)</p>
<ul>
<li>\( \begin{bmatrix} 2 \\ 8 \end{bmatrix} \)</li>
</ul>
</li>
<li>
<p>3 numbers for 3D space, such as \( (1,2,4) \)</p>
<ul>
<li>\( \begin{bmatrix} 1 \\ 2 \\ 4 \end{bmatrix} \)</li>
</ul>
</li>
</ul>
</li>
<li>A vector can be in:
<ul>
<li>magnitude and direction (Polar) form,</li>
<li>or in x and y (Cartesian) form</li>
</ul>
</li>
</ul>
<pre><code class="language-python">
import numpy as np
import matplotlib.pyplot as plt

def plot(V):

  origin = np.array([[0, 0, 0],[0, 0, 0]]) # origin point

  plt.figure(figsize=(8,8)) # 8 inches x 8 inches

  plt.grid()

  # Plot a 2D field of arrows.
  plt.quiver(*origin, V[:,0], V[:,1], color=['r','b','g'], scale=10)
  plt.show()
</code></pre>
<pre><code class="language-python">plot(np.array( [ [2,4] ] ) )

</code></pre>
<p><img src="img/vec/vec-1.png" alt="vec (2,4)" /></p>
<pre><code class="language-python">plot(np.array( [ [2,4], [2,3], [-2, 5] ] ) )
</code></pre>
<p><img src="img/vec/vec-2.png" alt="vec 2 " /></p>
<h3 id="matrix"><a class="header" href="#matrix">Matrix</a></h3>
<ul>
<li>
<p>A Matrix is an <strong>array of numbers</strong> (one or more rows, one or more columns)</p>
</li>
<li>
<p>Has <strong>rows x columns</strong></p>
</li>
<li>
<p>\(  \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end{bmatrix}  \) </p>
</li>
<li>
<p>\(  \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}  \) </p>
</li>
<li>
<p>Note:</p>
<ul>
<li>A vector is also a matrix! </li>
<li>It is special case of a matrix with  just <strong>one row or one column</strong></li>
<li>So the rules that work for matrices also work for vectors.</li>
</ul>
</li>
<li>
<p>We can add and subtract matrices of the same size, </p>
</li>
<li>
<p>multiply one matrix with another as long as the sizes are compatible :</p>
<ul>
<li>\( (n × m) × (m × p) = n × p) \)</li>
</ul>
</li>
<li>
<p>multiply an entire matrix by a constant:</p>
</li>
</ul>
<h3 id="tensor"><a class="header" href="#tensor">Tensor</a></h3>
<p>Tensor is a generalized matrix. </p>
<ul>
<li>1-D matrix (a vector is actually such a tensor), </li>
<li>3-D matrix (something like a cube of numbers), </li>
<li>0-D matrix (a single number)</li>
<li>a higher dimensional structure that is harder to visualize.</li>
<li>The dimension of the tensor is called its <strong>rank</strong>.</li>
</ul>
<h3 id="liner-transformation-described-by-a-matrix"><a class="header" href="#liner-transformation-described-by-a-matrix">Liner Transformation described by a matrix</a></h3>
<p>This transformation in 2D :</p>
<p>\( \hat i \longrightarrow   \begin{bmatrix} 3  \\ 0  \end{bmatrix} \) </p>
<p>\( \hat j \longrightarrow  \begin{bmatrix} 1 \\ 2 \end{bmatrix} \)</p>
<p>is represented by the matrix:
\(  \begin{bmatrix} 3 &amp; 1 \\ 0 &amp; 2 \end{bmatrix} \)</p>
<ul>
<li>
<p>Eigen vectors of the transformation</p>
<ul>
<li>
<p>Each Eigen vector has <strong>Eigen value</strong> associated with it</p>
</li>
<li>
<p><strong>Eigen value</strong> is the factor by which it will <strong>stretch or squash</strong> during the transformation</p>
</li>
<li>
<p>\(  \begin{bmatrix} 3  \\ 0  \end{bmatrix} \)  will <strong>stretch</strong> the length by factor of 3 during the transformation</p>
<ul>
<li>Eigen value here is 3</li>
</ul>
</li>
<li>
<p>\(  \begin{bmatrix} -1  \\ 1  \end{bmatrix} \)  will <strong>stretch</strong> the length by factor of 2 during the transformation </p>
<ul>
<li>
<ul>
<li>Eigen value here is 2</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Eigen value with 1</p>
<ul>
<li>Provides <strong>rotation</strong></li>
<li>No stretching or squashing here, so length of the vector remains same</li>
</ul>
</li>
</ul>
<h2 id="eigen-value--lambda--"><a class="header" href="#eigen-value--lambda--">Eigen Value \( \lambda  \)</a></h2>
<ul>
<li>Matrix-Vector multiplication</li>
</ul>
<p>\(A \vec{v} = \lambda \vec{v}  \)</p>
<ul>
<li>
<p>Scales the  Eigen Vector <strong>\(  \vec{v} \)</strong> by \(  \lambda \) </p>
</li>
<li>
<p>\( A \) is Transformation matrix</p>
</li>
<li>
<p>\( \vec{v} \) is Eigen Vector of \( A \) </p>
</li>
<li>
<p>Left hand side is Matrix-Vector multiplication</p>
</li>
<li>
<p>Right hand side is Scalar-Vector multiplication </p>
</li>
<li>
<p>Let us make both side as Matrix-Vector multiplication</p>
</li>
</ul>
<p>\(A \vec{v} = \lambda \vec{v}  \)</p>
<ul>
<li>We can write  the scalar \( \lambda \) as product of scalar and a Identity matrix \( I \):</li>
</ul>
<p>\(  \begin{bmatrix} \lambda &amp; 0 &amp; 0 \\ 0 &amp; \lambda &amp; 0 \\ 0 &amp; 0 &amp; \lambda \end{bmatrix} \longrightarrow  \lambda \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \) </p>
<p>We can write this in terms of  Identity matrix:</p>
<p>\( I  = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}  \) </p>
<p>as:</p>
<p>\(A \vec{v} = (\lambda I) \vec{v}   \)</p>
<p>so both sides are now Matrix multiplication</p>
<p>so we get:</p>
<p>\(A \vec{v} - (\lambda I) \vec{v} = \vec 0  \)</p>
<p>let us factor out \( \vec{v} \)</p>
<p>\( (A - \lambda I) \vec{v} = \vec 0\)</p>
<p>we have a new Matrix</p>
<p>\( (A - \lambda I) \)</p>
<p>and <a href="https://www.mathsisfun.com/algebra/matrix-determinant.html">determinant</a>:</p>
<p>\( det (A - \lambda I) =  0 \)</p>
<p>So:</p>
<p>For this Matrix: \(    \begin{bmatrix} 3 &amp; 1 \\ 0 &amp; 2 \end{bmatrix} \) find this Matrix:</p>
<p>\( det( \begin{bmatrix} 3-\lambda &amp; 1 \\ 0 &amp; 2-\lambda      \end{bmatrix} ) 
= ( 3 - \lambda) (2 - \lambda) - (0)(1) = ( 3 - \lambda) (2 - \lambda) 
\) </p>
<p>we have a quadratic polynominal in \( \lambda \)</p>
<p>\( ( 3 - \lambda) (2 - \lambda) = 0 \) </p>
<p>only possible Eigen values are</p>
<p>\( \lambda = 3 \) or 
\( \lambda = 2 \)</p>
<h2 id="references-1"><a class="header" href="#references-1">References</a></h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/PFDu9oVAE-g" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<p>\(  \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix} \)
\( \hat i \)  to  \(  \begin{bmatrix} 3 &amp; 5 \\ 0 &amp; 10 \end{bmatrix}  \)
\( \longrightarrow \)</p>
<ul>
<li><a href="https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference">Mathjax</a></li>
<li><a href="https://www.mathsisfun.com/algebra/scalar-vector-matrix.html"></a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="21-references"><a class="header" href="#21-references">2.1. References</a></h1>
<iframe width="720" height="480" src="https://www.youtube.com/embed/LlKAna21fLE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="720" height="480" src="https://www.youtube.com/embed/fNk_zzaMoSs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<ul>
<li><a href="https://mathinsight.org/vector_introduction">An introduction to vectors</a></li>
<li><a href="https://www.youtube.com/watch?v=LE3NfEULV6k">Transfer learning and Transformer models (ML Tech Talks)</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="3-ml-models"><a class="header" href="#3-ml-models">3. ML Models</a></h1>
<h2 id="simple-model"><a class="header" href="#simple-model">Simple model</a></h2>
<p>\( y = mx + b \)</p>
<pre><code> where
 m = slope (gradient)
 b = y-intercept

x is the independent variable
y is the dependent variable depends on m and b

</code></pre>
<h3 id="plotting-the-equation"><a class="header" href="#plotting-the-equation">Plotting the equation</a></h3>
<p>\( y = x*2 + 1 \)</p>
<p><img src="img/models/eqn-plotting-0.png" alt="ML Model plotting" /></p>
<pre><code class="language-py">import matplotlib.pyplot as plt
import numpy as np

# setup the plot size 10 inches by 10 inches
fig = plt.figure(figsize=(10,10))

# 1 row, 1 col, and index is 1
ax = fig.add_subplot(111)

# put grid in the plot
plt.grid()

# let us generate x values start from -5 to 5  with 100 samples
x = np.linspace(-5,5,100)
print ('Number of samples = {}' .format(len(x)))

ax.spines['left'].set_position('center')
ax.spines['bottom'].set_position('center')

ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')

# we need ticks at bottom and left
ax.xaxis.set_ticks_position('bottom')
ax.yaxis.set_ticks_position('left')

## our plot function
def plot_eqn(eqn,  color, label):
  plt.plot(x, eqn, color, label=label)
  # put legend at upper left cornor
  plt.legend(loc='upper left')

plot_eqn( x*2 + 1, '-r', 'eqn for x*2 + 1')
#plot_eqn( x*2 - 1, '-b', 'eqn for x*2 - 1')
#plot_eqn( x*2 - 3, ':b', 'eqn for x*2 - 3')
#plot_eqn( x*2 + 3, '--m', 'eqn for x*2 + 3')

## show our plot
plt.show()


</code></pre>
<h2 id="what-happens-when-we-train-a-ml-model-for-this-equation"><a class="header" href="#what-happens-when-we-train-a-ml-model-for-this-equation">What happens when we train a ML model for this equation?</a></h2>
<ul>
<li>We provide a training dataset with values for <strong>x</strong> and <strong>y</strong></li>
</ul>
<table><thead><tr><th>x</th><th>y</th></tr></thead><tbody>
<tr><td>2</td><td>5</td></tr>
<tr><td>1</td><td>3</td></tr>
<tr><td>7</td><td>15</td></tr>
<tr><td>...</td><td>...</td></tr>
</tbody></table>
<ul>
<li>During the training ML Model calculates the optimum value for <strong>m</strong> and <strong>b</strong> variables based on the training dataset we have provided</li>
<li>Once training completed, ML model is ready for predicting value for <strong>y</strong> for the given <strong>x</strong></li>
</ul>
<pre><code> You:   Hey, model my x value is 2, can you predict the value of y?
 Model: Sure, it is 5
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="word2vec"><a class="header" href="#word2vec">Word2Vec</a></h1>
<p><a href="https://en.wikipedia.org/wiki/Word2vec">Word2vec</a> is a technique for natural language processing (NLP).
Word2vec is used to produce word embeddings.</p>
<p>Uses a neural network model to learn <strong>word associations</strong> from a <strong>large corpus of text</strong>.</p>
<p>Once trained, such a model can detect <em>synonymous words</em>  or suggest additional words for a partial sentence.</p>
<p>Word2vec represents each <em>distinct word</em> with a particular list of numbers called a <em>vector</em>.</p>
<p>The vectors are chosen carefully such that a simple mathematical function (the <em>cosine similarity</em> between the vectors) indicates the level of <em>semantic similarity</em> between the words represented by those vectors.</p>
<p>Takes in large corpus of text as input and produces a vector space, typically of <em>several hundred dimensions</em>, with each unique word in the corpus being assigned a corresponding vector in the space.</p>
<ul>
<li>Word vectors are positioned in the vector space such that words that share <em>common contexts</em> in the corpus are located <em>close to one another</em> in the space.</li>
</ul>
<h3 id="papers"><a class="header" href="#papers">Papers</a></h3>
<ul>
<li><a href="https://arxiv.org/pdf/1301.3781.pdf">Efficient Estimation of Word Representations in Vector Space</a></li>
<li><a href="https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf">Distributed Representations of Words and Phrases and their Compositionality</a></li>
</ul>
<h3 id="continuous-bag-of-words-model"><a class="header" href="#continuous-bag-of-words-model">Continuous Bag-of-Words Model</a></h3>
<ul>
<li>Predicts the middle word based on surrounding context words. 
<ul>
<li>The context consists of a few words before and after the current (middle) word. This architecture is called a bag-of-words model as the order of words in the context is not important.</li>
</ul>
</li>
</ul>
<h3 id="continuous-skip-gram-model"><a class="header" href="#continuous-skip-gram-model">Continuous Skip-gram Model</a></h3>
<ul>
<li>Predicts words within a certain range before and after the current word in the same sentence. </li>
</ul>
<p>Consider the following sentence of 8 words:</p>
<pre><code>The wide road shimmered in the hot sun.
</code></pre>
<p>The context words for each of the 8 words of this sentence are defined by a <em>window size</em>.
The window size determines the <em>span of words on either side of a target_word</em> (one underlined) that can be considered context word. Take a look at this table of skip-grams for target words based on different window sizes.</p>
<p><img src="https://tensorflow.org/tutorials/text/images/word2vec_skipgram.png" alt="window size" /></p>
<h4 id="training-objective"><a class="header" href="#training-objective">Training Objective</a></h4>
<ul>
<li>Maximize the probability of predicting <strong>context words</strong> (w) given the target word (\(w_t\)).</li>
<li>For a sequence of words \(w_1,w_2, ... w_T\), the objective can be written as the average log probability. where \(c\) is the size of the training context. </li>
</ul>
<h2 id="playing-with-google-word2vec"><a class="header" href="#playing-with-google-word2vec">Playing with Google Word2Vec</a></h2>
<ul>
<li><a href="https://code.google.com/archive/p/word2vec/">word2vec</a> </li>
</ul>
<pre><code class="language-python">
import numpy as np
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

import gensim.downloader as api
word2vec_model = api.load('word2vec-google-news-300')

word2vec_model[&quot;green&quot;]
word2vec_model.most_similar(&quot;king&quot;)


print(word2vec_model.most_similar(positive=['king', 'girl'], negative=['boy'], topn=2))


vocab = ['apple', 'mango', 'sitar', 'violin', 'piano', 'pear', 'jackfruit', 'drums','beach', 'mountain', 'cloud' , 'laptop', 'minicomputer']
# TSNE T-distributed Stochastic Neighbor Embedding.
&quot;&quot;&quot;

t-SNE [1] is a tool to visualize high-dimensional data.

Refer: https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html


&quot;&quot;&quot;

def tsne_plot(model):
    labels = []
    wordvecs = []

    for word in vocab:
        wordvecs.append(model[word]) # add the wordvecs for the 'word'
        labels.append(word)
    
    tsne_model = TSNE(perplexity=3, n_components=2, init='pca', random_state=42)
    # n_components  Dimension of the embedded space.
    # random_state : determines the random number generator

    coordinates = tsne_model.fit_transform(wordvecs)

    # prepare 2-d data (x,y)
    x = []
    y = []
    for value in coordinates:
        x.append(value[0])
        y.append(value[1])
        
    plt.figure(figsize=(12,12)) # 12 inches x 12 inches
    for i in range(len(x)):
        plt.scatter(x[i],y[i])
        plt.annotate(labels[i],
                     xy=(x[i], y[i]),
                     xytext=(2, 2),
                     textcoords='offset points',
                     ha='right',
                     va='bottom')
    plt.grid()
    plt.show()

tsne_plot(word2vec_model)


</code></pre>
<p><img src="https://tensorflow.org/tutorials/text/images/word2vec_skipgram_objective.png" alt="prob" /></p>
<h3 id="notebooks"><a class="header" href="#notebooks">Notebooks</a></h3>
<ul>
<li>
<p><a href="https://colab.research.google.com/drive/1oSx0wd2lC4B15KWDinOjqptZtytqRkhk?usp=sharing">word2vec</a></p>
</li>
<li>
<p><a href="https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469#file-understanding-word-vectors-ipynb">Understanding word vectors</a></p>
</li>
<li>
<p><a href="https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/word2vec.ipynb">Word2Vec - TensorFlow</a></p>
</li>
</ul>
<h3 id="videos-1"><a class="header" href="#videos-1">Videos</a></h3>
<iframe width="720" height="480" src="https://www.youtube.com/embed/LSS_bos_TPI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="720" height="480" src="https://www.youtube.com/embed/L3D0JEA1Jdc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="720" height="480" src="https://www.youtube.com/embed/mI23bDF0VRI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<h2 id="word-embedding"><a class="header" href="#word-embedding">Word embedding</a></h2>
<p>Word embedding is a term used for the representation of words for text analysis, typically in the form of a <em>real-valued vector</em> that encodes the <em>meaning of the word</em> such that:
-  the words that are <em>closer in the vector space</em> are expected to be <em>similar</em> in meaning</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="glove---global-vectors-for-word-representation"><a class="header" href="#glove---global-vectors-for-word-representation">GloVe - Global Vectors for Word Representation</a></h1>
<p>GloVe is an <strong>unsupervised learning algorithm</strong> for obtaining vector representations for words. </p>
<p>Training is performed on aggregated global word-word <strong>co-occurrence statistics</strong> from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.</p>
<h2 id="playing-with-glove"><a class="header" href="#playing-with-glove">Playing with GloVe</a></h2>
<pre><code class="language-python">import matplotlib as plt
import numpy as np
from sklearn.manifold import TSNE
</code></pre>
<h3 id="get-word-vectors-tranined-on-wikipedia-from-gensim"><a class="header" href="#get-word-vectors-tranined-on-wikipedia-from-gensim">Get word vectors (tranined on Wikipedia) from gensim</a></h3>
<ul>
<li>376MB size file</li>
</ul>
<pre><code class="language-python">import gensim.downloader as dn
gensim_wiki_model = dn.load('glove-wiki-gigaword-300')
## 300 is dimensions

</code></pre>
<h3 id="let-us-find-the-vector-representation-of-a-given-word"><a class="header" href="#let-us-find-the-vector-representation-of-a-given-word">Let us find the vector representation of a given <em>word</em></a></h3>
<pre><code class="language-python">def get_vec_rep(word : str) :
  return gensim_wiki_model[word]

def get_most_similar(word : str) :
  return gensim_wiki_model.most_similar(word)

</code></pre>
<pre><code class="language-python">print(get_vec_rep('green'))

</code></pre>
<pre><code>[ 9.7111e-02 -3.9549e-01  5.0061e-01 -2.6536e-01  8.1473e-02 -5.1845e-01
  2.4072e-01  3.1200e-01  2.8080e-02 -6.8087e-01  3.8081e-01 -2.0683e-01
 -2.0663e-01  4.7282e-01  3.9394e-01  2.7941e-01 -7.5484e-01  1.4609e-01
 -4.7726e-01  4.5302e-01 -2.0524e-01  1.6755e-01 -1.8848e-01  2.1746e-01
  9.6432e-02 -6.8901e-01 -8.8415e-02  2.9760e-01 -2.1951e-01  1.2810e-02
 -1.7955e-03 -2.5013e-03 -2.7744e-01  3.7136e-01 -9.8262e-01  6.8767e-01
  2.6734e-01 -6.3868e-01 -3.1059e-01 -5.6088e-01 -1.4389e-02  1.8422e-01
  ...
]
</code></pre>
<pre><code class="language-python">
print(get_most_similar('king'))

</code></pre>
<pre><code class="language-python">
[
 ('queen', 0.6336469054222107),
 ('prince', 0.619662344455719), 
 ('monarch', 0.5899620652198792), 
 ('kingdom', 0.5791267156600952), 
 ('throne', 0.5606487989425659), 
 ('ii', 0.5562329888343811), 
 ('iii', 0.5503199100494385), 
 ('crown', 0.5224862694740295), 
 ('reign', 0.521735429763794), 
 ('kings', 0.5066401362419128)
 ]

</code></pre>
<ul>
<li>What is king - boy + girl?</li>
</ul>
<pre><code class="language-python">
print(gensim_wiki_model.most_similar(positive=['king', 'girl'], negative=['boy'], topn=2))
 
</code></pre>
<pre><code class="language-python">
[('queen', 0.6850624680519104), 
 ('monarch', 0.5474708676338196)
]


</code></pre>
<h2 id="plotting"><a class="header" href="#plotting">Plotting</a></h2>
<pre><code class="language-python">
## Plotting

vocab = ['apple', 'mango', 'sitar', 'violin', 'piano', 'pear', 'jackfruit', 'drums','beach', 'mountain', 'cloud' , 'laptop', 'minicomputer']
# TSNE T-distributed Stochastic Neighbor Embedding.
&quot;&quot;&quot;

t-SNE [1] is a tool to visualize high-dimensional data.

Refer: https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html


&quot;&quot;&quot;

def tsne_plot(model):
    labels = []
    wordvecs = []

    for word in vocab:
        wordvecs.append(model[word]) # add the wordvecs for the 'word'
        labels.append(word)
    
    tsne_model = TSNE(perplexity=3, n_components=2, init='pca', random_state=42)
    # n_components  Dimension of the embedded space.
    # random_state : determines the random number generator

    coordinates = tsne_model.fit_transform(wordvecs)

    # prepare 2-d data (x,y)
    x = []
    y = []
    for value in coordinates:
        x.append(value[0])
        y.append(value[1])
        
    plt.figure(figsize=(12,12)) # 12 inches x 12 inches
    for i in range(len(x)):
        plt.scatter(x[i],y[i])
        plt.annotate(labels[i],
                     xy=(x[i], y[i]),
                     xytext=(2, 2),
                     textcoords='offset points',
                     ha='right',
                     va='bottom')
    plt.grid()
    plt.show()

tsne_plot(gensim_wiki_model)


</code></pre>
<p><img src="img/tsne-1.png" alt="tsne" /></p>
<h2 id="references-2"><a class="header" href="#references-2">References</a></h2>
<ul>
<li><a href="https://nlp.stanford.edu/projects/glove/">GloVe</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="4-deep-learning"><a class="header" href="#4-deep-learning">4. Deep Learning</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="neural-network"><a class="header" href="#neural-network">Neural Network</a></h1>
<iframe width="720" height="480" src="https://www.youtube.com/embed/aircAruvnKk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<ul>
<li><a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning</a></li>
</ul>
<p><strong>Neural networks</strong> is a beautiful biologically-inspired programming <strong>paradigm</strong> which enables a computer to learn from <strong>observational data</strong>.</p>
<p><strong>Deep learning</strong>, a powerful set of techniques for <strong>learning in neural networks</strong>.</p>
<p>Provide solutions in :</p>
<ul>
<li>image recognition</li>
<li>speech recognition</li>
<li>natural language processing (NLP)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gradient-descent"><a class="header" href="#gradient-descent">Gradient descent</a></h1>
<p><a href="https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html">Gradient descent</a> is an optimization algorithm used to:</p>
<ul>
<li>minimize some function (cost function) by iteratively moving in the <strong>direction of steepest descent</strong> as defined by the negative of the gradient. </li>
</ul>
<p>In machine learning, we use gradient descent to update the parameters (weights and biases) of our model.</p>
<p><img src="https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent_demystified.png" alt="grad descent" /></p>
<ul>
<li>Starting at the top of the mountain, we take our first step <em>downhill</em> in the direction specified by the <em>negative gradient</em>. </li>
<li>Next we recalculate the negative gradient (passing in the coordinates of our new point) and take another step in the direction it specifies. </li>
<li>We continue this process iteratively until we get to the <em>bottom of our graph</em>, or to a point where we can no longer move downhill.</li>
</ul>
<h2 id="learning-rate"><a class="header" href="#learning-rate">Learning Rate</a></h2>
<p>The size of these steps is called the <strong>learning rate</strong>.</p>
<p>With a <strong>high learning rate</strong> we can cover more ground each step, but we risk overshooting the lowest point since the slope of the hill is constantly changing.</p>
<p>With a <strong>very low learning rate</strong>, we can confidently move in the direction of the negative gradient since we are recalculating it so frequently. A low learning rate is more precise, but calculating the gradient is time-consuming, so it will take us a very long time to get to the bottom.</p>
<h2 id="cost-function"><a class="header" href="#cost-function">Cost function</a></h2>
<ul>
<li>
<p>It is a loss function. </p>
</li>
<li>
<p>It is a measure of how wrong the model is in terms of its ability to estimate the relationship between x and y </p>
</li>
<li>
<p>It is a measure of how far we are away from the target:</p>
<ul>
<li>\(y - (mx + b)\)</li>
<li>Cost function :
<ul>
<li>\[ f(m,b) = \frac{1}{N} \sum_{i=0}^n (y_i - (mx_i + b))^2 \]</li>
</ul>
</li>
</ul>
</li>
<li>
<p>This tells us <strong>how bad</strong> our model is at making predictions for a given set of parameters. </p>
</li>
</ul>
<p>The cost function has its own curve and its own gradients. The slope of this curve tells us how to update our parameters (weight) to make the model more accurate.</p>
<p>We run gradient descent using our cost function.</p>
<ul>
<li>Calculate the partial derivatives of the cost function \( f(m,b)    \) with respect to each parameter( m and b) and store the results in a gradient.</li>
<li>This new gradient tells us the slope of our cost function at our current position (current parameter values) and the direction we should move to update our parameters (m and b).</li>
</ul>
<h3 id="derivative"><a class="header" href="#derivative">Derivative</a></h3>
<p><a href="https://en.wikipedia.org/wiki/Derivative">Derivative</a> of a function of a real variable measures the sensitivity to change of the function value (output value) with respect to a change in its argument (input value).</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/0/0f/Tangent_to_a_curve.svg" alt="derivative" /></p>
<p>The slope of the tangent line is equal to the derivative of the function at the marked point.</p>
<h3 id="partial-derivative"><a class="header" href="#partial-derivative">Partial derivative</a></h3>
<ul>
<li>
<p><a href="https://en.wikipedia.org/wiki/Partial_derivative">Partial derivative</a> of a function of several variables (in our case m and b)  is:</p>
<ul>
<li>its <a href="https://en.wikipedia.org/wiki/Derivative">derivative</a> with respect to one of those variables.</li>
</ul>
</li>
<li>
<p>with respect to m (weight): \( \frac{df}{dm}\)</p>
<ul>
<li>-2x(y - (mx + b))</li>
<li>\[  \frac{1}{N} \sum_{i=0}^n -2x_i(y_i - (mx_i + b)) \]</li>
</ul>
</li>
<li>
<p>with respect to b (bias): \( \frac{df}{db}\)</p>
<ul>
<li>-2(y - (mx + b))</li>
<li>\[  \frac{1}{N} \sum_{i=0}^n -2(y_i - (mx_i + b)) \] </li>
</ul>
</li>
</ul>
<iframe width="720" height="480" src="https://www.youtube.com/embed/HaHsqDjWMLU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="720" height="480" src="https://www.youtube.com/embed/tIeHLnjs5U8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<pre><code class="language-py">

# Y is target for the given input X
# mx + b is predicted
# learning_rate is size of the steps

def update_weights(m, b, X, Y, learning_rate):
    m_deriv = 0
    b_deriv = 0
    N = len(X)
    for i in range(N):
        # Calculate partial derivatives

        # -2x(y - (mx + b))
        m_deriv += -2*X[i] * (Y[i] - (m*X[i] + b))

        # -2(y - (mx + b))
        b_deriv += -2*(Y[i] - (m*X[i] + b))

    # We subtract because the derivatives point in direction of steepest ascent
    m -= (m_deriv / float(N)) * learning_rate
    b -= (b_deriv / float(N)) * learning_rate

    return m, b

</code></pre>
<iframe width="720" height="480" src="https://www.youtube.com/embed/IHZwWFHWa-w" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<div style="break-before: page; page-break-before: always;"></div><h1 id="back-propagation"><a class="header" href="#back-propagation">Back Propagation</a></h1>
<iframe width="720" height="480" src="https://www.youtube.com/embed/Ilg3gGewQ5U" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<div style="break-before: page; page-break-before: always;"></div><h1 id="calculus"><a class="header" href="#calculus">Calculus</a></h1>
<iframe width="720" height="480" src="https://www.youtube.com/embed/tIeHLnjs5U8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<div style="break-before: page; page-break-before: always;"></div><h1 id="activation-function"><a class="header" href="#activation-function">Activation Function</a></h1>
<p>Activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. </p>
<p>When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be <strong>fired to the next neuron</strong></p>
<h2 id="sigmoid-function"><a class="header" href="#sigmoid-function">Sigmoid function</a></h2>
<p>A sigmoid function is a mathematical function having a characteristic &quot;S&quot;-shaped curve or sigmoid curve.</p>
<p><img src="img/1/sigmoid-1.png" alt="Sigmoid" /></p>
<ul>
<li>\( \sigma(x) =  \frac{1}{1 + e^{-x}}\)</li>
</ul>
<pre><code class="language-py">
import math
import matplotlib.pyplot as plt
import numpy as np

def sigmoid(x):
    a = []
    for item in x:
        a.append(1/(1 + math.exp(-item)))
    return a

x = np.arange(-10., 10., 0.2)
sig = sigmoid(x)

# plot sig
plt.plot(x,sig)
plt.show()

</code></pre>
<h2 id="hyperbolic-tangent-activation-function"><a class="header" href="#hyperbolic-tangent-activation-function">Hyperbolic tangent activation function</a></h2>
<p>It is also referred  the \(Tanh\) (also “tanh” and “TanH“) function. 
It is very similar to the <strong>sigmoid activation function</strong> and even has the same S-shape. 
The function takes any real value as input and outputs values in the range -1 to 1.</p>
<pre><code class="language-py"># plot for the tanh activation function
from math import exp
import matplotlib.pyplot as plt
 
# tanh activation function
def tanh(x):
	return (exp(x) - exp(-x)) / (exp(x) + exp(-x))
 
# define input data
inputs = [x for x in range(-10, 10)]
# calculate outputs
outputs = [tanh(x) for x in inputs]

# plot inputs vs outputs
plt.plot(inputs, outputs)
plt.grid()
plt.show()

</code></pre>
<p><img src="img/1/tanh-1.png" alt="TanH" /></p>
<p><a href="https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/">Refer: How to Choose an Activation Function for Deep Learnin</a></p>
<h2 id="softmax"><a class="header" href="#softmax">Softmax</a></h2>
<pre><code class="language-py">from numpy import exp

# softmax activation function
def softmax(x):
	return exp(x) / exp(x).sum()

# define input data
inputs = [1.0, 3.0, 2.0]
# calculate outputs
outputs = softmax(inputs)
# report the probabilities
print(outputs)
# report the sum of the probabilities
print(outputs.sum())

</code></pre>
<pre><code class="language-bash">[0.09003057 0.66524096 0.24472847]
1.0
</code></pre>
<h2 id="rectified-linear-activation-function"><a class="header" href="#rectified-linear-activation-function">Rectified Linear Activation Function</a></h2>
<p>A node or unit that implements this activation function is referred to as a rectified linear activation unit, or ReLU for short. </p>
<pre><code class="language-py">
# demonstrate the rectified linear function
 
# rectified linear function
def rectified(x):
	return max(0.0, x)
 
# demonstrate with a positive input
x = 1.0
print('rectified(%.1f) is %.1f' % (x, rectified(x)))
x = 1000.0
print('rectified(%.1f) is %.1f' % (x, rectified(x)))
# demonstrate with a zero input
x = 0.0
print('rectified(%.1f) is %.1f' % (x, rectified(x)))
# demonstrate with a negative input
x = -1.0
print('rectified(%.1f) is %.1f' % (x, rectified(x)))
x = -1000.0
print('rectified(%.1f) is %.1f' % (x, rectified(x)))

</code></pre>
<h3 id="plotting-1"><a class="header" href="#plotting-1">Plotting</a></h3>
<pre><code class="language-py"># plot inputs and outputs
from matplotlib import pyplot
 
# rectified linear function
def rectified(x):
	return max(0.0, x)
 
# define a series of inputs
series_in = [x for x in range(-10, 11)]
# calculate outputs for our inputs
series_out = [rectified(x) for x in series_in]
# line plot of raw inputs to rectified outputs
pyplot.plot(series_in, series_out)
pyplot.show()
</code></pre>
<p><img src="img/1/rectified-1.png" alt="Rectified" /> </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="convolutional-neural-networks"><a class="header" href="#convolutional-neural-networks">Convolutional Neural Networks</a></h1>
<h2 id="resources"><a class="header" href="#resources">Resources</a></h2>
<ul>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_1.pdf">CNN for Visual Recognition</a></p>
</li>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_2.pdf">Image Classification</a></p>
</li>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_5.pdf">Convolutional Neural Networks - Lecture-5</a></p>
</li>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_3.pdf">Loss Functions and Optimization</a></p>
</li>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_4.pdf">Neural Networks and Backpropagation</a></p>
</li>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_5.pdf">CNN</a></p>
</li>
<li>
<p>[Hardware and Software][http://cs231n.stanford.edu/slides/2021/lecture_6.pdf]</p>
</li>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_9.pdf">CNN Architectures</a></p>
</li>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_7.pdf">Convolutional Neural Networks - Lecture-7 - Training Neural Networks - Part-1s</a></p>
</li>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_8.pdf">Convolutional Neural Networks - Lecture-7 - Training Neural Networks - Part-2</a></p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="recurrent-neural-networks"><a class="header" href="#recurrent-neural-networks">Recurrent Neural Networks</a></h1>
<h2 id="generative-models"><a class="header" href="#generative-models">Generative Models</a></h2>
<p>Given training data, generate new samples from same distribution</p>
<h2 id="references-3"><a class="header" href="#references-3">References</a></h2>
<ul>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_9.pdf">RNN</a></p>
</li>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_11.pdf">Attention and Transformers</a></p>
</li>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_12.pdf">Generative Models</a></p>
</li>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_13.pdf">Self-Supervised Learning</a></p>
</li>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_14.pdf">Visualizing and Understanding</a></p>
</li>
<li>
<p><a href="http://cs231n.stanford.edu/slides/2021/lecture_15.pdf">Detection and Segmentation</a></p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mnist"><a class="header" href="#mnist">MNIST</a></h1>
<p>The MNIST database <em>Modified National Institute of Standards and Technology database</em> is a large database of <strong>handwritten digits</strong> that is commonly used for training various image processing systems.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png" alt="MNIST" /></p>
<h2 id="sample-application-for-handwritten-digit-recognition"><a class="header" href="#sample-application-for-handwritten-digit-recognition">Sample Application for handwritten digit recognition</a></h2>
<iframe src='https://mohan-chinnappan-n2.github.io/2021/ai/mnist/minst.html' 
width="860" height="400">
</iframe>
<ul>
<li><a href="https://mohan-chinnappan-n2.github.io/2021/ai/mnist/minst.html">MNIST App - brain.js based</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="5-tensorflow"><a class="header" href="#5-tensorflow">5. Tensorflow</a></h1>
<h2 id="distributed-training-with-tensorflow"><a class="header" href="#distributed-training-with-tensorflow">Distributed Training with Tensorflow</a></h2>
<p>Deep Learning involves very large datasets.</p>
<p>Faster model training time helps to provide faster iterations to reach model goals faster and trying new ideas.</p>
<p>Distribution is not automatic.</p>
<p>In case of a 4 GPU hardware:</p>
<table><thead><tr><th>GPU#</th><th>Name</th><th>Temp</th><th>PowerUsage</th><th>GPU Util</th></tr></thead><tbody>
<tr><td>0</td><td>Tesla</td><td>59C</td><td>113/250W</td><td>72%</td></tr>
<tr><td>1</td><td>Tesla</td><td>52C</td><td>113/250W</td><td>0%</td></tr>
<tr><td>2</td><td>Tesla</td><td>50C</td><td>113/250W</td><td>0%</td></tr>
<tr><td>3</td><td>Tesla</td><td>59C</td><td>113/250W</td><td>0%</td></tr>
</tbody></table>
<p>To enable to use of all GPUs, modeling code needs to be modified to make TensorFlow to coordinate across the GPUs at runtime.</p>
<h3 id="model-parallelism"><a class="header" href="#model-parallelism">Model Parallelism</a></h3>
<ul>
<li>Running independent parts of the computations which we can run in parallel.
<code>WX</code>  (matrix multpicaton) is done at gpu-0 which adding (add op) with <code>b</code> bias is done at gpu-1</li>
</ul>
<h3 id="data-parallelism"><a class="header" href="#data-parallelism">Data Parallelism</a></h3>
<ul>
<li>Works with any model architecture, so widely adopted.</li>
</ul>
<pre><code class="language-py">
# Linear Model, W: Weights, b: Bias
y_pred =  WX + b

# Equivalent to a keras Dense single unit
tf.keras.layers.Dense(units=1)

model.fit(x,y, batch_size=32)


</code></pre>
<p>For <strong>each step</strong> of the model training, <strong>a batch of data</strong> is used to calculate gradients. Thus obtained gradients are used to update the weights of the model.
<strong>Larger</strong> the batch size, the <strong>more accurate</strong> the gradients are.
Making batch size <strong>too large</strong> will make us to run out of <strong>GPU memory</strong>.</p>
<pre><code class="language-py"># with data parallelism we can add NUM_GPUS in the batch_size
model.fit(x,y, batch_size=32 * NUM_GPUS)


</code></pre>
<p>Each GPU gets a separate slice of the data, calculate the gradients, and those gradients are <strong>averaged</strong>. So the model is able to <strong>see more data</strong> during <strong>each training step</strong>. So less time is taken to finish an epoch (a full pass to the training data)</p>
<h4 id="synchronous-data-parallelism"><a class="header" href="#synchronous-data-parallelism">Synchronous Data Parallelism</a></h4>
<h4 id="asynchronous-data-parallelism"><a class="header" href="#asynchronous-data-parallelism">Asynchronous Data Parallelism</a></h4>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tensorflowjs"><a class="header" href="#tensorflowjs">TensorFlow.js</a></h1>
<p>TensorFlow.js (TFJS) is a library for machine learning in JavaScript.
Using TFJS you can develop ML models in JavaScript, and use ML directly in the browser or in Node.js.</p>
<h2 id="browser"><a class="header" href="#browser">Browser</a></h2>
<pre><code class="language-js">
&lt;script src=&quot;https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.0.0/dist/tf.min.js&quot;&gt;&lt;/script&gt;

</code></pre>
<h2 id="nodejs"><a class="header" href="#nodejs">Node.js</a></h2>
<pre><code class="language-bash">
# install TensorFlow.js. using npm or yarn
yarn add @tensorflow/tfjs

# Install TensorFlow.js with native C++ bindings.
yarn add @tensorflow/tfjs-node

# if your system has a NVIDIA® GPU with CUDA support, use the GPU package even for higher performance.
yarn add @tensorflow/tfjs-node-gpu


</code></pre>
<pre><code class="language-js">
const tf = require('@tensorflow/tfjs');

// Optional Load the binding:
// Use '@tensorflow/tfjs-node-gpu' if running with GPU.
require('@tensorflow/tfjs-node');

// Train a simple model:
const model = tf.sequential();
model.add(tf.layers.dense({units: 100, activation: 'relu', inputShape: [10]}));
model.add(tf.layers.dense({units: 1, activation: 'linear'}));
model.compile({optimizer: 'sgd', loss: 'meanSquaredError'});

const xs = tf.randomNormal([100, 10]);
const ys = tf.randomNormal([100, 1]);

model.fit(xs, ys, {
  epochs: 100,
  callbacks: {
    onEpochEnd: (epoch, log) =&gt; console.log(`Epoch ${epoch}: loss = ${log.loss}`)
  }
});
  
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="6-pytorch"><a class="header" href="#6-pytorch">6. PyTorch</a></h1>
<p>PyTorch is an open source machine learning library based on the <strong>Torch library</strong>, used for applications such as :</p>
<ul>
<li>computer vision </li>
<li>natural language processing
primarily developed by Facebook's AI Research lab.</li>
</ul>
<h2 id="quick-start"><a class="header" href="#quick-start">Quick Start</a></h2>
<ul>
<li><a href="https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/c30c1dcf2bc20119bcda7e734ce0eb42/quickstart_tutorial.ipynb#scrollTo=M10FvMZFJPvc">Tutorial</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="7-transformers"><a class="header" href="#7-transformers">7. Transformers</a></h1>
<p>A <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">transformer</a> is a deep learning model that adopts the mechanism of <strong>self-attention</strong>, <strong>differentially weighting the significance</strong> of each part of the input data.</p>
<p>Like recurrent neural networks (RNNs), transformers are designed to <strong>handle sequential input data</strong>, such as natural language, for tasks such as <strong>translation and text summarization</strong>. 
However, unlike RNNs, transformers do not necessarily process the data in order. Instead the attention mechanism provides <strong>context for any position</strong> in the input sequence. </p>
<p>For example, if the input data is a natural language sentence, the transformer <strong>does not need to process the beginning of the sentence before the end</strong>. Rather, it identifies the <strong>context</strong> that confers <strong>meaning</strong> to each word in the sentence. 
This feature allows for more parallelization than RNNs and therefore reduces training times.</p>
<p>Before transformers, most state-of-the-art NLP systems relied on gated RNNs, such as LSTM and gated recurrent units (GRUs), with <strong>added attention</strong> mechanisms. </p>
<p>Transformers are built <strong>on these attention</strong> technologies without using an RNN structure, highlighting the fact that <strong>attention mechanisms alone can match the performance</strong> of RNNs with attention.</p>
<p>Gated RNNs process tokens sequentially, maintaining <strong>a state vector</strong> that contains a representation of the data seen after every token.</p>
<p>To process the <em>n</em>th token, the model combines the state representing the sentence up to token <em>n-1</em> with the information of the new token to create a <strong>new state</strong>, representing the sentence up to token <em>n</em>. </p>
<p><strong>Theoretically</strong>, the information from one token can propagate arbitrarily far down the sequence, if at every point the state continues to encode contextual information about the token. </p>
<p><strong>In practice</strong> this mechanism is flawed: the <strong>vanishing gradient problem</strong> leaves the model's state at the end of a long sentence <strong>without precise, extractable information about preceding tokens</strong>.</p>
<p>This problem was addressed by <strong>attention mechanisms</strong>. Attention mechanisms let a model draw from the state at any preceding point along the sequence. </p>
<p>The attention layer can access <strong>all previous states and weigh them according to a learned measure of relevancy</strong>, providing relevant information about far-away tokens.</p>
<p>A clear example of the value of attention is in <strong>language translation</strong>, where <strong>context is essential</strong> to assign the meaning of a word in a sentence. 
In an English-to-French translation system, the first word of the French output most probably depends heavily on the <strong>first few words of the English input</strong>. 
However, in a classic LSTM model, in order to produce the first word of the French output, the model is <strong>given only the state vector of the last English word</strong>. 
Theoretically, this vector can encode information about the whole English sentence, giving the model all necessary knowledge. 
In practice, this information is often <strong>poorly preserved by the LSTM</strong>. 
An <em>attention mechanism can be added to address this problem</em>: 
- the decoder is given <em>access to the state vectors of every English input word, not just the last</em>, and can learn attention weights that dictate how much to attend to each English input state vector.</p>
<p>Transformers use an attention mechanism <strong>without an RNN</strong></p>
<ul>
<li>processing all tokens at the same time </li>
<li>calculating attention weights between them in <strong>successive layers</strong>.</li>
</ul>
<h3 id="vanishing-gradient-problem"><a class="header" href="#vanishing-gradient-problem"><a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">Vanishing Gradient problem</a></a></h3>
<p>In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with <em>gradient-based learning methods and backpropagation</em>.
- In such methods, each of the neural network's <strong>weights</strong> receives an update proportional to the <strong>partial derivative</strong> of the error function with respect to the current weight in each iteration of training.
- The problem is that in some cases, the <strong>gradient will be vanishingly small</strong>, effectively <strong>preventing the weight from changing its value</strong>
- In the worst case, this may completely <strong>stop</strong> the neural network from further training</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="bert"><a class="header" href="#bert">BERT</a></h1>
<p>BERT, which stands for Bidirectional Encoder Representations from Transformers, is based on <strong>Transformers</strong>, a deep learning model in which every output element is connected to every input element, and the weightings between them are dynamically calculated based upon their <strong>connection</strong>.</p>
<p>BERT is a technology to generate <strong>contextualized</strong> word embeddings/vectors, which is its biggest advantage but also it's biggest disadvantage as it is <strong>very compute-intensive at inference time</strong>, meaning that if you want to use it in production at scale, it can become costly.</p>
<ul>
<li><a href="https://arxiv.org/pdf/1706.03762.pdf">Paper: Attention Is All You Need</a></li>
</ul>
<p><strong>Transformer encoder</strong> reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that it’s non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word)</p>
<ul>
<li><a href="https://arxiv.org/pdf/1810.04805.pdf">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>
<li><a href="https://jalammar.github.io/illustrated-bert/">The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/bert">Hugging face - BERT</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gpt"><a class="header" href="#gpt">GPT</a></h1>
<p>Generative Pre-trained Transformer (GPT)</p>
<h2 id="gpt-2"><a class="header" href="#gpt-2">GPT-2</a></h2>
<p>GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset  of <strong>8 million web pages</strong>. 
GPT-2 is trained with a simple objective: </p>
<ul>
<li>predict the <strong>next word</strong>, given all of the previous words within some text. </li>
<li>The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. </li>
<li>GPT-2 is a direct scale-up of GPT, with more than <strong>10X the parameters</strong> and trained on more than 10X the amount of data.</li>
</ul>
<p>&quot; We (OpenAI) created a new dataset which emphasizes diversity of content, by scraping content from the Internet. In order to preserve document quality, we used only pages which have been <strong>curated/filtered by humans—specifically</strong>, we used outbound links from Reddit which received at least 3 karma. This can be thought of as a heuristic indicator for whether other users found the link interesting (whether educational or funny), leading to higher data quality than other similar datasets, such as CommonCrawl.&quot;</p>
<p>GPT-2 algorithm was trained on the task of language modeling--- which tests a program's ability to predict the next word in a given sentence--by ingesting huge numbers of articles, blogs, and websites. By using just this data it achieved state-of-the-art scores on a number of unseen language tests, an achievement known as <strong>zero-shot learning</strong>. It can also perform other writing-related tasks, like translating text from one language to another, summarizing long articles, and answering trivia questions.</p>
<p><a href="https://github.com/openai/gpt-2">GPT-2 code</a>
<a href="https://colab.research.google.com/github/ilopezfr/gpt-2/blob/master/gpt-2-playground_.ipynb">Notebook</a></p>
<h3 id="zero-shot-learning"><a class="header" href="#zero-shot-learning">Zero-shot learning</a></h3>
<p>Zero-shot learning (ZSL) is a problem setup in machine learning, where <strong>at test time</strong>, a learner observes samples from classes that <strong>were not observed during training</strong>, and needs to predict the class they belong to.</p>
<h2 id="gpt-3"><a class="header" href="#gpt-3"><a href="https://github.com/openai/gpt-3">GPT-3</a></a></h2>
<p>Generative Pre-trained Transformer 3 is an <strong>autoregressive language model</strong> that uses deep learning to produce human-like text. </p>
<p>It is the third-generation language prediction model in the <em>GPT-n</em> series created by OpenAI.</p>
<p>GPT-3 is a very <strong>large language model</strong> (the largest till date) with about <a href="https://github.com/openai/gpt-3/blob/master/175b_samples.jsonl"><strong>175B parameters</strong></a>. It is trained on about <a href="https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_character_count.csv"><strong>45TB of text data</strong></a> from different datasets.</p>
<ul>
<li><a href="https://arxiv.org/pdf/2005.14165.pdf">Language Models are Few-Shot Learners</a></li>
<li><a href="https://paperswithcode.com/method/gpt-3">GPT-3 - paper with code</a></li>
<li><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a></li>
<li><a href="https://openai.com/blog/better-language-models/">Better Language Models and Their Implication</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="t5"><a class="header" href="#t5">T5</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="github-copilot"><a class="header" href="#github-copilot"><a href="https://copilot.github.com/">GitHub Copilot</a></a></h1>
<p>When we type this:</p>
<pre><code class="language-js">function calculateDaysBetweenDates(date1, date2) {

</code></pre>
<p>Copilot will complete this function:</p>
<pre><code class="language-js"> 
function calculateDaysBetweenDates(date1, date2) {

  // following lines are written by copilot
  var oneDay = 24 * 60 * 60 * 1000;
  var date1InMillis = date1.getTime();
  var date2InMillis = date2.getTime();
  var days = Math.round(Math.abs(date2InMillis - date1InMillis) / oneDay);
  return days;
}


</code></pre>
<p>\( \int x dx = \frac{x^2}{2} + C \)</p>
<p>\( \int y dy = \frac{y^3}{5} \)</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="references-4"><a class="header" href="#references-4">References</a></h1>
<h2 id="transformers"><a class="header" href="#transformers">Transformers</a></h2>
<ul>
<li><a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a></li>
</ul>
<iframe width="720" height="480" src="https://www.youtube.com/embed/LE3NfEULV6k" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<hr />
<iframe width="720" height="480" src="https://www.youtube.com/embed/SZorAJ4I-sA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<ul>
<li><a href="https://www.youtube.com/watch?v=4Bdc55j80l8">Illustrated Guide to Transformers Neural Network: A step by step explanation</a></li>
<li><a href="https://www.youtube.com/watch?v=OyFJWRnt_AY">CS480/680 Lecture 19: Attention and Transformer Networks</a></li>
<li><a href="https://www.youtube.com/watch?v=S27pHKBEp30">LSTM is dead. Long Live Transformers!</a></li>
</ul>
<h2 id="open-ai"><a class="header" href="#open-ai">Open AI</a></h2>
<ul>
<li><a href="https://beta.openai.com/docs/introduction">OpenAI Documentation</a></li>
</ul>
<h3 id="codex"><a class="header" href="#codex">Codex</a></h3>
<ul>
<li>natural language to code</li>
<li><a href="https://www.youtube.com/watch?v=Zm9B-DvwOgw">Creating a Space Game with OpenAI Codex</a></li>
<li><a href="https://openai.com/blog/openai-codex/">OpenAI Codex</a></li>
</ul>
<h2 id="github-copilot-1"><a class="header" href="#github-copilot-1">Github Copilot</a></h2>
<ul>
<li>Trained on billions of lines of public code</li>
<li><a href="https://copilot.github.com/">Your AI pair programmer</a></li>
<li><a href="https://github.com/github/copilot-docs/blob/main/docs/visualstudiocode/gettingstarted.md#getting-started-with-github-copilot-in-visual-studio-code">VS Code extension</a></li>
</ul>
<h2 id="matplotlib"><a class="header" href="#matplotlib">Matplotlib</a></h2>
<p><a href="https://scriptverse.academy/tutorials/python-matplotlib-plot-function.html">Matplotlib: Plot a Function y=f(x) </a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="9-salesforce-einstein"><a class="header" href="#9-salesforce-einstein">9. Salesforce Einstein</a></h1>
<p>With Salesforce Einstein, we can: 
• Build custom predictions and recommendations with clicks
• Embed predictive insights into any record or in any app
• Operationalize AI by adding it to every workflow or business process</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="machine-learning"><a class="header" href="#machine-learning">Machine Learning</a></h1>
<h2 id="einstein-discovery"><a class="header" href="#einstein-discovery">Einstein Discovery</a></h2>
<p>Einstein Discovery automatically provides explanations and makes recommendations based on all your data sources so they can get smart insights, <strong>without the need of a data scientist</strong>.</p>
<h2 id="einstein-prediction-builder"><a class="header" href="#einstein-prediction-builder">Einstein Prediction Builder</a></h2>
<p>Einstein Prediction Builder helps you to predict the business outcomes, such as :</p>
<ul>
<li>churn or lifetime value. 
Create custom AI models on <strong>any Salesforce Object field</strong> or Object with <strong>clicks</strong>, not code.</li>
<li><a href="https://www.salesforce.com/content/dam/web/en_us/www/documents/e-books/analytics/sfdc-predictions.pdf">The big book of customer predictions - Get closer to your customers with Salesforce Einstein</a></li>
</ul>
<h2 id="einstein-next-best-action"><a class="header" href="#einstein-next-best-action">Einstein Next Best Action</a></h2>
<p>Einstein Next Best Action delivers proven recommendations to employees and customers, right in the apps where they work. </p>
<ul>
<li>Define recommendations, create <strong>action strategies</strong>, build predictive models, display recommendations, and activate automation.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="natural-language-processing"><a class="header" href="#natural-language-processing">Natural Language Processing</a></h1>
<h2 id="einstein-language"><a class="header" href="#einstein-language">Einstein Language</a></h2>
<p>Einstein Language helps you to understand:</p>
<ul>
<li>how <strong>customers feel</strong>, </li>
<li>automatically route inquiries</li>
<li>streamline your workflows. </li>
</ul>
<p>Build natural language processing (NLP) into your apps to classify the <strong>underlying intent</strong> and <strong>sentiment</strong> in a body of text, <strong>no matter what the language</strong>.</p>
<h2 id="einstein-bots"><a class="header" href="#einstein-bots">Einstein Bots</a></h2>
<p>Einstein Bots helps you to easily build, train, and deploy <strong>custom bots on digital channels</strong> that are connected to your CRM data. </p>
<ul>
<li>Enhance business processes, empower your employees, and delight your customers.</li>
<li><a href="https://www.salesforce.com/content/dam/web/en_us/www/documents/datasheets/einstein-for-service-data-sheet.pdf">Einstein for Service: AI-POWERED CUSTOMER SERVICE</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="computer-vision"><a class="header" href="#computer-vision">Computer Vision</a></h1>
<h2 id="einstein-vision"><a class="header" href="#einstein-vision">Einstein Vision</a></h2>
<p>Einstein Vision helps you to see the entire conversation about your brand on social media and beyond. </p>
<ul>
<li>Use intelligent image recognition in your apps by training deep learning models to <strong>recognize your brand, products</strong>, and more.</li>
<li><a href="https://www.salesforce.com/content/dam/web/en_us/www/assets/pdf/datasheets/einstein-vision-and-language-getting-started.pdf">Get Started Using Deep Learning for Business Users &amp; Techies</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="10-google-cloud-platform"><a class="header" href="#10-google-cloud-platform">10. Google Cloud Platform</a></h1>
<h2 id="dataflow"><a class="header" href="#dataflow">Dataflow</a></h2>
<p><img src="img/gcp/gcp-df-1.png" alt="GCP Dataflow" /></p>
<iframe width="720" height="480" src="https://www.youtube.com/embed/udKgN1_eThs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<div style="break-before: page; page-break-before: always;"></div><h1 id="11-processing-units"><a class="header" href="#11-processing-units">11. Processing Units</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cpu"><a class="header" href="#cpu">CPU</a></h1>
<p>CPU is constructed from millions of transistors.
It can have <strong>multiple processing cores</strong> and is commonly referred to as the brain of the computer. 
It is essential to all modern computing systems as it executes the commands and processes needed for your computer and operating system. 
The CPU is also important in determining how fast programs can run, from surfing the web to building spreadsheets.</p>
<p>The CPU is suited to a wide variety of workloads, especially those for which latency or per-core performance are important. A powerful execution engine, the CPU focuses its **smaller number of cores on individual tasks and on getting things done quickly. This makes it uniquely well equipped for jobs ranging from serial computing to running databases.</p>
<h2 id="intels-sandy-bridge-architecture--32-nm-micro-architecture"><a class="header" href="#intels-sandy-bridge-architecture--32-nm-micro-architecture">Intel's Sandy Bridge Architecture ( 32 nm micro architecture)</a></h2>
<p><img src="img/cpu/cpu-intel-sandy-bridge-1.png" alt="Intel Sandy Bridge CPU" /></p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Sandy_Bridge#:%7E:text=Sandy%20Bridge%20is%20the%20codename,to%20Nehalem%20and%20Westmere%20microarchitecture.">Refer</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gpu"><a class="header" href="#gpu">GPU</a></h1>
<p>The GPU is a processor that is made up of many smaller and more <strong>specialized cores</strong>. By working together, these cores deliver <strong>massive performance</strong> when a processing task can be <strong>divided up</strong> and processed across these cores.</p>
<p>GPUs began as specialized <a href="https://en.wikipedia.org/wiki/Application-specific_integrated_circuit">ASICs - Application Specific Integrated Circuit</a> developed to <strong>accelerate specific 3D rendering tasks</strong>. </p>
<p>Over time, these fixed-function engines became more programmable and more flexible. While graphics and the increasingly lifelike visuals of today’s top games remain their principal function, GPUs have evolved to <strong>become more general-purpose parallel processors as well</strong>, handling a growing range of applications.</p>
<p>Initially GPUs were solving computer <strong>graphics</strong> related problems in Gaming
The General Purpose GPU (GPGPU) plays a vital role in the deep learning and parallel computing.</p>
<p><img src="img/gpu/NVIDIA-GPU-arch.png" alt="NIVIDA GPU" /></p>
<h3 id="what-is-cuda"><a class="header" href="#what-is-cuda">What is CUDA?</a></h3>
<p>Compute Unified Device Architecture (<a href="https://developer.nvidia.com/blog/cuda-refresher-getting-started-with-cuda/">CUDA</a>) is is a parallel computing platform developed by NVIDIA. It enables software programs to perform calculations using both the CPU and GPU.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tpu---tensor-processing-units"><a class="header" href="#tpu---tensor-processing-units">TPU - Tensor Processing Units</a></h1>
<p><a href="https://cloud.google.com/tpu/docs/tpus">TPUs</a> are Google’s custom-developed application-specific integrated circuits (ASICs) used to accelerate machine learning workloads.</p>
<p>Designed from the ground up with the benefit of Google’s deep experience and leadership in machine learning.</p>
<p>Enable us to run our machine learning workloads on <strong>Google’s TPU accelerator hardware</strong> using <strong>TensorFlow</strong></p>
<p>Designed for maximum performance and flexibility to help researchers, developers, and businesses to build TensorFlow compute clusters that can leverage <strong>CPUs, GPUs, and TPUs</strong>.</p>
<p>High-level <strong>TensorFlow APIs</strong> help us to get models running on the Cloud TPU hardware.</p>
<h2 id="advantages-for-using-tpus"><a class="header" href="#advantages-for-using-tpus">Advantages for using TPUs</a></h2>
<ul>
<li>
<p>Cloud TPU resources accelerate the performance of <strong>linear algebra computation</strong>, which is used heavily in machine learning applications.</p>
</li>
<li>
<p>TPUs <strong>minimize</strong> the <strong>time-to-accuracy</strong> when you <strong>train large, complex neural network models</strong>. Weeks to hours (150 times faster)</p>
<ul>
<li>Models that previously took <strong>weeks to train</strong> on other hardware platforms can converge in <strong>hours on TPUs</strong>.</li>
</ul>
</li>
</ul>
<h2 id="tpu-v3"><a class="header" href="#tpu-v3">TPU v3</a></h2>
<p>A <a href="https://cloud.google.com/tpu/docs/system-architecture-tpu-vm">TPU v3</a> board contains four TPU chips and 32 GiB of HBM. Each TPU chip contains two cores. Each core has a MXU, a vector unit, and a scalar unit.</p>
<p><img src="https://cloud.google.com/tpu/docs/images/tpu-v3-layout.png" alt="TPU architecture" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="11-ml-pipelines"><a class="header" href="#11-ml-pipelines">11. ML Pipelines</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ml-ops-or-mlops"><a class="header" href="#ml-ops-or-mlops">ML Ops or MLOps</a></h1>
<p>Set of practices that aims to deploy and maintain machine learning models in <strong>production reliably and efficiently</strong>.</p>
<p>The word <strong>MLOps</strong> is a compound of <em>machine learning</em> and the continuous development practice of <em>DevOps</em> in the software field. </p>
<ul>
<li>Machine learning models are tested and developed in isolated experimental systems.</li>
<li>When an algorithm is ready to be launched, MLOps is practiced between Data Scientists, DevOps, and Machine Learning engineers to transition the algorithm to production systems.</li>
<li>MLOps seeks to increase automation and improve the quality of production models, while also focusing on business and regulatory requirements.</li>
</ul>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/1/1b/ML_Ops_Venn_Diagram.svg" alt="MLOps Wiki" />
<img src="https://ml-ops.org/img/mlops-loop-en.jpg" alt="MlOps" /></p>
<iframe width="800" height="420" src="https://www.youtube.com/embed/Ta14KpeZJok" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<h2 id="papers-1"><a class="header" href="#papers-1">Papers</a></h2>
<ul>
<li><a href="https://proceedings.neurips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf">Hidden Technical Debt in Machine Learning Systems</a></li>
</ul>
<h2 id="salesforce-transmogrifai"><a class="header" href="#salesforce-transmogrifai">Salesforce TransmogrifAI</a></h2>
<ul>
<li>AutoML library for building modular, reusable, strongly typed machine learning workflows on Apache Spark with minimal hand-tuning</li>
<li>Transmogrification as the process of transforming, often in a surprising or magical manner, which is what TransmogrifAI does for Salesforce 
<ul>
<li>enabling data science teams to transform customer data into meaningful, actionable predictions</li>
<li>thousands of <strong>customer-specific machine learning models</strong> have been deployed across the platform, powering more than 3 billion predictions every day.</li>
</ul>
</li>
</ul>
<p>TransmogrifAI is a library built on Scala and SparkML that does precisely this. </p>
<p>With just a few lines of code, a data scientist can automate data cleansing, feature engineering, and model selection to arrive at a performant model from which she can explore and iterate further.</p>
<p><img src="https://miro.medium.com/max/5280/1*nRxOm1irE_aNB-UweA7IXA.png" alt="TransmogrifAI" /></p>
<p>Type Safety: The TransmogrifAI Feature type hierarchy</p>
<p><img src="https://miro.medium.com/max/1540/1*vkrbLrOyIinhonrPCGHivA.png" alt="Type Safety" /></p>
<p>Transmogrification
These transformations are not just about getting the data into a format which algorithms can use, TransmogrifAI also optimizes the transformations to make it easier for machine learning algorithms to learn from the data.</p>
<ul>
<li>For example, it might transform a numeric feature like age into the most appropriate age buckets for a particular problem — age buckets for the fashion industry might differ from wealth management age buckets.</li>
</ul>
<p>TransgmogrifAI has algorithms that perform automatic feature validation to remove features with little to <em>no predictive power</em>.
- Example: <em>Closed Deal Amount</em></p>
<p>The TransmogrifAI <em>Model Selector</em> runs a tournament of several different machine learning algorithms on the data and uses the average validation error to automatically choose the best one</p>
<p>TransmogrifAI comes with some techniques for automatically tuning these hyperparameters and a framework to extend to more advanced tuning techniques.</p>
<pre><code class="language-scala">
// Read the Deal data
val dealData = DataReaders.Simple.csvCase[Deal](path = pathToData).readDataset().toDF()

// Extract response and predictor Features
val (isClosed, predictors) = FeatureBuilder.fromDataFrame[RealNN](dealData, response = &quot;isClosed&quot;)

// Automated feature engineering
val featureVector = predictors.transmogrify()

// Automated feature validation
val cleanFeatures = isClosed.sanityCheck(featureVector, removeBadFeatures = true)

// Automated model selection
val (pred, raw, prob) = BinaryClassificationModelSelector().setInput(isClosed, cleanFeatures).getOutput()

// Setting up the workflow and training the model
val model = new OpWorkflow().setInputDataset(dealData).setResultFeatures(pred).train()

</code></pre>
<p>TransmogrifAI is built  on top of Apache Spark</p>
<ul>
<li>
<p>Able to handle large variation in the size of the data</p>
<ul>
<li>Some use cases involve tens of millions of records that need to be aggregated or joined, others depend on a few thousands of records.</li>
</ul>
</li>
<li>
<p>Spark has primitives for dealing with distributed joins and aggregates on big data </p>
</li>
<li>
<p>Able to serve our machine learning models in both a batch and streaming (Spark Streaming) setting</p>
</li>
<li>
<p>Transmogrification, Feature Validation, and Model Selection above, are all powered by Estimators)</p>
<ul>
<li>A Feature is essentially a type-safe pointer to a column in a DataFrame and contains all the information about that column — its name, the type of data it contains, as well as lineage information about how it was derived.</li>
</ul>
</li>
<li>
<p>TransmogrifAI provides the ability to easily define features that are the result of complex time-series aggregates and joins</p>
</li>
<li>
<p>Features are strongly typed. This allows TransmogrifAI to do type checks on the entire machine learning workflow, and ensure that <em>errors are caught as early on as possible</em> instead of hours into a running pipeline</p>
</li>
<li>
<p>Developers can easily specify custom transformers and estimators to be used in the pipeline</p>
</li>
</ul>
<pre><code class="language-scala">val lowerCaseText = textFeature.map[Text](_.value.map(_.toLowerCase).toText)
</code></pre>
<h3 id="scale-and-performance"><a class="header" href="#scale-and-performance">Scale and performance</a></h3>
<ul>
<li>With automated feature engineering, data scientists can easily <em>blow up the feature space</em>, and end up with wide DataFrames that are hard for Spark to deal with. </li>
<li>TransmogrifAI workflows address this by inferring the entire DAG of transformations that are needed to materialize features, and optimize the execution of this DAG by collapsing all transformations that occur at the same level of the DAG into a single operation.</li>
</ul>
<h3 id="summary"><a class="header" href="#summary">Summary</a></h3>
<p>TransmogrifAI enables our data scientists to deploy thousands of models in production with minimal hand tuning and reducing the average turn-around time for training a performant model from weeks to just a couple of hours.</p>
<p><a href="https://github.com/salesforce/TransmogrifAI">TransmogrifAI</a></p>
<ul>
<li><a href="https://engineering.salesforce.com/open-sourcing-transmogrifai-4e5d0e098da2">Open Sourcing TransmogrifAI</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tensorflow-serving"><a class="header" href="#tensorflow-serving">TensorFlow Serving</a></h1>
<p>TensorFlow Serving is a flexible, <strong>high-performance serving system</strong> for machine learning models, designed for <strong>production environments</strong>. </p>
<p>TensorFlow Serving makes it easy to <strong>deploy new algorithms and experiments</strong>, while keeping the same server architecture and APIs. </p>
<p><a href="https://www.tensorflow.org/tfx/serving/architecture">TensorFlow Serving</a> provides out-of-the-box integration with TensorFlow models, but can be easily extended to serve <strong>other types of models and data</strong>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tensorflow-extended-tfx"><a class="header" href="#tensorflow-extended-tfx">TensorFlow Extended (TFX)</a></h1>
<p>TFX is a Google-production-scale machine learning (ML) platform based on TensorFlow. </p>
<p>It provides a configuration framework and shared libraries to integrate common components needed to define, launch, and monitor your machine learning system.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="apache-airflow"><a class="header" href="#apache-airflow">Apache Airflow</a></h1>
<p>Apache Airflow is a platform to programmatically author, schedule and monitor workflows. 
TFX (TensorFlow Extended) uses <strong>Airflow</strong> to author workflows as directed acyclic graphs (<a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">DAGs</a>) of tasks. </p>
<p>The Airflow <strong>scheduler executes tasks</strong> on an array of workers while following the specified dependencies. </p>
<p>Rich command line utilities (CLI) make performing complex surgeries on DAGs a snap. 
The rich user interface (UI) makes it easy to:</p>
<ul>
<li><strong>visualize pipelines</strong> running in production, </li>
<li><strong>monitor progress</strong>, </li>
<li><strong>troubleshoot issues</strong> when needed. </li>
</ul>
<p>When workflows are <strong>defined as code</strong>, they become more maintainable, versionable, testable, and collaborative.</p>
<h3 id="dag"><a class="header" href="#dag">DAG</a></h3>
<p>DAG (directed acyclic graph) is a directed graph with no directed cycles. That is, it consists of vertices and edges, with each edge directed from one vertex to another, such that following those directions will <strong>never form a closed loop</strong>. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="apache-beam"><a class="header" href="#apache-beam">Apache Beam</a></h1>
<p>Apache Bean help us to implement <strong>batch and streaming data processing jobs</strong> that run on any execution engine.</p>
<p>Several TFX (TensorFlow Extended) components rely on Beam for <strong>distributed data processing</strong>. 
In addition, TFX can use Apache Beam to orchestrate and execute the pipeline DAG. </p>
<p>Beam orchestrator uses a different <strong>BeamRunner</strong> than the one which is used for component data processing. 
With the default <strong>DirectRunner</strong> setup the Beam orchestrator can be used for local debugging without incurring the extra <strong>Airflow or Kubeflow dependencies</strong>, which simplifies system configuration.</p>
<p><img src="img/apachebeam/beam-1.webp" alt="Apache Beam" /></p>
<iframe width="720" height="480" src="https://www.youtube.com/embed/udKgN1_eThs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kubeflow"><a class="header" href="#kubeflow">Kubeflow</a></h1>
<p>The Kubeflow project is dedicated to making <strong>deployments of machine learning (ML) workflows</strong> on <strong>Kubernetes</strong> simple, portable and scalable.</p>
<p><img src="img/kubeflow/kubeflow-arch-1.svg" alt="Kubeflow Arch" /></p>
<p>Kubeflow is an open source ML platform dedicated to making deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable. </p>
<p>Kubeflow Pipelines is part of the <strong>Kubeflow platform</strong> that enables composition and execution of reproducible workflows on Kubeflow, integrated with experimentation and notebook based experiences. </p>
<p>Kubeflow Pipelines services on Kubernetes include the hosted Metadata store, container based orchestration engine, notebook server, and UI to help users develop, run, and manage complex ML pipelines at scale. </p>
<p>The Kubeflow Pipelines SDK allows for creation and sharing of components and composition and of pipelines programmatically.</p>
<h2 id="kubernetes"><a class="header" href="#kubernetes">Kubernetes</a></h2>
<p>Kubernetes is an open-source <strong>container-orchestration system</strong> for automating computer application </p>
<ul>
<li>deployment</li>
<li>scaling</li>
<li>management</li>
</ul>
<p>It was originally designed by Google and is now maintained by the Cloud Native Computing Foundation.</p>
<h2 id="references-5"><a class="header" href="#references-5">References</a></h2>
<ul>
<li><a href="https://www.kubeflow.org/docs/started/introduction/">Kubeflow home</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="automl"><a class="header" href="#automl">AutoML</a></h1>
<p>AutoML makes the power of machine learning available to you even if you have limited knowledge of machine learning.</p>
<p>You can use AutoML to build on Google's machine learning capabilities to <strong>create your own custom machine learning models that are tailored to your business needs</strong>, and then integrate those models into your applications and web sites.</p>
<iframe width="800" height="420" src="https://www.youtube.com/embed/93vsqjfGPCw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="800" height="420" src="https://www.youtube.com/embed/PKTvo9X9Sjg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kubernetes-1"><a class="header" href="#kubernetes-1">Kubernetes</a></h1>
<iframe width="560" height="315" src="https://www.youtube.com/embed/X48VuDVv0do" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<div style="break-before: page; page-break-before: always;"></div><h1 id="12-speedup"><a class="header" href="#12-speedup">12. Speedup</a></h1>
<p>In this chapter we will see the ways to speed up Machine Learning</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="12-jax"><a class="header" href="#12-jax">12. JAX</a></h1>
<p><img src="https://raw.githubusercontent.com/google/jax/main/images/jax_logo_250px.png" alt="JAX" /></p>
<p><a href="https://github.com/google/jax">JAX</a> is a new library from <a href="https://research.google/">Google Research</a>. 
JAX can automatically differentiate native Python and Numpy functions.</p>
<ul>
<li>Loops</li>
<li>Branches</li>
<li>Recursion </li>
<li><a href="closure.html">Closures</a></li>
<li>Can take Derivative of Derivatives
<ul>
<li>Supports <strong>reverse mode differentiation</strong>, also known as [Back Propagation] using Grad function</li>
<li>Supports <strong>forward mode differentiation</strong></li>
</ul>
</li>
</ul>
<h2 id="xla"><a class="header" href="#xla">XLA</a></h2>
<p><a href="https://www.tensorflow.org/xla">XLA</a> is <strong>Accelerated Linear Algebra</strong>. </p>
<ul>
<li>
<p>It is a <strong>domain-specific compiler</strong> for <strong>linear algebra</strong> that can <strong>accelerate TensorFlow models</strong> with potentially <strong>no source code changes</strong>.</p>
</li>
<li>
<p>Performs optimizations like:</p>
<ul>
<li><strong>Fusing</strong> operations together (something like consolidation) so the intermediate results do not have to written out the memory. Instead it get <strong>streamed</strong> into next operation.</li>
<li>This enable faster and more efficient processing</li>
</ul>
<p>This is some what <strong>crudely</strong> equal to nodejs stream:
- Refer: TableauCRM CLI using this stream concept, where it loads data from a Oracle SQL Query results directly into Tableau CRM dataset
- <a href="https://github.com/mohan-chinnappan-n/cli-dx/blob/master/db/ora2ea.md">refer: sfdx mohanc:ea:dataset:loadFromOra</a></p>
<pre><code class="language-py"> def model (x, y, z):
     return tf.reduce_sum( x + y * z)
</code></pre>
</li>
</ul>
<p>JAX uses XLA to compile and run our Numpy program on <a href="https://www.intel.com/content/www/us/en/products/docs/processors/what-is-a-gpu.html">GPUs</a> and <a href="https://cloud.google.com/tpu/docs/tpus">TPUs</a></p>
<p>JAX uses JIT (just-in-time) compile of custom functions into XLA optimized kernels using decorator <code>@jit </code></p>
<pre><code class="language-py">
@jit # jit decorator
def update(params, x, y):
    grads = gard(loss)(params, x, y)
    return [ (w - step_size * dw, b - step_size * db) for (w, b), (dw, db) in zip (params, grads)]

</code></pre>
<h2 id="pmap"><a class="header" href="#pmap">pmap</a></h2>
<ul>
<li>JAX applies pmap (<a href="https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap">Parallel Map</a>) replicating computations across multiple cores
<img src="img/jax/jax-pmap.png" alt="pmap" /></li>
</ul>
<h2 id="autograd"><a class="header" href="#autograd">Autograd</a></h2>
<p>Autograd (https://github.com/hips/autograd) can automatically differentiate native <strong>Python and Numpy</strong> code.</p>
<h3 id="functions-available-for-the-transformations"><a class="header" href="#functions-available-for-the-transformations">Functions available for the transformations</a></h3>
<ul>
<li>grad</li>
<li>jit</li>
<li>pmap</li>
<li>vmap - automatic vectorization
<ul>
<li>allowing us to turn a function which can handle only <strong>one data point</strong> into a function which can handle <strong>batch of these data points</strong> of any size with just one wrapper function <em>vmap</em></li>
</ul>
</li>
</ul>
<h3 id="sample---mnist"><a class="header" href="#sample---mnist"><a href="https://colab.research.google.com/github/google/jax/blob/master/docs/notebooks/neural_network_with_tfds_data.ipynb">Sample - MNIST </a></a></h3>
<ul>
<li><a href="http://yann.lecun.com/exdb/mnist/">MIST Database</a></li>
</ul>
<pre><code class="language-py">
import jax.numpy as jnp
from jax import grad, vmap, jit


def predict(params, inputs):
  for W, b in params:
    outputs = jnp.dot(inputs, W) + b
    inputs = jnp.tanh(outputs)
  return outputs

def loss (params, batch):
  inputs, targets = batch
  preds = predict(params, inputs)
  return jnp.sum( (preds - targets) **2 ) # SME


gradient_fun = jit(grad(loss))
preexample_grads = vmap(grad(loss), in_axes=(None, 0))

</code></pre>
<h3 id="key-ideas"><a class="header" href="#key-ideas">Key Ideas</a></h3>
<ul>
<li>Python code is traced into an Intermediate Representation (IR)
<ul>
<li>IR can be transformed (automatic differentiation) </li>
<li>IR enables domain-specific compilation (XLA - Accelerated Linear Algebra)</li>
</ul>
</li>
<li>Has very powerful transforms
<ul>
<li>grad</li>
<li>jit</li>
<li>vmap</li>
<li>pmap</li>
</ul>
</li>
<li>Python's dynamism makes this possible
<ul>
<li>JAX makes use of this dynamism and evaluates a function's behavior by calling it on a <strong>tracer</strong> value</li>
</ul>
</li>
</ul>
<pre><code class="language-py">def sum(x):
    return x + 2

class ShapedArray(object):
    def __add__ (self, other):
        self.record_computation(&quot;add&quot;, self, other)
        return ShapedArray(self.shape, self.dtype) # dtype is like float32

sum( ShapedArray( (2,2), float32 ))


</code></pre>
<p><img src="img/jax/jax-ir-1.png" alt="jax-IR" /></p>
<p>With this IR, JAX knows how to do the transforms like:</p>
<ul>
<li>grad</li>
<li>jit</li>
<li>vmap</li>
<li>pmap</li>
</ul>
<p><img src="img/jax/jax-ir-2.png" alt="jax-IR" /></p>
<p><img src="img/jax/jax-ir-3.png" alt="jax-IR" /></p>
<pre><code class="language-py">TF_CPP_MIN_LOG_LEVEL=0 

import jax
import jax.numpy as jnp

global_list = []

def log2(x):
  global_list.append(x)
  ln_x = jnp.log(x)
  ln_2 = jnp.log(2.0)
  return ln_x / ln_2

print( jax.make_jaxpr(log2)(3.0) )

</code></pre>
<ul>
<li><a href="https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html#how-jax-transforms-work">Document</a></li>
<li>Output </li>
</ul>
<pre><code>{ lambda ; a:f32[]. let
    b:f32[] = log a
    c:f32[] = log 2.0
    d:f32[] = div b c
  in (d,) 
}

</code></pre>
<h3 id="jake-on-jax"><a class="header" href="#jake-on-jax">Jake on JAX</a></h3>
<iframe width="720" height="480" src="https://www.youtube.com/embed/WdTeDXsOSj4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="720" height="480" src="https://www.youtube.com/embed/0mVmRHMaOJ4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="720" height="480" src="https://www.youtube.com/embed/fuAyUQcVzTY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<div style="break-before: page; page-break-before: always;"></div><h1 id="closures-and-decorators"><a class="header" href="#closures-and-decorators">Closures and Decorators</a></h1>
<h2 id="python-closures"><a class="header" href="#python-closures">Python Closures</a></h2>
<p>Let us explain <strong>closure</strong> by an example:</p>
<ul>
<li><a href="https://www.programiz.com/python-programming/closure">Refer</a></li>
</ul>
<pre><code class="language-py">
# This is the outer enclosing function
def print_msg(msg):

    def printer():
        # This is the nested function
        print(msg)

    return printer  # returns the nested function


# Now let's try calling this function.
another = print_msg(&quot;Hello&quot;)
another()
# Output: Hello
</code></pre>
<p>This technique by which <strong>some data</strong> in our case &quot;Hello&quot; gets attached to the code - <em>another()</em> is called <strong>closure</strong> in Python.</p>
<p>Three characteristics of a Python closure are: </p>
<ol>
<li>it is a nested function, in our example: <em>printer()</em> </li>
<li>it has access to a free variable in outer scope, in our example: <em>msg</em>. </li>
<li>it is returned from the <strong>enclosing</strong> function, in our example: <em>print_msg()</em></li>
</ol>
<pre><code>

``
# Python Decorators make an extensive use of closures 

</code></pre>
<h2 id="python-decorators"><a class="header" href="#python-decorators">Python Decorators</a></h2>
<ul>
<li><a href="https://www.programiz.com/python-programming/decorator">Refer</a></li>
</ul>
<p>A decorator takes in a function, adds some functionality and returns it. </p>
<pre><code class="language-py">#  a decorator takes in a function, adds some functionality and returns it.

# takes in function to be decorated
def make_pretty(func):
    def inner():
        print(&quot;I got decorated&quot;) # getting decorated
        func() # back to the given function
    return inner


def ordinary():
    print(&quot;I am ordinary&quot;)

</code></pre>
<pre><code class="language-py"># will print:  I am ordinary

ordinary()

</code></pre>
<pre><code class="language-py">decorated = make_pretty(ordinary)
decorated() 
&quot;&quot;&quot; will print: 
    I got decorated
    I am ordinary
&quot;&quot;&quot;

# decorator function (make_pretty) has added
##  some new functionality to the original function (ordinary)

</code></pre>
<pre><code class="language-py"># annoation way
@make_pretty # syntactic sugar
def ordinary():
    print(&quot;I am ordinary&quot;)

</code></pre>
<pre><code class="language-py">iam_special = ordinary()
&quot;&quot;&quot; will print: 
    I got decorated
    I am ordinary
&quot;&quot;&quot;

</code></pre>
<h3 id="decorating-functions-with-parameters"><a class="header" href="#decorating-functions-with-parameters">Decorating functions with parameters</a></h3>
<pre><code class="language-py"># Decorating functions with parameters

def smart_divide(func):
    def inner(a, b):
        print(&quot;I am going to divide&quot;, a, &quot;and&quot;, b)
        if b == 0:
            print(&quot;Whoops! cannot divide by zero&quot;)
            return

        return func(a, b)
    return inner


@smart_divide
def divide(a, b):
    print(a/b)
</code></pre>
<pre><code class="language-py">
divide(10,2)

&quot;&quot;&quot; will print: 
    I am going to divide 10 and 2
    5.0
&quot;&quot;&quot;
</code></pre>
<pre><code class="language-py">
divide(10,0)

&quot;&quot;&quot; will print: 
    I am going to divide 10 and 0
    Whoops! cannot divide by zero
&quot;&quot;&quot;

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="references-6"><a class="header" href="#references-6">References</a></h1>
<iframe width="720" height="480" src="https://www.youtube.com/embed/0mVmRHMaOJ4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<ul>
<li><a href="https://realpython.com/primer-on-python-decorators/">Primer on Python Decorators</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="13-openai"><a class="header" href="#13-openai">13. OpenAI</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openai-api"><a class="header" href="#openai-api">OpenAI API</a></h1>
<h3 id="completion"><a class="header" href="#completion">Completion</a></h3>
<pre><code class="language-bash">curl https://api.openai.com/v1/engines/davinci/completions \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer YOUR_API_KEY \
  -d '{
  &quot;prompt&quot;: &quot;Once upon a time&quot;,
  &quot;max_tokens&quot;: 5
}'

</code></pre>
<ul>
<li>Result</li>
</ul>
<pre><code class="language-json">{
  &quot;id&quot;: &quot;cmpl-4DmYlIcNgBh26avH8t5mMMWwgILGE&quot;,
  &quot;object&quot;: &quot;text_completion&quot;,
  &quot;created&quot;: 1639190275,
  &quot;model&quot;: &quot;davinci:2020-05-03&quot;,
  &quot;choices&quot;: [
    {
      &quot;text&quot;: &quot;, there was a software&quot;,
      &quot;index&quot;: 0,
      &quot;logprobs&quot;: null,
      &quot;finish_reason&quot;: &quot;length&quot;
    }
  ]
}

</code></pre>
<h3 id="search"><a class="header" href="#search">Search</a></h3>
<pre><code class="language-bash">curl https://api.openai.com/v1/engines/davinci/search \
  -H &quot;Content-Type: application/json&quot; \
  -H 'Authorization: Bearer YOUR_API_KEY' \
  -d '{
  &quot;documents&quot;: [&quot;White House&quot;, &quot;hospital&quot;, &quot;school&quot;],
  &quot;query&quot;: &quot;the president&quot;
}'
</code></pre>
<pre><code class="language-json">{
  &quot;object&quot;: &quot;list&quot;,
  &quot;data&quot;: [
    {
      &quot;object&quot;: &quot;search_result&quot;,
      &quot;document&quot;: 0,
      &quot;score&quot;: 215.56
    },
    {
      &quot;object&quot;: &quot;search_result&quot;,
      &quot;document&quot;: 1,
      &quot;score&quot;: 55.614
    },
    {
      &quot;object&quot;: &quot;search_result&quot;,
      &quot;document&quot;: 2,
      &quot;score&quot;: 40.932
    }
  ],
  &quot;model&quot;: &quot;davinci:2020-05-03&quot;
}

</code></pre>
<h3 id="create-classification"><a class="header" href="#create-classification">Create Classification</a></h3>
<pre><code class="language-bash">curl https://api.openai.com/v1/classifications \
  -X POST \
  -H &quot;Authorization: Bearer YOUR_API_KEY&quot; \
  -H 'Content-Type: application/json' \
  -d '{
    &quot;examples&quot;: [
      [&quot;A happy moment&quot;, &quot;Positive&quot;],
      [&quot;I am sad.&quot;, &quot;Negative&quot;],
      [&quot;I am feeling awesome&quot;, &quot;Positive&quot;]],
    &quot;query&quot;: &quot;It is a raining day :(&quot;,
    &quot;search_model&quot;: &quot;ada&quot;,
    &quot;model&quot;: &quot;curie&quot;,
    &quot;labels&quot;:[&quot;Positive&quot;, &quot;Negative&quot;, &quot;Neutral&quot;]
  }'

</code></pre>
<pre><code class="language-json">
{
  &quot;completion&quot;: &quot;cmpl-4DmdStcV7tC6o5VJnoihr4TDFHse4&quot;,
  &quot;label&quot;: &quot;Negative&quot;,
  &quot;model&quot;: &quot;curie:2020-05-03&quot;,
  &quot;object&quot;: &quot;classification&quot;,
  &quot;search_model&quot;: &quot;ada&quot;,
  &quot;selected_examples&quot;: [
    {
      &quot;document&quot;: 1,
      &quot;label&quot;: &quot;Negative&quot;,
      &quot;text&quot;: &quot;I am sad.&quot;
    },
    {
      &quot;document&quot;: 0,
      &quot;label&quot;: &quot;Positive&quot;,
      &quot;text&quot;: &quot;A happy moment&quot;
    },
    {
      &quot;document&quot;: 2,
      &quot;label&quot;: &quot;Positive&quot;,
      &quot;text&quot;: &quot;I am feeling awesome&quot;
    }
  ]
}


</code></pre>
<h3 id="answers"><a class="header" href="#answers">Answers</a></h3>
<pre><code class="language-bash">curl https://api.openai.com/v1/answers \
  -X POST \
  -H &quot;Authorization: Bearer YOUR_API_KEY&quot; \
  -H 'Content-Type: application/json' \
  -d '{
    &quot;documents&quot;: [&quot;Puppy A is happy.&quot;, &quot;Puppy B is sad.&quot;],
    &quot;question&quot;: &quot;which puppy is happy?&quot;,
    &quot;search_model&quot;: &quot;ada&quot;,
    &quot;model&quot;: &quot;curie&quot;,
    &quot;examples_context&quot;: &quot;In 2017, U.S. life expectancy was 78.6 years.&quot;,
    &quot;examples&quot;: [[&quot;What is human life expectancy in the United States?&quot;,&quot;78 years.&quot;]],
    &quot;max_tokens&quot;: 5,
    &quot;stop&quot;: [&quot;\n&quot;, &quot;&lt;|endoftext|&gt;&quot;]
  }'

</code></pre>
<pre><code class="language-json">{
&quot;answers&quot;: [
  &quot;puppy A.&quot;
],
&quot;completion&quot;: &quot;cmpl-4DmgSrZJ7sQx6lWRbaMyskSN68qCE&quot;,
&quot;model&quot;: &quot;curie:2020-05-03&quot;,
&quot;object&quot;: &quot;answer&quot;,
&quot;search_model&quot;: &quot;ada&quot;,
&quot;selected_documents&quot;: [
  {
    &quot;document&quot;: 0,
    &quot;text&quot;: &quot;Puppy A is happy. &quot;
  },
  {
    &quot;document&quot;: 1,
    &quot;text&quot;: &quot;Puppy B is sad. &quot;
  }
]
}

</code></pre>
<h3 id="list-files"><a class="header" href="#list-files">List Files</a></h3>
<pre><code class="language-bash">curl https://api.openai.com/v1/files \
  -H 'Authorization: Bearer YOUR_API_KEY'


</code></pre>
<pre><code class="language-json">{
  &quot;object&quot;: &quot;list&quot;,
  &quot;data&quot;: []
}

</code></pre>
<h3 id="upload-files"><a class="header" href="#upload-files">Upload Files</a></h3>
<pre><code class="language-bash">curl https://api.openai.com/v1/files \
  -H &quot;Authorization: Bearer YOUR_API_KEY&quot; \
  -F purpose=&quot;answers&quot; \
  -F file='@puppy.jsonl'

</code></pre>
<h3 id="delete-file"><a class="header" href="#delete-file">Delete File</a></h3>
<pre><code class="language-bash">curl https://api.openai.com/v1/files/file-XjGxS3KTG0uNmNOK362iJua3 \
  -X DELETE \
  -H 'Authorization: Bearer YOUR_API_KEY'

</code></pre>
<h3 id="retrieve-file-information"><a class="header" href="#retrieve-file-information">Retrieve File Information</a></h3>
<pre><code class="language-bash">curl https://api.openai.com/v1/files/file-XjGxS3KTG0uNmNOK362iJua3 \
  -H 'Authorization: Bearer YOUR_API_KEY'

</code></pre>
<h3 id="retrieve-file-content"><a class="header" href="#retrieve-file-content">Retrieve File Content</a></h3>
<pre><code class="language-bash">
curl https://api.openai.com/v1/files/file-XjGxS3KTG0uNmNOK362iJua3/content \
  -H 'Authorization: Bearer YOUR_API_KEY' &gt; file.jsonl


</code></pre>
<h3 id="fine-tunes"><a class="header" href="#fine-tunes">Fine Tunes</a></h3>
<ul>
<li>Manage fine-tuning jobs to tailor a model to your specific training data.</li>
</ul>
<pre><code class="language-bash">curl https://api.openai.com/v1/fine-tunes \
  -X POST \
  -H &quot;Content-Type: application/json&quot; \
  -H &quot;Authorization: Bearer YOUR_API_KEY&quot; \
  -d '{
  &quot;training_file&quot;: &quot;file-XGinujblHPwGLSztz8cPS8XY&quot;
}'

</code></pre>
<h3 id="list-fine-tunes"><a class="header" href="#list-fine-tunes">List fine-tunes</a></h3>
<pre><code class="language-bash">curl https://api.openai.com/v1/fine-tunes \
  -H 'Authorization: Bearer YOUR_API_KEY'

</code></pre>
<h3 id="list-fine-tune-information"><a class="header" href="#list-fine-tune-information">List fine-tune information</a></h3>
<pre><code class="language-bash">curl https://api.openai.com/v1/fine-tunes/ftjob-AF1WoRqd3aJAHsqc9NY7iL8F \
  -H &quot;Authorization: Bearer YOUR_API_KEY&quot;

</code></pre>
<h3 id="cancel-a-fine-tune"><a class="header" href="#cancel-a-fine-tune">Cancel a fine-tune</a></h3>
<pre><code class="language-bash">curl https://api.openai.com/v1/fine-tunes/ftjob-AF1WoRqd3aJAHsqc9NY7iL8F/cancel \
  -X POST \
  -H &quot;Authorization: Bearer YOUR_API_KEY&quot;

</code></pre>
<h3 id="list-fine-tune-events"><a class="header" href="#list-fine-tune-events">List fine-tune events</a></h3>
<pre><code class="language-bash">curl https://api.openai.com/v1/fine-tunes/ftjob-AF1WoRqd3aJAHsqc9NY7iL8F/events \
  -H &quot;Authorization: Bearer YOUR_API_KEY&quot;

</code></pre>
<h3 id="embeddings"><a class="header" href="#embeddings">Embeddings</a></h3>
<ul>
<li>Get a vector representation of a given input that can be easily consumed by machine learning models and algorithms.</li>
</ul>
<pre><code class="language-bash">curl https://api.openai.com/v1/engines/babbage-similarity/embeddings \
  -X POST \
  -H &quot;Authorization: Bearer YOUR_API_KEY&quot; \
  -H &quot;Content-Type: application/json&quot; \
  -d '{&quot;input&quot;: &quot;The food was delicious and the waiter...&quot;}' 

</code></pre>
<pre><code class="language-json">{
  &quot;object&quot;: &quot;list&quot;,
  &quot;data&quot;: [
    {
      &quot;object&quot;: &quot;embedding&quot;,
      &quot;embedding&quot;: [
        0.002866707742214203,
        0.01886799931526184,
        -0.03013569489121437,
        -0.004034548997879028,
        ...
      ]
     &quot;index&quot;: 0
    }
  ],
  &quot;model&quot;: &quot;babbage-similarity-model:2021-09-20&quot;
}

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chat"><a class="header" href="#chat">Chat</a></h1>
<p><img src="https://raw.githubusercontent.com/mohan-chinnappan-n/ml-book-assets/master/openAI/openAI-chat-1.webm.gif" alt="OpenAI chat" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="summarize"><a class="header" href="#summarize">Summarize</a></h1>
<p><img src="https://raw.githubusercontent.com/mohan-chinnappan-n/ml-book-assets/master/openAI/openAI-summarize-1.webm.gif" alt="OpenAI chat" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tldr"><a class="header" href="#tldr">TLDR</a></h1>
<h1 id="translate"><a class="header" href="#translate">Translate</a></h1>
<p><img src="https://raw.githubusercontent.com/mohan-chinnappan-n/ml-book-assets/master/openAI/openAI-tldr-1.webm.gif" alt="OpenAI chat" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="translate-1"><a class="header" href="#translate-1">Translate</a></h1>
<p><img src="https://raw.githubusercontent.com/mohan-chinnappan-n/ml-book-assets/master/openAI/openAI-translate-en-fr-1.webm.gif" alt="OpenAI chat" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="codex-1"><a class="header" href="#codex-1">Codex</a></h1>
<iframe src='https://mohan-chinnappan-n2.github.io/2021/ai/openai/codex/1.html' 
width="860" height="500">
</iframe>
<h2 id="demo-javascript-code-writing-with-codex"><a class="header" href="#demo-javascript-code-writing-with-codex"><a href="https://mohan-chinnappan-n2.github.io/2021/ai/openai/codex/img/1.webm">Demo: Javascript code writing with Codex</a></a></h2>
<h2 id="videos-2"><a class="header" href="#videos-2">Videos</a></h2>
<iframe width="720" height="480" src="https://www.youtube.com/embed/SGUCcjHTmGY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="720" height="480" src="https://www.youtube.com/embed/Zm9B-DvwOgw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Ru5fQZ714x8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>    <div style="break-before: page; page-break-before: always;"></div><h1 id="14-inspirations"><a class="header" href="#14-inspirations">14. Inspirations</a></h1>
<h2 id="chris-lattner"><a class="header" href="#chris-lattner">Chris Lattner</a></h2>
<p><a href="https://en.wikipedia.org/wiki/Chris_Lattner">Chris Lattner</a> talks about working with Steve Jobs, Elon Musk and Jeff Dean:</p>
<ul>
<li>Being OK with not knowing now</li>
<li>Keys is not having right answer, but it is getting the right answer</li>
<li>If you ask a lot of dumb questions you get <strong>smarter</strong> really quick</li>
</ul>
<p>Chris is the main author of <a href="https://llvm.org/">LLVM</a> and related projects such as the <a href="https://clang.llvm.org/">Clang compiler</a> and the <a href="https://docs.swift.org/swift-book/">Swift</a> programming language</p>
<iframe width="720" height="480" src="https://www.youtube.com/embed/5siAT0jmw-Q" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<h2 id="jeff-dean"><a class="header" href="#jeff-dean">Jeff Dean</a></h2>
<iframe width="720" height="480" src="https://www.youtube.com/embed/modXC5IWTJI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<div style="break-before: page; page-break-before: always;"></div><h1 id="15-datasets"><a class="header" href="#15-datasets">15. Datasets</a></h1>
<p>Common Crawl</p>
<p>The Common Crawl corpus contains <strong>petabytes of data collected since 2008</strong>. </p>
<p>It contains raw web page data, extracted metadata and text extractions.</p>
<p><a href="https://commoncrawl.org/the-data/get-started/">Common Crawl</a></p>
<h2 id="example-projects"><a class="header" href="#example-projects">Example Projects</a></h2>
<iframe width="720" height="480" src="https://www.youtube.com/embed/gOT7El8rMws" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<div style="break-before: page; page-break-before: always;"></div><h1 id="boston-housing-dataset"><a class="header" href="#boston-housing-dataset">Boston Housing Dataset</a></h1>
<ul>
<li><a href="https://www.cs.toronto.edu/%7Edelve/data/boston/bostonDetail.html">Boston House prices</a></li>
<li><a href="https://raw.githubusercontent.com/mohan-chinnappan-n/ml-book-assets/master/BostonHousing.csv">Boston House Price Dataset</a></li>
</ul>
<pre><code class="language-py">
import pandas as pd

url=&quot;https://raw.githubusercontent.com/mohan-chinnappan-n/ml-book-assets/master/BostonHousing.csv&quot;
df = pd.read_csv(url)
df.head()

</code></pre>
<p><img src="datasets/img/1/boston-house-price-1.png" alt="Boston House Price" />
<img src="datasets/img/1/boston-house-price-2.png" alt="Boston House Price" /></p>
<p>There are 14 attributes (<strong>features</strong>) in each case of the dataset. They are:</p>
<pre><code>CRIM - per capita crime rate by town
ZN - proportion of residential land zoned for lots over 25,000 sq.ft.
INDUS - proportion of non-retail business acres per town.
CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)
NOX - nitric oxides concentration (parts per 10 million)
RM - average number of rooms per dwelling
AGE - proportion of owner-occupied units built prior to 1940
DIS - weighted distances to five Boston employment centres
RAD - index of accessibility to radial highways
TAX - full-value property-tax rate per $10,000
PTRATIO - pupil-teacher ratio by town
B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
LSTAT - % lower status of the population
MEDV - Median value of owner-occupied homes in $1000's

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="16-building-ml-for-industries"><a class="header" href="#16-building-ml-for-industries">16. Building ML for Industries</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lost-found-item-management"><a class="header" href="#lost-found-item-management">Lost-Found Item Management</a></h1>
<h2 id="problem-description"><a class="header" href="#problem-description">Problem Description</a></h2>
<p>Parcel delivery companies like UPS, FedEx needs a <strong>lost-and-found management</strong> solution:</p>
<p>There is possibility that item(s) packaged by the customers may fall out and becomes a candidate for the lost-and-found item</p>
<h2 id="solution"><a class="header" href="#solution">Solution</a></h2>
<p>We can build a ML and Deep Learning based Image detection and comparison system.</p>
<p><img src="industry/img/lost-found-item-mgmt-1.png" alt="lfm" /></p>
<h3 id="how-this-works"><a class="header" href="#how-this-works">How this works?</a></h3>
<ul>
<li>
<p>We have a database of the images which customers have reported that they have lost</p>
</li>
<li>
<p>At the storage and processing facility, we run those lost-and-found items on a conveyor</p>
</li>
<li>
<p>When the item reaches the Trigger point for scan image capture devices at Reader-L, Reader-R and Reader-T captures images at left, right and top. This may include bar-codes, UPC, QR-Code</p>
</li>
<li>
<p>These images are sent via WiFI to Image Collection, Composer and Processor (ICCP) device</p>
</li>
<li>
<p>ICCP makes use of Vision API and runs:</p>
<ul>
<li>
<p>Image Detection of the composed image (composed out of 3 images received for this item)</p>
</li>
<li>
<p>Image Compare to compare images stored in the Customer cases</p>
</li>
<li>
<p>Stores these attributes</p>
<ul>
<li>ScannedImageUUID</li>
<li>CustomerCaseImageUUID</li>
<li>MatchScore</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Based on the MatchScore we can detect/match the owner of this lost-and-found item</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="17hardware"><a class="header" href="#17hardware">17.Hardware</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="raspberry-pi"><a class="header" href="#raspberry-pi">Raspberry Pi</a></h1>
<p>The Raspberry Pi is a <strong>small computer</strong> that can do <strong>lots of things</strong>. </p>
<p>You plug it into a monitor and attach a keyboard and mouse.</p>
<p><img src="hardware/img/pi-plug-in.gif" alt="Ras Pi" /></p>
<p><img src="hardware/img/pi-labelled-names.png" alt="Ras Pi Labelled" /></p>
<h2 id="tensorflow-lite-on-pi"><a class="header" href="#tensorflow-lite-on-pi">Tensorflow Lite on Pi</a></h2>
<iframe width="720" height="460" src="https://www.youtube.com/embed/aimSGOAUI8Y" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<h2 id="raspberry-pi-emulation-on-macos"><a class="header" href="#raspberry-pi-emulation-on-macos">Raspberry PI emulation on macOS</a></h2>
<pre><code># install info: https://github.com/faf0/macos-qemu-rpi
# https://joshondesign.com/2021/04/15/emu_pi_mac

pi@raspberrypi:~$ uname -a
Linux raspberrypi 5.4.51 #1 Sat Aug 8 23:28:32 +03 2020 armv6l GNU/Linux
pi@raspberrypi:~$ 


</code></pre>
<h2 id="resources-1"><a class="header" href="#resources-1">Resources</a></h2>
<ul>
<li>
<p><a href="https://projects.raspberrypi.org/en/projects/raspberry-pi-getting-started/1">Raspberry Pi Getting Started</a></p>
</li>
<li>
<p><a href="https://florianmuller.com/raspberry-development-environment-on-macosx-with-qemu">Raspberry Development Environment on MacOSX with QEMU</a></p>
</li>
<li>
<p>TensorFlow Lite</p>
<ul>
<li><a href="https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/Raspberry_Pi_Guide.md">Part 2 - How to Run TensorFlow Lite Object Detection Models on the Raspberry Pi</a></li>
<li><a href="https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi">TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi</a></li>
</ul>
</li>
<li>
<p>Cell phone</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=jjX7nS3kIao">Make Your Own Raspberry Pi Cell Phone</a></li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="18-conversational-ai"><a class="header" href="#18-conversational-ai">18. Conversational-AI</a></h1>
<blockquote>
<p>Teaching computers to communicate more like humans and not the other way around - Cathy Pearl - Google Conversation Design Outreach</p>
</blockquote>
<blockquote>
<p>There is no great writing, only great rewriting</p>
</blockquote>
<ul>
<li>
<p>Guide the user to the success by showing them the type of things they are going to be able to say in order for the system to respond.</p>
</li>
<li>
<p>Start with a sample dialog (like movie script) between the user and the  system</p>
</li>
<li>
<p>Flesh out Ideas and make sure you are in the right direction</p>
</li>
<li>
<p>Iterate over this sample dialog to make it perfect</p>
</li>
<li>
<p>Do a <a href="https://www.masterclass.com/articles/what-is-a-table-read-how-to-set-up-a-table-read-including-who-to-invite-and-what-to-provide">table read</a></p>
<ul>
<li>someone play user role</li>
<li>you play the system role</li>
<li>find out the gaps and fix it </li>
</ul>
</li>
<li>
<p>Prepare for:</p>
<ul>
<li>User will say things you did not expect</li>
<li>In human-to-human conversation, we repair the conversations and move forward </li>
<li>So repair things gracefully when user says you did not expect and get them back on track and keep going</li>
</ul>
</li>
</ul>
<blockquote>
<p>User: I have issue a billing issue
System: Provide current options available right now at this point in the conversation </p>
</blockquote>
<ul>
<li>Rapid re-prompt</li>
</ul>
<blockquote>
<p><strong>User</strong>: I like chocolate</p>
</blockquote>
<blockquote>
<p><em>System</em>: You like Milk or Dark chocolate?</p>
</blockquote>
<blockquote>
<p><strong>User</strong>: 65%</p>
</blockquote>
<blockquote>
<p><em>System</em>: Sorry, it was Milk chocolate or Dark chocolate?</p>
</blockquote>
<pre><code>Do not say: I did not get that.
Be more specific and what was the system expecting at that time
and how the user can get back on track

</code></pre>
<h2 id="references-7"><a class="header" href="#references-7">References</a></h2>
<ul>
<li>
<p>Inclusive design is a design process (not restricted to interfaces or technologies) in which a mainstream product, service or environment is designed to be usable by as many people as reasonably possible, without the need for specialized adaptions.</p>
</li>
<li>
<p>Situational impairments</p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/Inclusive_design">Inclusive design</a></p>
</li>
<li>
<p><a href="https://developers.google.com/assistant/conversation-design/welcome">Designing Actions on Google </a></p>
</li>
</ul>
<h2 id="videos-3"><a class="header" href="#videos-3">Videos</a></h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/5vwvyi5UmP8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chatbots"><a class="header" href="#chatbots">Chatbots</a></h1>
<p><img src="img/chatbots/chatbot-1.png" alt="chatbot" /></p>
<p><img src="img/chatbots/df-appt-type-2.png" alt="Demo appointment type-2" /></p>
<ul>
<li>
<p>A chatbot is a <strong>conversational AI system</strong> that is able to communicate with a human in a natural language.</p>
</li>
<li>
<p>It can be integrated into websites, messaging platforms, and devices.</p>
</li>
<li>
<p>Companies can delegate routine tasks to a chatbot</p>
<ul>
<li>which will be able to process <em>multiple user requests simultaneously</em></li>
</ul>
</li>
<li>
<p>Chatbots are <strong>always available</strong> to assist the users and provide huge labor cost savings.</p>
</li>
</ul>
<h2 id="two-groups-of-chatbots"><a class="header" href="#two-groups-of-chatbots">Two groups of chatbots</a></h2>
<ul>
<li>
<p><strong>Rule Based</strong></p>
<ul>
<li>Rely on predefined commands and templates. </li>
<li>Each of these commands should be written by a chatbot developer using regular expressions and textual data analysis</li>
</ul>
</li>
<li>
<p><strong>Data-Driven</strong></p>
<ul>
<li>Rely on machine learning (ML) models pre-trained on dialogue data.</li>
</ul>
</li>
</ul>
<h2 id="main-parts-of-the-chatbot"><a class="header" href="#main-parts-of-the-chatbot">Main parts of the chatbot</a></h2>
<ul>
<li>
<p><strong>Natural Language Understanding (NLU)</strong></p>
<ul>
<li>chatbot needs to understand <strong>utterances</strong> in a natural language</li>
<li>NLU translates a user query from natural language into a <strong>labeled semantic</strong> representation.</li>
<li>Example: The following in English:</li>
</ul>
<pre><code>What is the rental price in Boston?
</code></pre>
<ul>
<li>will be translated into:</li>
</ul>
</li>
</ul>
<pre><code class="language-py">rent_price(&quot;Boston&quot;)
</code></pre>
<ul>
<li>
<p>Then chatbot has to decide what is expected of it</p>
</li>
<li>
<p><strong>Dialogue Manager (DM)</strong></p>
<ul>
<li>keeps track of the dialogue <strong>state</strong> and decides what should be answered to the user. </li>
</ul>
</li>
<li>
<p><strong>Natural Language Generator (NLG)</strong></p>
<ul>
<li>translates a semantic representation back into human language</li>
<li>Example:</li>
</ul>
</li>
</ul>
<pre><code>    rent_price_in_USD(&quot;Boston&quot;) = 2500
</code></pre>
<ul>
<li>will be translated to:</li>
</ul>
<pre><code> The average rent price in Boston is around $2,500

</code></pre>
<p><img src="img/chatbots/chatbot-1.svg" alt="chatbot components" /></p>
<h2 id="let-us-build-a-chatbot"><a class="header" href="#let-us-build-a-chatbot">Let us build a chatbot</a></h2>
<p>The smallest building block of the library is <strong>Component</strong>. 
Component stands for any kind of function in an NLP pipeline. 
It can be implemented as: </p>
<ul>
<li>a neural network</li>
<li>a non-neural ML model </li>
<li>a rule-based system. 
Besides that, Component can have a nested structure, i.e. Component can include other Components.</li>
</ul>
<p>Components can be joined into <strong>Skill</strong>. </p>
<ul>
<li>Skill solves a larger NLP task compared to Component. However, in terms of implementation Skills are not different from Components. </li>
</ul>
<p><strong>Agent</strong> is supposed to be a multi-purpose dialogue system that comprises several Skills and can switch between them. It can be a dialogue system that contains a goal-oriented and chatbot skills and chooses which one to use for generating the answer depending on an user input.</p>
<ul>
<li><a href="https://colab.research.google.com/github/deepmipt/dp_notebooks/blob/master/DP_hello_bot.ipynb">DeepPavlov: Hello bot! </a></li>
</ul>
<h2 id="openai-chat"><a class="header" href="#openai-chat">openAI Chat</a></h2>
<p><img src="https://raw.githubusercontent.com/mohan-chinnappan-n/ml-book-assets/master/openAI/openAI-chat-1.webm.gif" alt="OpenAI chat" /></p>
<h2 id="references-8"><a class="header" href="#references-8">References</a></h2>
<ul>
<li>Einstein
<ul>
<li><a href="https://help.salesforce.com/s/articleView?id=sf.bots_service_intro.htm&amp;type=5">Einstein Bots</a></li>
<li><a href="https://help.salesforce.com/s/articleView?id=sf.bots_service_deploy_to_channels.htm&amp;type=5">Deploy Your Bot to Your Channels</a></li>
</ul>
</li>
<li>Google Contact Center AI
<ul>
<li><a href="https://cloud.google.com/dialogflow">Google Dialogflow</a></li>
</ul>
</li>
<li>DEEPPAVLOV
<ul>
<li><a href="https://github.com/deepmipt/dp_notebooks">DeepPavlov articles with Python code</a></li>
</ul>
</li>
<li>RASA
<ul>
<li><a href="https://rasa.com/">RASA: The Future of Customer Experience</a></li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="einstein-bots-1"><a class="header" href="#einstein-bots-1">Einstein Bots</a></h1>
<h2 id="simple-setup"><a class="header" href="#simple-setup">Simple setup</a></h2>
<p>Licenses required</p>
<ul>
<li>
<p>Service Cloud</p>
</li>
<li>
<p>A Chat license</p>
</li>
<li>
<p>Enable Lightning Experience</p>
</li>
<li>
<p>Click the toggle on the Einstein Bots Setup page</p>
</li>
<li>
<p>Chat API Endpoint</p>
</li>
</ul>
<pre><code>https://d.la4-c1-ia4.salesforceliveagent.com/chat/rest/

</code></pre>
<ul>
<li>
<p>Entities</p>
<ul>
<li>the bot to ask a question and then store the answer in a variable to use later. But first we need to set up an entity.</li>
</ul>
</li>
<li>
<p>Variable</p>
<ul>
<li>A variable is a container that stores a specific piece of data collected from the customer. </li>
</ul>
</li>
</ul>
<h2 id="demo"><a class="header" href="#demo">Demo</a></h2>
<p><img src="img/chatbots/einstein-bot-demo-1.webm.gif" alt="Einstein Bot Demo" /></p>
<h2 id="appointment-scheduler"><a class="header" href="#appointment-scheduler">Appointment Scheduler</a></h2>
<h3 id="flow"><a class="header" href="#flow">Flow</a></h3>
<pre><code>
flowchart TB
    O[Appointment Menu] --&gt;A[Schedule Appointment] --&gt; B[What time you like to set up the appointment?]
    O --&gt;Z[Transfer To Agent] 
    B --&gt; C{Get Appointment Date and Time}
    C --&gt; D[What type of appointment?] 
    D --&gt;  E{Get Appointment Type}
    E --&gt;  F[What type of car you have?]
    F --&gt;  G{Get Car Type}
    G --&gt;  H[You are all set for your appointment \non ApptDateTime \nfor appointmentType for your car Car_Type]

</code></pre>
<h2 id="proposed-bot-def"><a class="header" href="#proposed-bot-def">Proposed Bot Def</a></h2>
<pre><code class="language-yaml">
name: Kovai
ver: 1.0.0
description: Bot able to provide appointment management and take actions at the CRM system
menus:
  mainMenu: 
    - Appointment Booking
    - Transfer to an Agent
  AppointMenu:
    - question: What time and date you like to book the appointment?
        answer: 
          - Tomorrow 10 am
          - Tuesday 11 am
          - Next week Wednesday 12 noon
        entities:
           type:  sys.DateTime
           name: $AppointmentDateTime

    - question: What type of appointment type you need?
        menu:
          - Car Inspection
          - Car Maintenance
        entities:
          type: String
          name: $AppointmentType 
    - question: What type of car you have?
        menu:
          - Ford F-150
          - Ford Explorer
          - Toyota Camry
          - Toyota Corolla
        entities:
          type: String
          name :$CarType

    - response: |-
                 You are all set with your appointment on $AppointmentDateTime
                for $AppointmentType  for your $CarType


</code></pre>
<p><img src="img/chatbots/eb-appointment-sched.svg" alt="Flow" /></p>
<h3 id="setup"><a class="header" href="#setup">Setup</a></h3>
<p><img src="img/chatbots/eb-appt-setup-1.webm.gif" alt="Einstein bot appointment scheduler setup" /></p>
<h3 id="demo-1"><a class="header" href="#demo-1">Demo</a></h3>
<p><img src="img/chatbots/eb-appt-demo-1.webm.gif" alt="Einstein bot appointment scheduler demo" /></p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/3wDpNNMu6Y0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<h2 id="channels-supported"><a class="header" href="#channels-supported">Channels supported</a></h2>
<p>Einstein Bots support the following channels: </p>
<ul>
<li>Chat (In-App and Web) </li>
<li>Messaging 
<ul>
<li>SMS</li>
<li>Facebook Messenger</li>
<li>WhatsApp channels </li>
</ul>
</li>
</ul>
<h2 id="bot-metadata"><a class="header" href="#bot-metadata">Bot <a href="https://github.com/mohan-chinnappan-n/cli-dx/blob/master/metadata/Bot/bot.md">metadata</a></a></h2>
<pre><code>sfdx mohanc:mdapi:list -u mohan.chinnappan.n_ea2@gmail.com -t Bot
</code></pre>
<pre><code>{
  result: [
    {
      createdById: '0053h000002xQ5sAAE',
      createdByName: 'Mohan Chinnappan',
      createdDate: 2021-06-03T01:01:58.000Z,
      fileName: 'bots/kovai.bot',
      fullName: 'kovai',
      id: '0Xx3h000000H4T0CAK',
      lastModifiedById: '0053h000002xQ5sAAE',
      lastModifiedByName: 'Mohan Chinnappan',
      lastModifiedDate: 2021-06-03T01:01:58.000Z,
      type: 'Bot'
    }
  ]
}

</code></pre>
<pre><code>
sfdx mohanc:mdapi:retrieve -u mohan.chinnappan.n_ea2@gmail.com -t Bot
</code></pre>
<pre><code>{
    &quot;RetrieveRequest&quot;: {
        &quot;apiVersion&quot;: &quot;53.0&quot;,
        &quot;unpackaged&quot;: [
            {
                &quot;types&quot;: {
                    &quot;members&quot;: &quot;*&quot;,
                    &quot;name&quot;: &quot;Bot&quot;
                }
            }
        ]
    }
}
{ result: { done: false, id: '09S3h000005rNsXEAU', state: 'Queued' } }
</code></pre>
<pre><code> sfdx mohanc:mdapi:checkRetrieveStatus  -u mohan.chinnappan.n_ea2@gmail.com -i 09S3h000005rNsXEAU 
</code></pre>
<pre><code>[
  {
    createdById: '0053h000002xQ5sAAE',
    createdByName: 'Mohan Chinnappan',
    createdDate: 2021-06-03T01:01:58.000Z,
    fileName: 'unpackaged/bots/kovai.bot',
    fullName: 'kovai',
    id: '0Xx3h000000H4T0CAK',
    lastModifiedById: '0053h000002xQ5sAAE',
    lastModifiedByName: 'Mohan Chinnappan',
    lastModifiedDate: 2021-06-03T01:01:58.000Z,
    type: 'Bot'
  },
  {
    createdById: '0053h000002xQ5sAAE',
    createdByName: 'Mohan Chinnappan',
    createdDate: 2022-01-03T20:43:32.647Z,
    fileName: 'unpackaged/package.xml',
    fullName: 'unpackaged/package.xml',
    id: '',
    lastModifiedById: '0053h000002xQ5sAAE',
    lastModifiedByName: 'Mohan Chinnappan',
    lastModifiedDate: 2022-01-03T20:43:32.647Z,
    manageableState: 'unmanaged',
    type: 'Package'
  }
]
=== Writing zipFile base64 content to 09S3h000005rNsXEAU.zip.txt ...
=== Writing zipFile binary content to 09S3h000005rNsXEAU.zip ... 


</code></pre>
<pre><code>unzip 09S3h000005rNsXEAU.zip
Archive:  09S3h000005rNsXEAU.zip
  inflating: unpackaged/bots/kovai.bot  
  inflating: unpackaged/package.xml  

</code></pre>
<pre><code>├── 09S3h000005rNsXEAU.zip
├── 09S3h000005rNsXEAU.zip.txt
└── unpackaged
    ├── bots
    │   └── kovai.bot
    └── package.xml

</code></pre>
<pre><code class="language-xml">cat unpackaged/bots/kovai.bot 
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;Bot xmlns=&quot;http://soap.sforce.com/2006/04/metadata&quot;&gt;
    &lt;botMlDomain&gt;
        &lt;label&gt;kovai&lt;/label&gt;
        &lt;mlIntents&gt;
            &lt;developerName&gt;Confused&lt;/developerName&gt;
            &lt;label&gt;Confused&lt;/label&gt;
        &lt;/mlIntents&gt;
        &lt;mlIntents&gt;
            &lt;developerName&gt;Transfer_To_Agent&lt;/developerName&gt;
            &lt;label&gt;Transfer To Agent&lt;/label&gt;
        &lt;/mlIntents&gt;
        &lt;mlSlotClasses&gt;
            &lt;dataType&gt;Text&lt;/dataType&gt;
            &lt;developerName&gt;appointment_type&lt;/developerName&gt;
            &lt;extractionRegex&gt;.*&lt;/extractionRegex&gt;
            &lt;extractionType&gt;Pattern&lt;/extractionType&gt;
            &lt;label&gt;appointment type&lt;/label&gt;
        &lt;/mlSlotClasses&gt;
        &lt;mlSlotClasses&gt;
            &lt;dataType&gt;Text&lt;/dataType&gt;
            &lt;developerName&gt;Car_Type&lt;/developerName&gt;
            &lt;extractionRegex&gt;.*&lt;/extractionRegex&gt;
            &lt;extractionType&gt;Pattern&lt;/extractionType&gt;
            &lt;label&gt;Car Type&lt;/label&gt;
        &lt;/mlSlotClasses&gt;
        &lt;mlSlotClasses&gt;
            &lt;dataType&gt;Text&lt;/dataType&gt;
            &lt;description&gt;Memory size&lt;/description&gt;
            &lt;developerName&gt;Memory_size&lt;/developerName&gt;
            &lt;extractionRegex&gt;[1-9][0-9]*&lt;/extractionRegex&gt;
            &lt;extractionType&gt;Pattern&lt;/extractionType&gt;
            &lt;label&gt;Memory size&lt;/label&gt;
        &lt;/mlSlotClasses&gt;
        &lt;name&gt;kovai&lt;/name&gt;
    &lt;/botMlDomain&gt;
    &lt;botVersions&gt;
        &lt;fullName&gt;v1&lt;/fullName&gt;
        &lt;botDialogs&gt;
            &lt;botSteps&gt;
                &lt;conversationRecordLookup&gt;
                    &lt;SObjectType&gt;Contact&lt;/SObjectType&gt;
                    &lt;conditions&gt;
                        &lt;leftOperand&gt;Contact.LastName&lt;/leftOperand&gt;
                        &lt;operatorType&gt;Contains&lt;/operatorType&gt;
                        &lt;rightOperandName&gt;LastName&lt;/rightOperandName&gt;
                        &lt;rightOperandType&gt;ConversationVariable&lt;/rightOperandType&gt;
                        &lt;sortOrder&gt;1&lt;/sortOrder&gt;
                    &lt;/conditions&gt;
                    &lt;filterLogic&gt;And&lt;/filterLogic&gt;
                    &lt;lookupFields&gt;
                        &lt;fieldName&gt;Contact.Birthdate&lt;/fieldName&gt;
                    &lt;/lookupFields&gt;
                    &lt;maxLookupResults&gt;3&lt;/maxLookupResults&gt;
                    &lt;targetVariableName&gt;ListVar&lt;/targetVariableName&gt;
                &lt;/conversationRecordLookup&gt;
                &lt;stepIdentifier&gt;33b7e5b2-e600-449d-ad6c-977980f017fd&lt;/stepIdentifier&gt;
                &lt;type&gt;RecordLookup&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;botSteps&gt;
                &lt;stepIdentifier&gt;7bee15be-3454-49a9-a954-da9ec6d8ca1e&lt;/stepIdentifier&gt;
                &lt;type&gt;Wait&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;developerName&gt;Check_My_Order&lt;/developerName&gt;
            &lt;label&gt;Find  Birth Date&lt;/label&gt;
            &lt;showInFooterMenu&gt;false&lt;/showInFooterMenu&gt;
        &lt;/botDialogs&gt;
        &lt;botDialogs&gt;
            &lt;botSteps&gt;
                &lt;botMessages&gt;
                    &lt;message&gt;We will fix it soon!&lt;/message&gt;
                    &lt;messageIdentifier&gt;9f7211b0-141a-4ac8-a15a-da405e3df903&lt;/messageIdentifier&gt;
                &lt;/botMessages&gt;
                &lt;stepIdentifier&gt;bc4b5ba5-d5d7-4e49-a3ea-e181d6b98aa8&lt;/stepIdentifier&gt;
                &lt;type&gt;Message&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;botSteps&gt;
                &lt;stepIdentifier&gt;6d4435bf-c2cb-4dda-a0f9-9915213e766e&lt;/stepIdentifier&gt;
                &lt;type&gt;Wait&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;developerName&gt;Printer_not_working&lt;/developerName&gt;
            &lt;label&gt;Printer not working&lt;/label&gt;
            &lt;showInFooterMenu&gt;false&lt;/showInFooterMenu&gt;
        &lt;/botDialogs&gt;
        &lt;botDialogs&gt;
            &lt;botSteps&gt;
                &lt;botVariableOperation&gt;
                    &lt;askCollectIfSet&gt;false&lt;/askCollectIfSet&gt;
                    &lt;botMessages&gt;
                        &lt;message&gt;How much memory&lt;/message&gt;
                        &lt;messageIdentifier&gt;ad3b8a35-52b2-4dda-b19d-d427c121bc21&lt;/messageIdentifier&gt;
                    &lt;/botMessages&gt;
                    &lt;botQuickReplyOptions&gt;
                        &lt;literalValue&gt;6 GB&lt;/literalValue&gt;
                        &lt;quickReplyOptionIdentifier&gt;be9aba98-d1ad-43a8-8919-8ff93dcc653a&lt;/quickReplyOptionIdentifier&gt;
                    &lt;/botQuickReplyOptions&gt;
                    &lt;botQuickReplyOptions&gt;
                        &lt;literalValue&gt;12 GB&lt;/literalValue&gt;
                        &lt;quickReplyOptionIdentifier&gt;09772c46-ac87-4606-8dfc-6151be0f7392&lt;/quickReplyOptionIdentifier&gt;
                    &lt;/botQuickReplyOptions&gt;
                    &lt;botQuickReplyOptions&gt;
                        &lt;literalValue&gt;16 GB&lt;/literalValue&gt;
                        &lt;quickReplyOptionIdentifier&gt;763e8f95-937d-4d6a-b293-b5efdbcd6e39&lt;/quickReplyOptionIdentifier&gt;
                    &lt;/botQuickReplyOptions&gt;
                    &lt;botQuickReplyOptions&gt;
                        &lt;literalValue&gt;32 GB&lt;/literalValue&gt;
                        &lt;quickReplyOptionIdentifier&gt;f6155fa2-9a94-4062-a58f-3480bb49dd64&lt;/quickReplyOptionIdentifier&gt;
                    &lt;/botQuickReplyOptions&gt;
                    &lt;botVariableOperands&gt;
                        &lt;disableAutoFill&gt;false&lt;/disableAutoFill&gt;
                        &lt;sourceName&gt;Memory_size&lt;/sourceName&gt;
                        &lt;sourceType&gt;MlSlotClass&lt;/sourceType&gt;
                        &lt;targetName&gt;Memory_Size&lt;/targetName&gt;
                        &lt;targetType&gt;ConversationVariable&lt;/targetType&gt;
                    &lt;/botVariableOperands&gt;
                    &lt;optionalCollect&gt;false&lt;/optionalCollect&gt;
                    &lt;quickReplyType&gt;Static&lt;/quickReplyType&gt;
                    &lt;quickReplyWidgetType&gt;Buttons&lt;/quickReplyWidgetType&gt;
                    &lt;retryMessages&gt;
                        &lt;message&gt;Please provide memory size in GB&lt;/message&gt;
                        &lt;messageIdentifier&gt;cde1f4ca-5d67-4792-ac54-a6ae49ff7f99&lt;/messageIdentifier&gt;
                    &lt;/retryMessages&gt;
                    &lt;type&gt;Collect&lt;/type&gt;
                    &lt;variableOperationIdentifier&gt;4c5b98ee-95ac-4d5d-9700-95cfa31c269e&lt;/variableOperationIdentifier&gt;
                &lt;/botVariableOperation&gt;
                &lt;stepIdentifier&gt;3bb7b606-69ec-4d2c-9616-8bb1419efc5a&lt;/stepIdentifier&gt;
                &lt;type&gt;VariableOperation&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;botSteps&gt;
                &lt;conversationSystemMessage&gt;
                    &lt;type&gt;Transfer&lt;/type&gt;
                &lt;/conversationSystemMessage&gt;
                &lt;stepIdentifier&gt;b299f386-1ff0-4f00-8530-7baf68a7b059&lt;/stepIdentifier&gt;
                &lt;type&gt;SystemMessage&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;developerName&gt;Find_a_MacBook_Pro&lt;/developerName&gt;
            &lt;label&gt;Find a MacBook Pro&lt;/label&gt;
            &lt;showInFooterMenu&gt;false&lt;/showInFooterMenu&gt;
        &lt;/botDialogs&gt;
        &lt;botDialogs&gt;
            &lt;botSteps&gt;
                &lt;botMessages&gt;
                    &lt;message&gt;Hi, I’m &amp;apos;kovai&amp;apos;,  your digital assistant.&lt;/message&gt;
                    &lt;messageIdentifier&gt;2426b915-4efd-426d-be46-c45713d7ed44&lt;/messageIdentifier&gt;
                &lt;/botMessages&gt;
                &lt;stepIdentifier&gt;c419550f-92e8-47d4-a127-9f0aaf0507df&lt;/stepIdentifier&gt;
                &lt;type&gt;Message&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;botSteps&gt;
                &lt;botMessages&gt;
                    &lt;message&gt;Let me offer you options&lt;/message&gt;
                    &lt;messageIdentifier&gt;6b789c42-4d45-4b96-84ab-2971cfcf68a8&lt;/messageIdentifier&gt;
                &lt;/botMessages&gt;
                &lt;stepIdentifier&gt;ce9bfdd6-7993-48fd-abf4-f87b3d7efcf7&lt;/stepIdentifier&gt;
                &lt;type&gt;Message&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;botSteps&gt;
                &lt;botNavigation&gt;
                    &lt;botNavigationLinks&gt;
                        &lt;targetBotDialog&gt;Main_Menu&lt;/targetBotDialog&gt;
                    &lt;/botNavigationLinks&gt;
                    &lt;type&gt;Redirect&lt;/type&gt;
                &lt;/botNavigation&gt;
                &lt;stepIdentifier&gt;85da64be-da77-4965-b978-2fa8488d758b&lt;/stepIdentifier&gt;
                &lt;type&gt;Navigation&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;developerName&gt;Welcome&lt;/developerName&gt;
            &lt;label&gt;Welcome&lt;/label&gt;
            &lt;showInFooterMenu&gt;false&lt;/showInFooterMenu&gt;
        &lt;/botDialogs&gt;
        &lt;botDialogs&gt;
            &lt;botSteps&gt;
                &lt;botNavigation&gt;
                    &lt;botNavigationLinks&gt;
                        &lt;label&gt;Check My Order&lt;/label&gt;
                        &lt;targetBotDialog&gt;Check_My_Order&lt;/targetBotDialog&gt;
                    &lt;/botNavigationLinks&gt;
                    &lt;botNavigationLinks&gt;
                        &lt;label&gt;Printer not working&lt;/label&gt;
                        &lt;targetBotDialog&gt;Printer_not_working&lt;/targetBotDialog&gt;
                    &lt;/botNavigationLinks&gt;
                    &lt;botNavigationLinks&gt;
                        &lt;label&gt;Find a MacBook Pro&lt;/label&gt;
                        &lt;targetBotDialog&gt;Find_a_MacBook_Pro&lt;/targetBotDialog&gt;
                    &lt;/botNavigationLinks&gt;
                    &lt;type&gt;Redirect&lt;/type&gt;
                &lt;/botNavigation&gt;
                &lt;stepIdentifier&gt;3d40fef7-a514-4c68-957b-ba2f3f9f0776&lt;/stepIdentifier&gt;
                &lt;type&gt;Navigation&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;developerName&gt;Main_Menu&lt;/developerName&gt;
            &lt;label&gt;Main Menu&lt;/label&gt;
            &lt;showInFooterMenu&gt;true&lt;/showInFooterMenu&gt;
        &lt;/botDialogs&gt;
        &lt;botDialogs&gt;
            &lt;botSteps&gt;
                &lt;botMessages&gt;
                    &lt;message&gt;Transferring to the  Agent&lt;/message&gt;
                    &lt;messageIdentifier&gt;c76163dc-a4f3-47f5-890e-17de71f73006&lt;/messageIdentifier&gt;
                &lt;/botMessages&gt;
                &lt;stepIdentifier&gt;31b86047-f3e8-4eb5-9595-e41d45588cf5&lt;/stepIdentifier&gt;
                &lt;type&gt;Message&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;botSteps&gt;
                &lt;conversationSystemMessage&gt;
                    &lt;type&gt;Transfer&lt;/type&gt;
                &lt;/conversationSystemMessage&gt;
                &lt;stepIdentifier&gt;17a17172-c762-427d-ab07-ec23001781c6&lt;/stepIdentifier&gt;
                &lt;type&gt;SystemMessage&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;developerName&gt;Transfer_To_Agent&lt;/developerName&gt;
            &lt;label&gt;Transfer To Agent&lt;/label&gt;
            &lt;mlIntent&gt;Transfer_To_Agent&lt;/mlIntent&gt;
            &lt;mlIntentTrainingEnabled&gt;false&lt;/mlIntentTrainingEnabled&gt;
            &lt;showInFooterMenu&gt;true&lt;/showInFooterMenu&gt;
        &lt;/botDialogs&gt;
        &lt;botDialogs&gt;
            &lt;botSteps&gt;
                &lt;botMessages&gt;
                    &lt;message&gt;Goodbye! Click the &amp;quot;End Chat&amp;quot; button to end this chat&lt;/message&gt;
                    &lt;messageIdentifier&gt;c11e71d1-bdcf-45c6-a8e4-cc7c5b97d134&lt;/messageIdentifier&gt;
                &lt;/botMessages&gt;
                &lt;stepIdentifier&gt;1c06aad9-d63c-4e47-88e4-58f3af6b858e&lt;/stepIdentifier&gt;
                &lt;type&gt;Message&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;botSteps&gt;
                &lt;stepIdentifier&gt;fd33a42e-c466-4f9e-8c5e-ad53c3bdf81a&lt;/stepIdentifier&gt;
                &lt;type&gt;Wait&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;developerName&gt;End_Chat&lt;/developerName&gt;
            &lt;label&gt;End Chat&lt;/label&gt;
            &lt;showInFooterMenu&gt;false&lt;/showInFooterMenu&gt;
        &lt;/botDialogs&gt;
        &lt;botDialogs&gt;
            &lt;botSteps&gt;
                &lt;botMessages&gt;
                    &lt;message&gt;Sorry, I didn&amp;apos;t understand the your request&lt;/message&gt;
                    &lt;messageIdentifier&gt;653aa7c9-d205-4a52-96b8-543efd760d96&lt;/messageIdentifier&gt;
                &lt;/botMessages&gt;
                &lt;stepIdentifier&gt;26067502-7f1d-4bed-803f-43ca26c0ba63&lt;/stepIdentifier&gt;
                &lt;type&gt;Message&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;botSteps&gt;
                &lt;stepIdentifier&gt;51dab797-c78f-41d0-a7b8-c30497a11446&lt;/stepIdentifier&gt;
                &lt;type&gt;Wait&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;developerName&gt;Confused&lt;/developerName&gt;
            &lt;label&gt;Confused&lt;/label&gt;
            &lt;mlIntent&gt;Confused&lt;/mlIntent&gt;
            &lt;mlIntentTrainingEnabled&gt;false&lt;/mlIntentTrainingEnabled&gt;
            &lt;showInFooterMenu&gt;false&lt;/showInFooterMenu&gt;
        &lt;/botDialogs&gt;
        &lt;botDialogs&gt;
            &lt;botSteps&gt;
                &lt;botMessages&gt;
                    &lt;message&gt;Unfortunately, there are no agents available at the moment&lt;/message&gt;
                    &lt;messageIdentifier&gt;8813055e-c518-4dbe-b430-ae02c76e6bf0&lt;/messageIdentifier&gt;
                &lt;/botMessages&gt;
                &lt;stepIdentifier&gt;4cd74b3f-7fa5-466b-89f5-a37be3ee8794&lt;/stepIdentifier&gt;
                &lt;type&gt;Message&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;botSteps&gt;
                &lt;stepIdentifier&gt;e15e7024-72e0-4656-a05e-15e0906bec8e&lt;/stepIdentifier&gt;
                &lt;type&gt;Wait&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;developerName&gt;No_Agent_Available&lt;/developerName&gt;
            &lt;label&gt;No Agent&lt;/label&gt;
            &lt;showInFooterMenu&gt;false&lt;/showInFooterMenu&gt;
        &lt;/botDialogs&gt;
        &lt;botDialogs&gt;
            &lt;botSteps&gt;
                &lt;botMessages&gt;
                    &lt;message&gt;Unfortunately, a system error occurred. I&amp;apos;ll connect you to an agent who can help.&lt;/message&gt;
                    &lt;messageIdentifier&gt;6bff781a-27bd-4fd3-9a5e-545d54313bd0&lt;/messageIdentifier&gt;
                &lt;/botMessages&gt;
                &lt;stepIdentifier&gt;390b8657-1e1d-4d5d-af4f-1ca00ca093db&lt;/stepIdentifier&gt;
                &lt;type&gt;Message&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;botSteps&gt;
                &lt;conversationSystemMessage&gt;
                    &lt;type&gt;Transfer&lt;/type&gt;
                &lt;/conversationSystemMessage&gt;
                &lt;stepIdentifier&gt;2cd2ecb8-e5d6-490a-a329-8fdd62e5e3ec&lt;/stepIdentifier&gt;
                &lt;type&gt;SystemMessage&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;developerName&gt;Error_Handling&lt;/developerName&gt;
            &lt;label&gt;Error Handler&lt;/label&gt;
            &lt;showInFooterMenu&gt;false&lt;/showInFooterMenu&gt;
        &lt;/botDialogs&gt;
        &lt;botDialogs&gt;
            &lt;botSteps&gt;
                &lt;botVariableOperation&gt;
                    &lt;askCollectIfSet&gt;false&lt;/askCollectIfSet&gt;
                    &lt;botMessages&gt;
                        &lt;message&gt;What time you like to set up the appointment?&lt;/message&gt;
                        &lt;messageIdentifier&gt;db1c8f72-5d22-e3ac-d940-a9f7cebc43da&lt;/messageIdentifier&gt;
                    &lt;/botMessages&gt;
                    &lt;botVariableOperands&gt;
                        &lt;disableAutoFill&gt;false&lt;/disableAutoFill&gt;
                        &lt;sourceName&gt;_DateTime&lt;/sourceName&gt;
                        &lt;sourceType&gt;StandardMlSlotClass&lt;/sourceType&gt;
                        &lt;targetName&gt;ApptDateTime&lt;/targetName&gt;
                        &lt;targetType&gt;ConversationVariable&lt;/targetType&gt;
                    &lt;/botVariableOperands&gt;
                    &lt;optionalCollect&gt;false&lt;/optionalCollect&gt;
                    &lt;quickReplyWidgetType&gt;Buttons&lt;/quickReplyWidgetType&gt;
                    &lt;type&gt;Collect&lt;/type&gt;
                    &lt;variableOperationIdentifier&gt;e23fa3b7-c854-1169-4ec7-030cb091a885&lt;/variableOperationIdentifier&gt;
                &lt;/botVariableOperation&gt;
                &lt;stepIdentifier&gt;b1196beb-46f1-4dcc-b4aa-7486ae4cfd17&lt;/stepIdentifier&gt;
                &lt;type&gt;VariableOperation&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;botSteps&gt;
                &lt;botNavigation&gt;
                    &lt;botNavigationLinks&gt;
                        &lt;targetBotDialog&gt;Ask_for_appointment_type&lt;/targetBotDialog&gt;
                    &lt;/botNavigationLinks&gt;
                    &lt;type&gt;Redirect&lt;/type&gt;
                &lt;/botNavigation&gt;
                &lt;stepIdentifier&gt;3493f124-9a0d-4ac5-965b-d2cccdff72b1&lt;/stepIdentifier&gt;
                &lt;type&gt;Navigation&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;developerName&gt;Schedule_Appointment&lt;/developerName&gt;
            &lt;label&gt;Schedule Appointment&lt;/label&gt;
            &lt;showInFooterMenu&gt;true&lt;/showInFooterMenu&gt;
        &lt;/botDialogs&gt;
        &lt;botDialogs&gt;
            &lt;botSteps&gt;
                &lt;botVariableOperation&gt;
                    &lt;askCollectIfSet&gt;false&lt;/askCollectIfSet&gt;
                    &lt;botMessages&gt;
                        &lt;message&gt;What type of appointment&lt;/message&gt;
                        &lt;messageIdentifier&gt;4bb7bab9-7ffb-6834-58d4-95fe42586d2d&lt;/messageIdentifier&gt;
                    &lt;/botMessages&gt;
                    &lt;botQuickReplyOptions&gt;
                        &lt;literalValue&gt;Car Inspection&lt;/literalValue&gt;
                        &lt;quickReplyOptionIdentifier&gt;01fca43c-4167-8ce0-4064-ea6014f6d1e4&lt;/quickReplyOptionIdentifier&gt;
                    &lt;/botQuickReplyOptions&gt;
                    &lt;botQuickReplyOptions&gt;
                        &lt;literalValue&gt;Car Maintenance&lt;/literalValue&gt;
                        &lt;quickReplyOptionIdentifier&gt;95203cce-8c74-11ad-04e9-3a7dfed1f24f&lt;/quickReplyOptionIdentifier&gt;
                    &lt;/botQuickReplyOptions&gt;
                    &lt;botVariableOperands&gt;
                        &lt;disableAutoFill&gt;true&lt;/disableAutoFill&gt;
                        &lt;sourceName&gt;appointment_type&lt;/sourceName&gt;
                        &lt;sourceType&gt;MlSlotClass&lt;/sourceType&gt;
                        &lt;targetName&gt;appointmentType&lt;/targetName&gt;
                        &lt;targetType&gt;ConversationVariable&lt;/targetType&gt;
                    &lt;/botVariableOperands&gt;
                    &lt;optionalCollect&gt;false&lt;/optionalCollect&gt;
                    &lt;quickReplyType&gt;Static&lt;/quickReplyType&gt;
                    &lt;quickReplyWidgetType&gt;Buttons&lt;/quickReplyWidgetType&gt;
                    &lt;type&gt;Collect&lt;/type&gt;
                    &lt;variableOperationIdentifier&gt;dcba7bf3-682d-6529-5dc3-20edebeaaf35&lt;/variableOperationIdentifier&gt;
                &lt;/botVariableOperation&gt;
                &lt;stepIdentifier&gt;7d685414-e474-4bb2-8d9f-0aa9ea3424ea&lt;/stepIdentifier&gt;
                &lt;type&gt;VariableOperation&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;botSteps&gt;
                &lt;botNavigation&gt;
                    &lt;botNavigationLinks&gt;
                        &lt;targetBotDialog&gt;Ask_for_car_type&lt;/targetBotDialog&gt;
                    &lt;/botNavigationLinks&gt;
                    &lt;type&gt;Redirect&lt;/type&gt;
                &lt;/botNavigation&gt;
                &lt;stepIdentifier&gt;d2c2c402-d027-47ef-b281-30f705cc2977&lt;/stepIdentifier&gt;
                &lt;type&gt;Navigation&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;developerName&gt;Ask_for_appointment_type&lt;/developerName&gt;
            &lt;label&gt;Ask for appointment type&lt;/label&gt;
            &lt;showInFooterMenu&gt;false&lt;/showInFooterMenu&gt;
        &lt;/botDialogs&gt;
        &lt;botDialogs&gt;
            &lt;botSteps&gt;
                &lt;botMessages&gt;
                    &lt;message&gt;You are all set for your appointment on {!ApptDateTime} for {!appointmentType} for your car {!Car_Type}&lt;/message&gt;
                    &lt;messageIdentifier&gt;118272b0-83ad-e8d8-0de0-92daa0ec9fc9&lt;/messageIdentifier&gt;
                &lt;/botMessages&gt;
                &lt;stepIdentifier&gt;141c3b77-466e-4f34-b347-f3a5805e7350&lt;/stepIdentifier&gt;
                &lt;type&gt;Message&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;botSteps&gt;
                &lt;stepIdentifier&gt;947d7297-6c8a-4e5b-a8a5-44d7a847d316&lt;/stepIdentifier&gt;
                &lt;type&gt;Wait&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;developerName&gt;Confirm_appointment&lt;/developerName&gt;
            &lt;label&gt;Confirm appointment&lt;/label&gt;
            &lt;showInFooterMenu&gt;false&lt;/showInFooterMenu&gt;
        &lt;/botDialogs&gt;
        &lt;botDialogs&gt;
            &lt;botSteps&gt;
                &lt;botNavigation&gt;
                    &lt;botNavigationLinks&gt;
                        &lt;targetBotDialog&gt;Schedule_Appointment&lt;/targetBotDialog&gt;
                    &lt;/botNavigationLinks&gt;
                    &lt;botNavigationLinks&gt;
                        &lt;targetBotDialog&gt;Transfer_To_Agent&lt;/targetBotDialog&gt;
                    &lt;/botNavigationLinks&gt;
                    &lt;type&gt;Redirect&lt;/type&gt;
                &lt;/botNavigation&gt;
                &lt;stepIdentifier&gt;dbb09296-4a74-4618-8373-1379fc37041d&lt;/stepIdentifier&gt;
                &lt;type&gt;Navigation&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;developerName&gt;Appointment_Menu&lt;/developerName&gt;
            &lt;label&gt;Appointment Menu&lt;/label&gt;
            &lt;showInFooterMenu&gt;true&lt;/showInFooterMenu&gt;
        &lt;/botDialogs&gt;
        &lt;botDialogs&gt;
            &lt;botSteps&gt;
                &lt;botVariableOperation&gt;
                    &lt;askCollectIfSet&gt;false&lt;/askCollectIfSet&gt;
                    &lt;botMessages&gt;
                        &lt;message&gt;What type of car you have?&lt;/message&gt;
                        &lt;messageIdentifier&gt;d3eb1be4-a229-eb2f-9f53-17970b8fbd11&lt;/messageIdentifier&gt;
                    &lt;/botMessages&gt;
                    &lt;botQuickReplyOptions&gt;
                        &lt;literalValue&gt;Toyota: Camry&lt;/literalValue&gt;
                        &lt;quickReplyOptionIdentifier&gt;09a8d86c-d225-c99b-148b-e0cd9604c334&lt;/quickReplyOptionIdentifier&gt;
                    &lt;/botQuickReplyOptions&gt;
                    &lt;botQuickReplyOptions&gt;
                        &lt;literalValue&gt;Toyota: 4Runner&lt;/literalValue&gt;
                        &lt;quickReplyOptionIdentifier&gt;d8a40b69-c712-c35d-853d-beeeca9f08ed&lt;/quickReplyOptionIdentifier&gt;
                    &lt;/botQuickReplyOptions&gt;
                    &lt;botQuickReplyOptions&gt;
                        &lt;literalValue&gt;Ford: Mustang&lt;/literalValue&gt;
                        &lt;quickReplyOptionIdentifier&gt;82d97836-9857-0db5-af32-708cbb114586&lt;/quickReplyOptionIdentifier&gt;
                    &lt;/botQuickReplyOptions&gt;
                    &lt;botQuickReplyOptions&gt;
                        &lt;literalValue&gt;Ford: Ranger&lt;/literalValue&gt;
                        &lt;quickReplyOptionIdentifier&gt;f9b16e07-fedb-dd27-e2e0-66c5cd7ea5b8&lt;/quickReplyOptionIdentifier&gt;
                    &lt;/botQuickReplyOptions&gt;
                    &lt;botQuickReplyOptions&gt;
                        &lt;literalValue&gt;Ford: Explorer&lt;/literalValue&gt;
                        &lt;quickReplyOptionIdentifier&gt;f2c96527-1dfa-0e7a-a711-8503d04c53b8&lt;/quickReplyOptionIdentifier&gt;
                    &lt;/botQuickReplyOptions&gt;
                    &lt;botQuickReplyOptions&gt;
                        &lt;literalValue&gt;Ford: F150&lt;/literalValue&gt;
                        &lt;quickReplyOptionIdentifier&gt;7c54b29f-7428-d1c1-e667-6480a74a5702&lt;/quickReplyOptionIdentifier&gt;
                    &lt;/botQuickReplyOptions&gt;
                    &lt;botVariableOperands&gt;
                        &lt;disableAutoFill&gt;true&lt;/disableAutoFill&gt;
                        &lt;sourceName&gt;Car_Type&lt;/sourceName&gt;
                        &lt;sourceType&gt;MlSlotClass&lt;/sourceType&gt;
                        &lt;targetName&gt;Car_Type&lt;/targetName&gt;
                        &lt;targetType&gt;ConversationVariable&lt;/targetType&gt;
                    &lt;/botVariableOperands&gt;
                    &lt;optionalCollect&gt;false&lt;/optionalCollect&gt;
                    &lt;quickReplyType&gt;Static&lt;/quickReplyType&gt;
                    &lt;quickReplyWidgetType&gt;Buttons&lt;/quickReplyWidgetType&gt;
                    &lt;type&gt;Collect&lt;/type&gt;
                    &lt;variableOperationIdentifier&gt;638af6ae-9405-6a15-c328-79e4cf4cd31e&lt;/variableOperationIdentifier&gt;
                &lt;/botVariableOperation&gt;
                &lt;stepIdentifier&gt;01542a2a-61d4-49cc-962b-0ac52767dae4&lt;/stepIdentifier&gt;
                &lt;type&gt;VariableOperation&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;botSteps&gt;
                &lt;botNavigation&gt;
                    &lt;botNavigationLinks&gt;
                        &lt;targetBotDialog&gt;Confirm_appointment&lt;/targetBotDialog&gt;
                    &lt;/botNavigationLinks&gt;
                    &lt;type&gt;Redirect&lt;/type&gt;
                &lt;/botNavigation&gt;
                &lt;stepIdentifier&gt;98bedd87-e0e3-4b71-8f57-81a79db90dea&lt;/stepIdentifier&gt;
                &lt;type&gt;Navigation&lt;/type&gt;
            &lt;/botSteps&gt;
            &lt;developerName&gt;Ask_for_car_type&lt;/developerName&gt;
            &lt;label&gt;Ask for car type&lt;/label&gt;
            &lt;showInFooterMenu&gt;false&lt;/showInFooterMenu&gt;
        &lt;/botDialogs&gt;
        &lt;conversationSystemDialogs&gt;
            &lt;dialog&gt;Error_Handling&lt;/dialog&gt;
            &lt;type&gt;ErrorHandling&lt;/type&gt;
        &lt;/conversationSystemDialogs&gt;
        &lt;conversationSystemDialogs&gt;
            &lt;dialog&gt;No_Agent_Available&lt;/dialog&gt;
            &lt;type&gt;TransferFailed&lt;/type&gt;
        &lt;/conversationSystemDialogs&gt;
        &lt;conversationVariables&gt;
            &lt;dataType&gt;Text&lt;/dataType&gt;
            &lt;developerName&gt;appointmentType&lt;/developerName&gt;
            &lt;label&gt;appointmentType&lt;/label&gt;
        &lt;/conversationVariables&gt;
        &lt;conversationVariables&gt;
            &lt;dataType&gt;DateTime&lt;/dataType&gt;
            &lt;developerName&gt;ApptDateTime&lt;/developerName&gt;
            &lt;label&gt;ApptDateTime&lt;/label&gt;
        &lt;/conversationVariables&gt;
        &lt;conversationVariables&gt;
            &lt;dataType&gt;Text&lt;/dataType&gt;
            &lt;developerName&gt;Car_Type&lt;/developerName&gt;
            &lt;label&gt;Car Type&lt;/label&gt;
        &lt;/conversationVariables&gt;
        &lt;conversationVariables&gt;
            &lt;dataType&gt;Text&lt;/dataType&gt;
            &lt;developerName&gt;LastName&lt;/developerName&gt;
            &lt;label&gt;LastName&lt;/label&gt;
        &lt;/conversationVariables&gt;
        &lt;conversationVariables&gt;
            &lt;collectionType&gt;List&lt;/collectionType&gt;
            &lt;dataType&gt;Object&lt;/dataType&gt;
            &lt;developerName&gt;ListVar&lt;/developerName&gt;
            &lt;label&gt;ListVar&lt;/label&gt;
        &lt;/conversationVariables&gt;
        &lt;conversationVariables&gt;
            &lt;dataType&gt;Text&lt;/dataType&gt;
            &lt;developerName&gt;Memory_Size&lt;/developerName&gt;
            &lt;label&gt;Memory Size&lt;/label&gt;
        &lt;/conversationVariables&gt;
        &lt;entryDialog&gt;Appointment_Menu&lt;/entryDialog&gt;
        &lt;mainMenuDialog&gt;Schedule_Appointment&lt;/mainMenuDialog&gt;
        &lt;nlpProviders&gt;
            &lt;language&gt;en_US&lt;/language&gt;
            &lt;nlpProviderType&gt;EinsteinAi&lt;/nlpProviderType&gt;
        &lt;/nlpProviders&gt;
    &lt;/botVersions&gt;
    &lt;contextVariables&gt;
        &lt;contextVariableMappings&gt;
            &lt;SObjectType&gt;LiveChatTranscript&lt;/SObjectType&gt;
            &lt;fieldName&gt;LiveChatTranscript.ChatKey&lt;/fieldName&gt;
            &lt;messageType&gt;WebChat&lt;/messageType&gt;
        &lt;/contextVariableMappings&gt;
        &lt;dataType&gt;Text&lt;/dataType&gt;
        &lt;developerName&gt;ChatKey&lt;/developerName&gt;
        &lt;label&gt;Chat Key&lt;/label&gt;
    &lt;/contextVariables&gt;
    &lt;contextVariables&gt;
        &lt;contextVariableMappings&gt;
            &lt;SObjectType&gt;LiveChatTranscript&lt;/SObjectType&gt;
            &lt;fieldName&gt;LiveChatTranscript.ContactId&lt;/fieldName&gt;
            &lt;messageType&gt;WebChat&lt;/messageType&gt;
        &lt;/contextVariableMappings&gt;
        &lt;dataType&gt;Id&lt;/dataType&gt;
        &lt;developerName&gt;ContactId&lt;/developerName&gt;
        &lt;label&gt;Contact Id&lt;/label&gt;
    &lt;/contextVariables&gt;
    &lt;contextVariables&gt;
        &lt;contextVariableMappings&gt;
            &lt;SObjectType&gt;LiveChatTranscript&lt;/SObjectType&gt;
            &lt;fieldName&gt;LiveChatTranscript.LiveChatVisitorId&lt;/fieldName&gt;
            &lt;messageType&gt;WebChat&lt;/messageType&gt;
        &lt;/contextVariableMappings&gt;
        &lt;dataType&gt;Id&lt;/dataType&gt;
        &lt;developerName&gt;EndUserId&lt;/developerName&gt;
        &lt;label&gt;End User Id&lt;/label&gt;
    &lt;/contextVariables&gt;
    &lt;contextVariables&gt;
        &lt;contextVariableMappings&gt;
            &lt;SObjectType&gt;LiveChatTranscript&lt;/SObjectType&gt;
            &lt;fieldName&gt;LiveChatTranscript.Id&lt;/fieldName&gt;
            &lt;messageType&gt;WebChat&lt;/messageType&gt;
        &lt;/contextVariableMappings&gt;
        &lt;dataType&gt;Id&lt;/dataType&gt;
        &lt;developerName&gt;RoutableId&lt;/developerName&gt;
        &lt;label&gt;Routable Id&lt;/label&gt;
    &lt;/contextVariables&gt;
    &lt;conversationChannelProviders&gt;
        &lt;agentRequired&gt;false&lt;/agentRequired&gt;
        &lt;chatButtonName&gt;botQueueGroup&lt;/chatButtonName&gt;
    &lt;/conversationChannelProviders&gt;
    &lt;description&gt;A bot from scratch.&lt;/description&gt;
    &lt;label&gt;kovai&lt;/label&gt;
    &lt;logPrivateConversationData&gt;false&lt;/logPrivateConversationData&gt;
    &lt;richContentEnabled&gt;false&lt;/richContentEnabled&gt;
&lt;/Bot&gt;
</code></pre>
<h2 id="references-9"><a class="header" href="#references-9">References</a></h2>
<ul>
<li><a href="https://trailhead.salesforce.com/content/learn/projects/build-an-einstein-bot">Build an Einstein Bot</a></li>
<li><a href="https://developer.salesforce.com/docs/atlas.en-us.bot_cookbook.meta/bot_cookbook/bot_cookbook_first_bot.htm">Einstein Bots Developer Cookbook</a></li>
<li><a href="https://help.salesforce.com/s/articleView?id=sf.bots_service_deploy_to_channels.htm&amp;type=5">Deploy Your Bot to Your Channels</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="deeppavlov"><a class="header" href="#deeppavlov">DeepPavlov</a></h1>
<ul>
<li>
<p>DeepPavlov is an open source framework for </p>
<ul>
<li>chatbots and virtual assistants development</li>
</ul>
</li>
<li>
<p>DeepPavlov has comprehensive and flexible tools that let developers and NLP researchers:</p>
<ul>
<li>
<p>create production ready conversational skills </p>
</li>
<li>
<p>complex multi-skill conversational assistants</p>
</li>
<li>
<p>Developed on top of the open source machine learning frameworks TensorFlow and Keras.</p>
</li>
</ul>
</li>
<li>
<p><a href="https://github.com/deepmipt/DeepPavlov/tree/master/examples">Examples</a></p>
</li>
</ul>
<h2 id="training-the-bot"><a class="header" href="#training-the-bot">Training the bot</a></h2>
<ul>
<li>Installing </li>
</ul>
<pre><code class="language-python deeppavlov">
!pip install deeppavlov
!python -m deeppavlov install gobot_dstc2_minimal

</code></pre>
<ul>
<li>A policy module of the bot decides what action should be taken in the current dialogue state.</li>
<li>The policy in our bot is implemented as a recurrent neural network (recurrency over user utterances) followed by a dense layer with softmax function on top. </li>
</ul>
<p><a href="https://colab.research.google.com/github/deepmipt/DeepPavlov/blob/master/examples/gobot_tutorial.ipynb#scrollTo=XouQ1IBegRvR"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></p>
<h3 id="training-data"><a class="header" href="#training-data">Training data</a></h3>
<pre><code class="language-json">[
  [
    {
      &quot;speaker&quot;: 1,
      &quot;text&quot;: &quot;hi&quot;
    },
    {
      &quot;speaker&quot;: 2,
      &quot;text&quot;: &quot;Hello, what is the weather today?&quot;,
      &quot;act&quot;: &quot;welcome_msg&quot;
    },
    {
      &quot;speaker&quot;: 1,
      &quot;text&quot;: &quot;Quite sunny outside&quot;
    },
    {
      &quot;speaker&quot;: 2,
      &quot;text&quot;: &quot;Then you should cycle!&quot;,
      &quot;act&quot;: &quot;suggest_cycling&quot;
    },
    {
      &quot;speaker&quot;: 1,
      &quot;text&quot;: &quot;Thanks! Great idea&quot;
    },
    {
      &quot;speaker&quot;: 2,
      &quot;text&quot;: &quot;You are welcome! Bye!&quot;,
      &quot;act&quot;: &quot;good_bye&quot;
    }
  ],
  [
    {
      &quot;speaker&quot;: 1,
      &quot;text&quot;: &quot;hey, bot&quot;
    },
    {
      &quot;speaker&quot;: 2,
      &quot;text&quot;: &quot;Hello, what is the weather today?&quot;,
      &quot;act&quot;: &quot;welcome_msg&quot;
    },
    {
      &quot;speaker&quot;: 1,
      &quot;text&quot;: &quot;raining a lot&quot;
    },
    {
      &quot;speaker&quot;: 2,
      &quot;text&quot;: &quot;Then you should try hot chinese tea!&quot;,
      &quot;act&quot;: &quot;suggest_tea&quot;
    },
    {
      &quot;speaker&quot;: 1,
      &quot;text&quot;: &quot;nice. thank you&quot;
    },
    {
      &quot;speaker&quot;: 2,
      &quot;text&quot;: &quot;You are welcome! Bye!&quot;,
      &quot;act&quot;: &quot;good_bye&quot;
    }
  ],
  [
    {
      &quot;speaker&quot;: 1,
      &quot;text&quot;: &quot;good morning you!&quot;
    },
    {
      &quot;speaker&quot;: 2,
      &quot;text&quot;: &quot;Hello, what is the weather today?&quot;,
      &quot;act&quot;: &quot;welcome_msg&quot;
    },
    {
      &quot;speaker&quot;: 1,
      &quot;text&quot;: &quot;hot and a little bit cloudy i guess&quot;
    },
    {
      &quot;speaker&quot;: 2,
      &quot;text&quot;: &quot;Then you should cycle!&quot;,
      &quot;act&quot;: &quot;suggest_cycling&quot;
    },
    {
      &quot;speaker&quot;: 1,
      &quot;text&quot;: &quot;no, i dont have a bike&quot;
    },
    {
      &quot;speaker&quot;: 2,
      &quot;text&quot;: &quot;That's a pity! Next time maybe. Have a good day!&quot;,
      &quot;act&quot;: &quot;bad_bye&quot;
    }
  ],
  [
    {
      &quot;speaker&quot;: 1,
      &quot;text&quot;: &quot;hello beautiful!&quot;
    },
    {
      &quot;speaker&quot;: 2,
      &quot;text&quot;: &quot;Hello, what is the weather today?&quot;,
      &quot;act&quot;: &quot;welcome_msg&quot;
    },
    {
      &quot;speaker&quot;: 1,
      &quot;text&quot;: &quot;too much snow, dont want to go out&quot;
    },
    {
      &quot;speaker&quot;: 2,
      &quot;text&quot;: &quot;Then you should try hot chinese tea!&quot;,
      &quot;act&quot;: &quot;suggest_tea&quot;
    },
    {
      &quot;speaker&quot;: 1,
      &quot;text&quot;: &quot;no i am not into tea&quot;
    },
    {
      &quot;speaker&quot;: 2,
      &quot;text&quot;: &quot;That's a pity! Next time maybe. Have a good day!&quot;,
      &quot;act&quot;: &quot;bad_bye&quot;
    }
  ],
  [
    {
      &quot;speaker&quot;: 1,
      &quot;text&quot;: &quot;good evening&quot;
    },
    {
      &quot;speaker&quot;: 2,
      &quot;text&quot;: &quot;Hello, what is the weather today?&quot;,
      &quot;act&quot;: &quot;welcome_msg&quot;
    },
    {
      &quot;speaker&quot;: 1,
      &quot;text&quot;: &quot;mainly cloudy and gray&quot;
    },
    {
      &quot;speaker&quot;: 2,
      &quot;text&quot;: &quot;Then you should try hot chinese tea!&quot;,
      &quot;act&quot;: &quot;suggest_tea&quot;
    },
    {
      &quot;speaker&quot;: 1,
      &quot;text&quot;: &quot;that sounds good&quot;
    },
    {
      &quot;speaker&quot;: 2,
      &quot;text&quot;: &quot;You are welcome! Bye!&quot;,
      &quot;act&quot;: &quot;good_bye&quot;
    }
  ],
  [
    {
      &quot;speaker&quot;: 1,
      &quot;text&quot;: &quot;hey&quot;
    },
    {
      &quot;speaker&quot;: 2,
      &quot;text&quot;: &quot;Hello, what is the weather today?&quot;,
      &quot;act&quot;: &quot;welcome_msg&quot;
    },
    {
      &quot;speaker&quot;: 1,
      &quot;text&quot;: &quot;very dark and murky&quot;
    },
    {
      &quot;speaker&quot;: 2,
      &quot;text&quot;: &quot;Then you should try hot chinese tea!&quot;,
      &quot;act&quot;: &quot;suggest_tea&quot;
    },
    {
      &quot;speaker&quot;: 1,
      &quot;text&quot;: &quot;tea is not funny&quot;
    },
    {
      &quot;speaker&quot;: 2,
      &quot;text&quot;: &quot;That's a pity! Next time maybe. Have a good day!&quot;,
      &quot;act&quot;: &quot;bad_bye&quot;
    }
  ],
  [
    {
      &quot;speaker&quot;: 1,
      &quot;text&quot;: &quot;how you doing&quot;
    },
    {
      &quot;speaker&quot;: 2,
      &quot;text&quot;: &quot;Hello, what is the weather today?&quot;,
      &quot;act&quot;: &quot;welcome_msg&quot;
    },
    {
      &quot;speaker&quot;: 1,
      &quot;text&quot;: &quot;the weather is gorgeous!&quot;
    },
    {
      &quot;speaker&quot;: 2,
      &quot;text&quot;: &quot;Then you should cycle!&quot;,
      &quot;act&quot;: &quot;suggest_cycling&quot;
    },
    {
      &quot;speaker&quot;: 1,
      &quot;text&quot;: &quot;maybe you are right, i'll try that&quot;
    },
    {
      &quot;speaker&quot;: 2,
      &quot;text&quot;: &quot;You are welcome! Bye!&quot;,
      &quot;act&quot;: &quot;good_bye&quot;
    }
  ],
  [
    {
      &quot;speaker&quot;: 1,
      &quot;text&quot;: &quot;hii&quot;
    },
    {
      &quot;speaker&quot;: 2,
      &quot;text&quot;: &quot;Hello, what is the weather today?&quot;,
      &quot;act&quot;: &quot;welcome_msg&quot;
    },
    {
      &quot;speaker&quot;: 1,
      &quot;text&quot;: &quot;nice weather really&quot;
    },
    {
      &quot;speaker&quot;: 2,
      &quot;text&quot;: &quot;Then you should cycle!&quot;,
      &quot;act&quot;: &quot;suggest_cycling&quot;
    },
    {
      &quot;speaker&quot;: 1,
      &quot;text&quot;: &quot;i dont wanna do such stuff&quot;
    },
    {
      &quot;speaker&quot;: 2,
      &quot;text&quot;: &quot;That's a pity! Next time maybe. Have a good day!&quot;,
      &quot;act&quot;: &quot;bad_bye&quot;
    }
  ]
]

</code></pre>
<p><img src="https://github.com/deepmipt/DeepPavlov/blob/master/examples/img/gobot_simple_policy.png?raw=1" alt="training the bot" /></p>
<table><thead><tr><th>action</th><th>system response</th></tr></thead><tbody>
<tr><td>welcome_msg</td><td>Hello, what is the weather today?</td></tr>
<tr><td>suggest_tea</td><td>Then you should try hot chinese tea!</td></tr>
<tr><td>suggest_cycling</td><td>Then you should cycle!</td></tr>
<tr><td>good_bye</td><td>You are welcome! Bye!</td></tr>
<tr><td>bad_bye</td><td>That's a pity! Next time maybe. Have a good day!</td></tr>
</tbody></table>
<p><img src="https://github.com/deepmipt/DeepPavlov/blob/master/examples/img/gobot_simple_pipeline.png?raw=1" alt="dialog system" /></p>
<h4 id="training-the-model"><a class="header" href="#training-the-model">Training the model</a></h4>
<pre><code class="language-python">from deeppavlov import train_model

gobot_config['train']['batch_size'] = 4 # set batch size
gobot_config['train']['max_batches'] = 30 # maximum number of training batches
gobot_config['train']['val_every_n_batches'] = 30 # evaluate on full 'valid' split every 30 epochs
gobot_config['train']['log_every_n_batches'] = 5 # evaluate on full 'train' split every 5 batches

train_model(gobot_config)

</code></pre>
<h3 id="building-the-model"><a class="header" href="#building-the-model">Building the model</a></h3>
<pre><code class="language-python">from deeppavlov import build_model
bot = build_model(gobot_config
</code></pre>
<h3 id="interacting-with-the-bot"><a class="header" href="#interacting-with-the-bot">Interacting with the bot</a></h3>
<pre><code class="language-python">
bot([[{&quot;text&quot;: &quot;good evening, bot&quot;}]])

</code></pre>
<ul>
<li>response</li>
</ul>
<pre><code class="language-python">[['Hello, what is the weather today?']]
</code></pre>
<pre><code class="language-python">
bot([[{&quot;text&quot;: &quot;the weather is clooudy and gloooomy&quot;}]])

</code></pre>
<ul>
<li>response</li>
</ul>
<pre><code class="language-python">[['Then you should cycle!']]
</code></pre>
<h3 id="resetting-the-bot"><a class="header" href="#resetting-the-bot">Resetting the bot</a></h3>
<pre><code class="language-python">bot.reset()
</code></pre>
<p><img src="https://github.com/deepmipt/DeepPavlov/blob/master/examples/img/gobot_simple_example.png?raw=1" alt="chat with bot" /></p>
<h2 id="references-10"><a class="header" href="#references-10">References</a></h2>
<ul>
<li><a href="https://deeppavlov.ai/">DEEPPAVLOV</a></li>
<li><a href="https://medium.com/@vaskon/how-to-build-hello-world-bot-with-deeppavlov-in-4-steps-b8636563ff81">How to build ‘Hello World!’ bot with DeepPavlov in 4 steps</a></li>
<li><a href="https://nlp.stanford.edu/projects/glove/">GloVe - Global Vectors for Word Representation</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dialogflow"><a class="header" href="#dialogflow">Dialogflow</a></h1>
<blockquote>
<p>Trying to teach a machine to have conversion is not easy!</p>
</blockquote>
<blockquote>
<p>User will ask same thing in a different ways!</p>
</blockquote>
<table><thead><tr><th>User Ask</th></tr></thead><tbody>
<tr><td>What is the forecast tomorrow?</td></tr>
<tr><td>What is the weather tomorrow?</td></tr>
<tr><td>What is the weather tomorrow in Boston?</td></tr>
</tbody></table>
<blockquote>
<p>Rule based systems to handle this not manageable! We need Natural Language Understanding (NLU)</p>
</blockquote>
<p><img src="img/chatbots/nlu-2.png" alt="nlu" /></p>
<ul>
<li>NLU works for both voice and text and with help of ML we can make chatbots really useful!
<img src="img/chatbots/nlu-ml-1.png" alt="nlu-ml" /></li>
</ul>
<p><img src="img/chatbots/dialogflow-1.png" alt="dialogflow" /></p>
<p>A natural language understanding (NLU) platform that makes it easy to design and integrate a conversational user interface into :</p>
<ul>
<li>mobile app</li>
<li>web application</li>
<li>device</li>
<li>bot </li>
<li>interactive voice response (IVR) system</li>
</ul>
<p>Using Dialogflow, you can provide new and <strong>engaging ways for users to interact</strong> with your product.</p>
<ul>
<li>
<p>Translate the Natural Language into machine readable data using ML models trained by the given set of examples.</p>
</li>
<li>
<p>It identifies about what the user is talking about, provides this data to the backend to take actions.</p>
</li>
<li>
<p>The backend performs the actions</p>
</li>
</ul>
<h2 id="steps"><a class="header" href="#steps">Steps</a></h2>
<ul>
<li>
<p>Create an Agent (the chatbot application) within Dialogflow</p>
<ul>
<li>Collecting what the <strong>user is saying</strong> and mapping into an <strong>intent</strong></li>
<li>Taking an action on that intent</li>
<li>Provide the user with the <strong>response</strong></li>
</ul>
</li>
<li>
<p>This all starts with a trigger event - <strong>Utterance</strong></p>
</li>
<li>
<p>This is how the user <strong>invokes</strong> the chatbot</p>
</li>
</ul>
<blockquote>
<p><em>Hey Google, what is the temperature at NY City?</em>  - is an utterance</p>
</blockquote>
<blockquote>
<p><em>Hey Google</em> - is a trigger</p>
</blockquote>
<p><img src="img/chatbots/dialogflow-utternce-1.png" alt="dialogflow utterance" /></p>
<blockquote>
<p><em>Hey Google, find the current stock of iPads from Inventory Management</em>  - is an utterance</p>
</blockquote>
<blockquote>
<p><em>find the current stock of iPads from Inventory Management</em> is the invocation phase for the chatbot</p>
</blockquote>
<blockquote>
<p><em>Inventory Management</em> is the invocation name</p>
</blockquote>
<h3 id="key-idea"><a class="header" href="#key-idea">Key idea</a></h3>
<ul>
<li>We need to understand:  <em>what is the user's intent?</em></li>
</ul>
<blockquote>
<p>User says: <em>I want to set an appointment</em></p>
</blockquote>
<blockquote>
<p><em>set an appointment</em> is the <strong>intent</strong></p>
</blockquote>
<hr />
<blockquote>
<p>User says: <em>what are your hours of operation</em></p>
</blockquote>
<blockquote>
<p><em>hours of operation</em> is the <strong>intent</strong></p>
</blockquote>
<ul>
<li>We provide Diagflow the different examples of user's intents
<ul>
<li>Diagflow trains a ML model with many more similar phrases 
<ul>
<li>maps the <strong>user phrases</strong> into the <strong>right intent</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="intent-matching"><a class="header" href="#intent-matching">Intent Matching</a></h3>
<table><thead><tr><th>Training Phrase</th><th>Intent</th><th>Action and Parameters</th></tr></thead><tbody>
<tr><td>I want to set an appointment</td><td>set an appointment</td><td><code>set_appointment()</code></td></tr>
<tr><td>what are your hours of operation</td><td>hours of operation</td><td><code>get_hoursOfOperation()</code></td></tr>
</tbody></table>
<ul>
<li>Parameters define <strong>variables</strong> we need to <strong>collect and store</strong></li>
</ul>
<h3 id="example"><a class="header" href="#example">Example</a></h3>
<table><thead><tr><th>User Phrase</th><th>Intent</th><th>Entities</th><th>Action and Parameters</th><th>Backend</th></tr></thead><tbody>
<tr><td>I want to set an appointment at 10am tomorrow</td><td>set an appointment</td><td>10am, tomorrow</td><td><code>set_appointment(&quot;10&quot;, &quot;tomorrow)</code></td><td>Provide a dynamic response</td></tr>
<tr><td>Good Morning</td><td>greeting</td><td></td><td><code>greet()</code></td><td>Provide a static response: <em>I am doing well</em></td></tr>
</tbody></table>
<h3 id="context"><a class="header" href="#context">Context</a></h3>
<ul>
<li>is the method for the chatbot to store and access <strong>variables</strong>  so it can <strong>exchange</strong> information from one intent to another in a conversation.</li>
</ul>
<h2 id="dialogflow-types-of-entities"><a class="header" href="#dialogflow-types-of-entities">Dialogflow types of entities</a></h2>
<ul>
<li></li>
</ul>
<h2 id="play-with-dialogflow"><a class="header" href="#play-with-dialogflow">Play with Dialogflow</a></h2>
<ul>
<li>
<p>Dialogflow creates GCP project to access logs and Cloud functions</p>
</li>
<li>
<p>Intents are mappings between a user's queries and actions fulfilled by your software. </p>
</li>
</ul>
<blockquote>
<p>User: good morning!</p>
</blockquote>
<blockquote>
<p>Bot: Hi! How are you doing?</p>
</blockquote>
<p>User|Bot|Intent|Action|Sentiment|
---|---|
good morning!|Hi! How are you doing?|Default Welcome Intent|input.welcome|Query Score: 0.9|
weather in Boston now|Sorry, what was that?|Default Fallback Intent|input.unknown|Query Score: 0.1|</p>
<ul>
<li>Resource URL</li>
</ul>
<pre><code> https://dialogflow.googleapis.com/v2/projects/appointmentscheduler-kjsl/agent/sessions/bcef58f8-e2ad-0641-7655-06f1945f3713:detectIntent

</code></pre>
<ul>
<li>Request Payload</li>
</ul>
<pre><code class="language-json">{
  &quot;queryInput&quot;: {
    &quot;text&quot;: {
      &quot;text&quot;: &quot;good morning!&quot;,
      &quot;languageCode&quot;: &quot;en&quot;
    }
  },
  &quot;queryParams&quot;: {
    &quot;source&quot;: &quot;DIALOGFLOW_CONSOLE&quot;,
    &quot;timeZone&quot;: &quot;America/New_York&quot;,
    &quot;sentimentAnalysisRequestConfig&quot;: {
      &quot;analyzeQueryTextSentiment&quot;: true
    }
  }
}
</code></pre>
<ul>
<li>Response</li>
</ul>
<pre><code class="language-json">
{
  &quot;responseId&quot;: &quot;0d8654f4-6b6e-4ac5-b99c-1054bcc653b3-e9fa6883&quot;,
  &quot;queryResult&quot;: {
    &quot;queryText&quot;: &quot;good morning!&quot;,
    &quot;action&quot;: &quot;input.welcome&quot;,
    &quot;parameters&quot;: {},
    &quot;allRequiredParamsPresent&quot;: true,
    &quot;fulfillmentText&quot;: &quot;Hello! How can I help you?&quot;,
    &quot;fulfillmentMessages&quot;: [
      {
        &quot;text&quot;: {
          &quot;text&quot;: [
            &quot;Hello! How can I help you?&quot;
          ]
        }
      }
    ],
    &quot;intent&quot;: {
      &quot;name&quot;: &quot;projects/appointmentscheduler-kjsl/agent/intents/ef927e0a-b805-4ada-9936-90aa79d710a5&quot;,
      &quot;displayName&quot;: &quot;Default Welcome Intent&quot;
    },
    &quot;intentDetectionConfidence&quot;: 0.4507024,
    &quot;languageCode&quot;: &quot;en&quot;,
    &quot;sentimentAnalysisResult&quot;: {
      &quot;queryTextSentiment&quot;: {
        &quot;score&quot;: 0.9,
        &quot;magnitude&quot;: 0.9
      }
    }
  }
}
</code></pre>
<h3 id="response-for-weather-in-boston-now"><a class="header" href="#response-for-weather-in-boston-now">Response for &quot;weather in Boston now&quot;</a></h3>
<pre><code class="language-json">{
  &quot;responseId&quot;: &quot;1dbd8e9d-3440-40e6-9605-67e84e7b2b0c-e9fa6883&quot;,
  &quot;queryResult&quot;: {
    &quot;queryText&quot;: &quot;weather in Boston now&quot;,
    &quot;action&quot;: &quot;input.unknown&quot;,
    &quot;parameters&quot;: {},
    &quot;allRequiredParamsPresent&quot;: true,
    &quot;fulfillmentText&quot;: &quot;Say that one more time?&quot;,
    &quot;fulfillmentMessages&quot;: [
      {
        &quot;text&quot;: {
          &quot;text&quot;: [
            &quot;Say that one more time?&quot;
          ]
        }
      }
    ],
    &quot;outputContexts&quot;: [
      {
        &quot;name&quot;: &quot;projects/appointmentscheduler-kjsl/agent/sessions/bcef58f8-e2ad-0641-7655-06f1945f3713/contexts/__system_counters__&quot;,
        &quot;lifespanCount&quot;: 1,
        &quot;parameters&quot;: {
          &quot;no-match&quot;: 2,
          &quot;no-input&quot;: 0
        }
      }
    ],
    &quot;intent&quot;: {
      &quot;name&quot;: &quot;projects/appointmentscheduler-kjsl/agent/intents/40d635ef-6274-4141-b6b3-7971c6866f53&quot;,
      &quot;displayName&quot;: &quot;Default Fallback Intent&quot;,
      &quot;isFallback&quot;: true
    },
    &quot;intentDetectionConfidence&quot;: 1,
    &quot;languageCode&quot;: &quot;en&quot;,
    &quot;sentimentAnalysisResult&quot;: {
      &quot;queryTextSentiment&quot;: {
        &quot;score&quot;: 0.1,
        &quot;magnitude&quot;: 0.1
      }
    }
  }
}
</code></pre>
<h2 id="create-intent---schedule-appointment"><a class="header" href="#create-intent---schedule-appointment">Create Intent - Schedule Appointment</a></h2>
<ul>
<li>Train the intent with what your users will say</li>
<li>Provide examples of how users will express their intent in natural language. </li>
<li>Adding <strong>numerous phrases</strong> with different <strong>variations and parameters</strong> will improve the <strong>accuracy</strong> of intent matching.</li>
</ul>
<table><thead><tr><th>Intent training phrase</th><th>Parameter Name</th><th>Entity</th><th>Resolved Value</th></tr></thead><tbody>
<tr><td>set an appointment on Friday at 10 am</td><td>date-time</td><td>@sys.data-time</td><td>Friday at 10 am</td></tr>
</tbody></table>
<h3 id="response-for"><a class="header" href="#response-for">Response for</a></h3>
<pre><code>set an appointment on Tuesday at 9 am

</code></pre>
<pre><code class="language-json">{
  &quot;responseId&quot;: &quot;676ca009-4f19-4e68-ac50-4f8db3c07fca-e9fa6883&quot;,
  &quot;queryResult&quot;: {
    &quot;queryText&quot;: &quot;set an appointment on Tuesday at 9 am&quot;,
    &quot;parameters&quot;: {
      &quot;date-time&quot;: {
        &quot;date_time&quot;: &quot;2022-01-04T09:00:00-05:00&quot;
      }
    },
    &quot;allRequiredParamsPresent&quot;: true,
    &quot;fulfillmentText&quot;: &quot;You all set for the appointment at  2022-01-04T09:00:00&quot;,
    &quot;fulfillmentMessages&quot;: [
      {
        &quot;text&quot;: {
          &quot;text&quot;: [
            &quot;You all set for the appointment at  2022-01-04T09:00:00&quot;
          ]
        }
      }
    ],
    &quot;intent&quot;: {
      &quot;name&quot;: &quot;projects/appointmentscheduler-kjsl/agent/intents/95898256-556e-4e53-a7af-a595e9f8ff7f&quot;,
      &quot;displayName&quot;: &quot;Schedule Appointment&quot;
    },
    &quot;intentDetectionConfidence&quot;: 1,
    &quot;languageCode&quot;: &quot;en&quot;,
    &quot;sentimentAnalysisResult&quot;: {
      &quot;queryTextSentiment&quot;: {}
    }
  }
}
</code></pre>
<table><thead><tr><th>User</th><th>Bot</th><th>Intent</th><th>Action</th><th>Sentiment</th><th>Comments</th></tr></thead><tbody>
<tr><td>set an appointment on Tuesday at 9 am</td><td>You all set for the appointment at 2022-01-04T09:00:00</td><td>Schedule Appointment</td><td>Not Available, Parameter:<code>date-time</code>, Value:<code>{ &quot;date_time&quot;: &quot;2022-01-04T09:00:00-05:00&quot; }</code></td><td>Query Score: 0.0</td><td></td></tr>
<tr><td>set an appointment</td><td>Not Available</td><td>Schedule Appointment</td><td>Not Available, Parameter:<code>date-time</code>, Value:<code> </code></td><td>Query Score: 0.0</td><td>missing <code>date-time</code>, requires <strong>slot filling</strong></td></tr>
</tbody></table>
<h3 id="slot-filling"><a class="header" href="#slot-filling">Slot filling</a></h3>
<ul>
<li>Make the entities as <strong>required</strong>
<ul>
<li>Dialog flow will make sure it ask both date and time before it can respond back</li>
</ul>
</li>
</ul>
<table><thead><tr><th>User</th><th>Bot</th><th>Intent</th><th>Action</th><th>Sentiment</th><th>Comments</th></tr></thead><tbody>
<tr><td>set an appointment</td><td>What time and date you like to come in for the appointment?</td><td>Schedule Appointment</td><td>Not Available, Parameter:<code>date-time</code>, Value:<code> </code></td><td>Query Score: 0.0</td><td>missing <code>date-time</code>, requires <strong>slot filling prompt is asked</strong></td></tr>
<tr><td>Monday 10 am</td><td>You all set for the appointment at 2022-01-03T10:00:00</td><td>Schedule Appointment</td><td>Not Available, Parameter:<code>date-time</code>, Value:<code>{ &quot;date_time&quot;: &quot;2022-01-04T09:00:00-05:00&quot; }</code></td><td>Query Score: 0.0</td><td><code>date-time</code> is provided by the user</td></tr>
</tbody></table>
<h3 id="testing-in-our-app"><a class="header" href="#testing-in-our-app">Testing in our app</a></h3>
<ul>
<li>Create a sample webapp using SFDX CLI</li>
</ul>
<pre><code>sfdx mohanc:app:webapp:gen -i /tmp/app.md -o df-appt.html \ 
                           -t 'Dialogflow Appointment testing app'
</code></pre>
<ul>
<li>Demo
<img src="img/chatbots/df-app-test.webm.gif" alt="testing Dialog flow Demo" /></li>
</ul>
<h2 id="dialog-flow-integration-with-dialogflow-messenger"><a class="header" href="#dialog-flow-integration-with-dialogflow-messenger">Dialog flow Integration with Dialogflow Messenger</a></h2>
<pre><code>
&lt;script src=&quot;https://www.gstatic.com/dialogflow-console/fast/messenger/bootstrap.js?v=1&quot;&gt;&lt;/script&gt;
&lt;df-messenger
  intent=&quot;WELCOME&quot;
  chat-title=&quot;AppointmentScheduler&quot;
  agent-id=&quot;d6e07c45-1523-4102-bf80-8fea7caf3caa&quot;
  language-code=&quot;en&quot;
&gt;&lt;/df-messenger&gt;

</code></pre>
<ul>
<li>Demo</li>
</ul>
<p><img src="img/chatbots/df-appt-messenger-1.webm.gif" alt="testing with Dialogflow Messenger flow Demo" /></p>
<h2 id="entities"><a class="header" href="#entities">Entities</a></h2>
<ul>
<li>System </li>
</ul>
<pre><code>@sys.date
@sys.time
@sys.number
@sys.unit-currency
@sys.percentage
@sys.address
@sys.phone-number
@sys.email
@sys.color

</code></pre>
<ul>
<li>Developer</li>
<li>Session</li>
</ul>
<h3 id="adding-developer-entity"><a class="header" href="#adding-developer-entity">Adding Developer Entity</a></h3>
<table><thead><tr><th>Entity Name</th><th>Value</th><th>Synonyms</th></tr></thead><tbody>
<tr><td>AppointmentType</td><td>Car Inspection</td><td>State Inspection, Vehicle Inspection</td></tr>
<tr><td>AppointmentType</td><td>Scheduled Maintenance</td><td>6 months Maintenance, Yearly Maintenance</td></tr>
</tbody></table>
<p><img src="img/chatbots/df-setting-up-appt-type-0.png" alt="setting up appointment type" />
<img src="img/chatbots/df-setting-up-appt-type-1.png" alt="setting up appointment type" />
<img src="img/chatbots/df-setting-up-appt-type-2.png" alt="setting up appointment type" /></p>
<ul>
<li>Demo - slot filling</li>
</ul>
<p><img src="img/chatbots/df-appt-type-1.png" alt="Demo appointment type" /></p>
<p><img src="img/chatbots/df-appt-type-2.png" alt="Demo appointment type-2" /></p>
<ul>
<li>Session Entity
<ul>
<li>Session ID</li>
<li>Have the information collected from the user from the rest of the conversion</li>
<li>Say, we can ask the user for the Vehicle Type  and get Toyota Camry, this value will be kept in the rest of the conversion</li>
</ul>
</li>
</ul>
<h2 id="integration-options"><a class="header" href="#integration-options">Integration options</a></h2>
<ul>
<li>One-click telephony BETA
<ul>
<li>Dialogflow Phone Gateway BETA</li>
<li>Avaya</li>
<li>SignalWire</li>
<li>Voximplant</li>
<li>AudioCodes</li>
<li>Twilio</li>
</ul>
</li>
</ul>
<p><img src="img/chatbots/df-twilio-1.png" alt="Twilio Messaging Service" /></p>
<ul>
<li>
<p>Telephony</p>
<ul>
<li>Genesys</li>
<li>Twilio</li>
</ul>
</li>
<li>
<p>Text Based</p>
<ul>
<li>Web Demo</li>
<li>Dialogflow Messenger BETA</li>
<li>Messenger from Facebook</li>
<li>Workplace from Facebook BETA</li>
<li>Slack</li>
<li>Telegram</li>
<li>LINE</li>
</ul>
</li>
</ul>
<h2 id="fulfillment---integration-with-google-calendar"><a class="header" href="#fulfillment---integration-with-google-calendar">Fulfillment - Integration with Google Calendar</a></h2>
<table><thead><tr><th>Intent</th><th>Fulfillment</th><th>Comments</th></tr></thead><tbody>
<tr><td>Intent-1</td><td>BizLogic-1</td><td></td></tr>
<tr><td>Intent-2</td><td>BizLogic-2</td><td></td></tr>
</tbody></table>
<p><img src="img/chatbots/df-fulfillment-1.png" alt="fulfillment" /></p>
<ul>
<li>webhook into Google Calendar</li>
</ul>
<blockquote>
<p>The web service (in our case Google Calendar) will receive a POST request from Dialogflow in the form of the response to a user query matched by intents with webhook enabled. </p>
</blockquote>
<pre><code class="language-json">
{
  &quot;type&quot;: &quot;service_account&quot;,
  &quot;project_id&quot;: &quot;projectid&quot;,
  &quot;private_key_id&quot;: &quot;sk id here&quot;,
  &quot;private_key&quot;: &quot;sk here&quot;,
  &quot;client_email&quot;: &quot;appointmentscheduler-kjsl@appspot.gserviceaccount.com&quot;,
  &quot;client_id&quot;: &quot;102792484459978676466&quot;,
  &quot;auth_uri&quot;: &quot;https://accounts.google.com/o/oauth2/auth&quot;,
  &quot;token_uri&quot;: &quot;https://oauth2.googleapis.com/token&quot;,
  &quot;auth_provider_x509_cert_url&quot;: &quot;https://www.googleapis.com/oauth2/v1/certs&quot;,
  &quot;client_x509_cert_url&quot;: &quot;https://www.googleapis.com/robot/v1/metadata/x509/xyz-kjsl%40appspot.gserviceaccount.com&quot;
}


</code></pre>
<pre><code class="language-json">{
  &quot;name&quot;: &quot;dialogflowFirebaseFulfillment&quot;,
  &quot;description&quot;: &quot;This is the default fulfillment for a Dialogflow agents using Cloud Functions for Firebase&quot;,
  &quot;version&quot;: &quot;0.0.1&quot;,
  &quot;private&quot;: true,
  &quot;license&quot;: &quot;Apache Version 2.0&quot;,
  &quot;author&quot;: &quot;Google Inc.&quot;,
  &quot;engines&quot;: {
    &quot;node&quot;: &quot;10&quot;
  },
  &quot;scripts&quot;: {
    &quot;start&quot;: &quot;firebase serve --only functions:dialogflowFirebaseFulfillment&quot;,
    &quot;deploy&quot;: &quot;firebase deploy --only functions:dialogflowFirebaseFulfillment&quot;
  },
  &quot;dependencies&quot;: {
    &quot;actions-on-google&quot;: &quot;^2.2.0&quot;,
    &quot;firebase-admin&quot;: &quot;^5.13.1&quot;,
    &quot;firebase-functions&quot;: &quot;^2.0.2&quot;,
    &quot;dialogflow&quot;: &quot;^0.6.0&quot;,
    &quot;dialogflow-fulfillment&quot;: &quot;^0.5.0&quot;
  }
}
</code></pre>
<pre><code class="language-js">/**
 * Copyright 2017 Google Inc. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

 'use strict';

 const functions = require('firebase-functions');
 const {google} = require('googleapis');
 const {WebhookClient} = require('dialogflow-fulfillment');
 
 // Enter your calendar ID below and service account JSON below
 const calendarId = &quot;xyx@group.calendar.google.com&quot;;
 const serviceAccount = &quot;xyx-kjsl@appspot.gserviceaccount.com&quot;; // Starts with {&quot;type&quot;: &quot;service_account&quot;,...
 
 // Set up Google Calendar Service account credentials
 const serviceAccountAuth = new google.auth.JWT({
   email: serviceAccount.client_email,
   key: serviceAccount.private_key,
   scopes: 'https://www.googleapis.com/auth/calendar'
 });
 
 const calendar = google.calendar('v3');
 process.env.DEBUG = 'dialogflow:*'; // enables lib debugging statements
 
 const timeZone = 'America/New_York';
 const timeZoneOffset = '-05:00';
 
 exports.dialogflowFirebaseFulfillment = functions.https.onRequest((request, response) =&gt; {
   const agent = new WebhookClient({ request, response });
   console.log(&quot;Parameters&quot;, agent.parameters);
   const appointment_type = agent.parameters.AppointmentType
   function makeAppointment (agent) {
     // Calculate appointment start and end datetimes (end = +1hr from start)
     //console.log(&quot;Parameters&quot;, agent.parameters.date);
     const dateTimeStart = new Date(Date.parse(agent.parameters.date.split('T')[0] + 'T' + agent.parameters.time.split('T')[1].split('-')[0] + timeZoneOffset));
     const dateTimeEnd = new Date(new Date(dateTimeStart).setHours(dateTimeStart.getHours() + 1));
     const appointmentTimeString = dateTimeStart.toLocaleString(
       'en-US',
       { month: 'long', day: 'numeric', hour: 'numeric', timeZone: timeZone }
     );
 
     // Check the availibility of the time, and make an appointment if there is time on the calendar
     return createCalendarEvent(dateTimeStart, dateTimeEnd, appointment_type).then(() =&gt; {
       agent.add(`Ok, let me see if we can fit you in. ${appointmentTimeString} is fine!.`);
     }).catch(() =&gt; {
       agent.add(`I'm sorry, there are no slots available for ${appointmentTimeString}.`);
     });
   }
 
   let intentMap = new Map();
   intentMap.set('Schedule Appointment', makeAppointment);
   agent.handleRequest(intentMap);
 });
 
 
 
 function createCalendarEvent (dateTimeStart, dateTimeEnd, appointment_type) {
   return new Promise((resolve, reject) =&gt; {
     calendar.events.list({
       auth: serviceAccountAuth, // List events for time period
       calendarId: calendarId,
       timeMin: dateTimeStart.toISOString(),
       timeMax: dateTimeEnd.toISOString()
     }, (err, calendarResponse) =&gt; {
       // Check if there is a event already on the Calendar
       if (err || calendarResponse.data.items.length &gt; 0) {
         reject(err || new Error('Requested time conflicts with another appointment'));
       } else {
         // Create event for the requested time period
         calendar.events.insert({ auth: serviceAccountAuth,
           calendarId: calendarId,
           resource: {summary: appointment_type +' Appointment', description: appointment_type,
             start: {dateTime: dateTimeStart},
             end: {dateTime: dateTimeEnd}}
         }, (err, event) =&gt; {
           err ? reject(err) : resolve(event);
         }
         );
       }
     });
   });
 }
</code></pre>
<h2 id="dialogflow-integration-with-google-assistant-actions"><a class="header" href="#dialogflow-integration-with-google-assistant-actions">Dialogflow Integration with Google Assistant Actions</a></h2>
<ul>
<li>Actions in Google
<ul>
<li>Way to extend the functionality of Google Assistant</li>
<li>We can reach 500 million devices that support Google Assistant
<ul>
<li>Smart speakers</li>
<li>Phones</li>
<li>Cars</li>
<li>TVs</li>
<li>Watches</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="knowledge-base-support-in-dialogflow-chatbots"><a class="header" href="#knowledge-base-support-in-dialogflow-chatbots">Knowledge Base Support in Dialogflow chatbots</a></h2>
<p><img src="img/chatbots/df-kb-1.png" alt="Df knowledge setup" /></p>
<ul>
<li>Demo
<img src="img/chatbots/dialog-kb-test.webm.gif" alt="Demo of KB with Dialogflow" /></li>
</ul>
<iframe width="560" height="315" src="https://www.youtube.com/embed/J8ttQ1Veo_I" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<h2 id="django-frontend"><a class="header" href="#django-frontend">Django frontend</a></h2>
<p><img src="https://github.com/priyankavergadia/Django-Dialogflow-Appointment-Scheduler/raw/master/Architecture-image.png" alt="Django Dialogflow Appointment Scheduler" /></p>
<ul>
<li><a href="https://github.com/priyankavergadia/Django-Dialogflow-Appointment-Scheduler">Django Dialogflow Appointment Scheduler</a></li>
</ul>
<h2 id="integration-with-google-cloud-ml"><a class="header" href="#integration-with-google-cloud-ml">Integration with Google Cloud ML</a></h2>
<p><img src="img/chatbots/df-ml-1.png" alt="DF integration with ML" /></p>
<h2 id="dialogflow-cxhttpsdialogflowcloudgooglecomcxprojects"><a class="header" href="#dialogflow-cxhttpsdialogflowcloudgooglecomcxprojects">Dialogflow (CX](https://dialogflow.cloud.google.com/cx/projects)</a></h2>
<p><img src="img/chatbots/dfcx-vb-1.png" alt="Visual Flow Builder" />
<img src="img/chatbots/dfcx-arch-1.png" alt="CX arch" /></p>
<h2 id="references-11"><a class="header" href="#references-11">References</a></h2>
<ul>
<li><a href="https://cloud.google.com/dialogflow/docs/">Dialogflow</a></li>
</ul>
<h2 id="videos-4"><a class="header" href="#videos-4">Videos</a></h2>
<ul>
<li>What is Dialog flow</li>
</ul>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Ov3CDTxZRQc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<ul>
<li>Intents, Prompts, Appointment Builder</li>
</ul>
<iframe width="560" height="315" src="https://www.youtube.com/embed/oU88sHd6ilE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<h2 id="references-12"><a class="header" href="#references-12">References</a></h2>
<ul>
<li><a href="https://cloud.google.com/dialogflow/docs/">Dialogflow</a></li>
<li><a href="https://github.com/asrivas/SmartAccountsBot">Integration with Sheets API</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rasa"><a class="header" href="#rasa">Rasa</a></h1>
<p>Rasa is an open source machine learning framework for <strong>automated text and voice-based conversations</strong>.</p>
<h2 id="sample-training-data-in-yaml"><a class="header" href="#sample-training-data-in-yaml">Sample Training data in yaml</a></h2>
<ul>
<li>NLU Data</li>
</ul>
<pre><code class="language-yaml">nlu:
- intent: greet
  examples: |
    - Hi
    - Hey!
    - Hallo
    - Good day
    - Good morning

- intent: subscribe
  examples: |
    - I want to get the newsletter
    - Can you send me the newsletter?
    - Can you sign me up for the newsletter?

- intent: inform
  examples: |
    - My email is example@example.com
    - random@example.com
    - Please send it to anything@example.com
    - Email is something@example.com
</code></pre>
<ul>
<li>Responses</li>
</ul>
<pre><code class="language-yaml">responses:
   utter_greet:
       - text: |
           Hello! How can I help you?
       - text: |
           Hi!
   utter_ask_email:
       - text: |
           What is your email address?
   utter_subscribed:
       - text: |
           Check your inbox at {email} in order to finish subscribing to the newsletter!
       - text: |
           You're all set! Check your inbox at {email} to confirm your subscription.
</code></pre>
<ul>
<li>Stores - connect intents with actions (utters)</li>
</ul>
<pre><code class="language-yaml">stories:
 - story: greet and subscribe
   steps:
   - intent: greet
   - action: utter_greet
   - intent: subscribe
   - action: newsletter_form
   - active_loop: newsletter_form
</code></pre>
<ul>
<li>Forms - collect information from the user</li>
</ul>
<pre><code class="language-yaml">slots:
  email:
    type: text
    mappings:
    - type: from_text
      conditions:
      - active_loop: newsletter_form
        requested_slot: email
forms:
  newsletter_form:
    required_slots:
    - email
</code></pre>
<ul>
<li>Rules</li>
</ul>
<pre><code class="language-yaml">rules:
 - rule: activate subscribe form
   steps:
   - intent: subscribe
   - action: newsletter_form
   - active_loop: newsletter_form

 - rule: submit form
   condition:
   - active_loop: newsletter_form
   steps:
   - action: newsletter_form
   - active_loop: null
   - action: utter_subscribed
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="19--transformers"><a class="header" href="#19--transformers">19.  Transformers</a></h1>
<h2 id="papers-2"><a class="header" href="#papers-2">Papers</a></h2>
<ul>
<li>
<p><a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is All You Need</a></p>
<ul>
<li>
<p>A new simple network architecture, the <strong>Transformer</strong>, based solely on <strong>attention mechanisms</strong>, </p>
<ul>
<li>dispensing with recurrence and convolutions entirely</li>
</ul>
</li>
<li>
<p>Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.</p>
</li>
<li>
<p>sequence modeling tasks are based on</p>
<ul>
<li>LSTM  (long short-term memory)</li>
<li>language modeling and machine translation</li>
<li>generate a sequence of hidden states ht as a function of the previous hidden state ht−1 1 and the input for position t. This inherently sequential nature precludes <strong>parallelization</strong> within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.</li>
</ul>
</li>
<li>
<p><strong>Attention mechanisms</strong> have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences</p>
</li>
<li>
<p>Transformer</p>
<ul>
<li>a model architecture eschewing recurrence and instead relying entirely on <strong>an attention mechanism</strong> to draw <strong>global dependencies</strong> between input and output.</li>
<li>allows for significantly more <strong>parallelization</strong> and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.</li>
</ul>
</li>
<li>
<p>Usually we compute the hidden representations in parallel for all input and output positions. </p>
</li>
<li>
<p>In these models,the <strong>number of operations</strong> required to relate signals from two arbitrary input or output positions grows in the <strong>distance between positions</strong></p>
</li>
<li>
<p>This makes it more difficult to learn dependencies between distant positions </p>
</li>
<li>
<p>In Transformer this is reduced to a <strong>constant number of operations</strong></p>
</li>
<li>
<p><strong>Self-attention</strong>, sometimes called intra-attention</p>
<ul>
<li>attention mechanism <strong>relating different positions</strong> of a single sequence in order to compute a representation of the sequence.</li>
<li>relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution. </li>
</ul>
</li>
</ul>
<p><img src="img/transform-1.png" alt="transformer" /></p>
<h3 id="encoder"><a class="header" href="#encoder">Encoder</a></h3>
<p><a href="http://localhost:7010/github-mirror/mohan-chinnappan-n5.github.io/wc/wc.html?c=1">wc</a></p>
<ul>
<li>
<p>The encoder is composed of a stack of <code>N = 6</code> identical layers</p>
</li>
<li>
<p>Each Layer has  2 Sub Layers, each Sub Layer has:</p>
<ul>
<li>multi-head <strong>self-attention</strong></li>
<li>position-wise fully connected feed-forward network</li>
</ul>
</li>
<li>
<p><code>Sublayer(x)</code> is the function implemented by the sub-layer itself.</p>
</li>
<li>
<p>Output of each sub-layer is <code>LayerNorm(x + Sublayer(x))</code></p>
</li>
<li>
<p>To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension <code>dmodel = 512</code></p>
</li>
</ul>
<h3 id="decoder"><a class="header" href="#decoder">Decoder</a></h3>
<ul>
<li>also composed of a stack of <code>N = 6</code> identical layers</li>
<li>In addition to the two sub-layers in each encoder layer, the decoder <strong>inserts a third sub-layer</strong>
<ul>
<li>which performs multi-head <strong>attention over the output</strong> of the encoder stack</li>
<li>Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization</li>
</ul>
</li>
<li>We also modify the self-attention sub-layer in the decoder stack to <strong>prevent positions from attending to subsequent positions</strong>. 
<ul>
<li>This masking, combined with fact that the output embeddings are offset by one position
<ul>
<li>ensures that the predictions for position <code>i</code> can depend only on the known outputs at positions   <code>&lt; i</code>.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="attention"><a class="header" href="#attention">Attention</a></h3>
<ul>
<li>Can be described as mapping a <strong>query</strong> and a <strong>set of key-value pairs</strong> to an <strong>output</strong>,</li>
<li>where the query, keys, values, and output are all <strong>vectors</strong>.</li>
<li>The output is computed as a <strong>weighted sum of the values</strong>
<ul>
<li>where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2005.14165.pdf">Language Models are Few-Shot Learners</a></p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gpt-1"><a class="header" href="#gpt-1">GPT</a></h1>
<h1 id="generatively-pretrained-transformer-gpt"><a class="header" href="#generatively-pretrained-transformer-gpt">Generatively Pretrained Transformer (GPT)</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="building-gpt"><a class="header" href="#building-gpt">Building GPT</a></h1>
<pre><code class="language-py">
input_txt = &quot;Once upon a time there was a king called Askhoka. He planted fruit bearing trees for the benefit of the animals and humans&quot;
chars = sorted(set(input_txt))
vocab_size = len(chars)

print (''.join(chars)

# to integer (encoding)
stoi = {ch:i for i,ch in enumerate(chars)}
itos = {i:ch for i,ch in enumerate(chars)}

encode = lambda s:  [ stoi[c] for c in s]
decode = lambda l:  [ itos[i] for i in l ]

encode('thompson')
decode(encode('thompson'))

''.join(decode(encode('thompson')))

</code></pre>
<ul>
<li><a href="https://github.com/mohan-chinnappan-n/ml-book/blob/main/nanogpt2.ipynb">Colab link</a></li>
</ul>
<h2 id="references-13"><a class="header" href="#references-13">References</a></h2>
<ul>
<li><a href="https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference">MathJax basic tutorial and quick reference</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="20-tools"><a class="header" href="#20-tools">20. Tools</a></h1>
<ul>
<li><a href="tablesgenerator.com/markdown_tables">Markdown Table Generator </a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="infrastructure-as-code"><a class="header" href="#infrastructure-as-code">Infrastructure as code</a></h1>
<h2 id="benefits"><a class="header" href="#benefits">Benefits</a></h2>
<ul>
<li>Repeatable</li>
<li>Infrastructure automation</li>
<li>Integration with CI/CD</li>
<li>Git integration (GitOps)</li>
<li>Visibility and auditing 
<ul>
<li>Doc source of the infrastructure </li>
</ul>
</li>
</ul>
<h3 id="imperative-way"><a class="header" href="#imperative-way">Imperative way</a></h3>
<pre><code class="language-python">
def setup(env_name):
    setupEC2Instance(env_name)
    buckets = ['one', 'two', 'three']
    for bucket in buckets:
        setupEC2Bucket(env_name, bucket)


</code></pre>
<ul>
<li>lot of work!</li>
</ul>
<h3 id="declarative"><a class="header" href="#declarative">Declarative</a></h3>
<ul>
<li>
<p>GCP - <a href="https://cloud.google.com/deployment-manager/docs">Deployment Manager</a></p>
</li>
<li>
<p>AWS - <a href="https://aws.amazon.com/cloudformation/">AWS CloudFormation</a></p>
</li>
<li>
<p>Azure - <a href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/management/overview">Resource Manager</a></p>
</li>
<li>
<p>For multi-cloud <a href="https://www.terraform.io/">Terraform</a> </p>
<ul>
<li>infrastructure as code software tool that provides a consistent CLI workflow to manage hundreds of cloud services.</li>
<li>Open source</li>
<li>Uses a language hcl <a href="https://www.terraform.io/language/syntax/configuration">HashiCorp Config Language</a></li>
</ul>
</li>
</ul>
<pre><code class="language-java">
resource &quot;aws_instance&quot; &quot;example&quot; {
  ami = &quot;data.aws_ami.redhat.id&quot;
  instance_type = &quot;t3.micro&quot;

  network_interface {
    # ...
  }
}


</code></pre>
<ul>
<li>remembers the state of the already provisioned resources - [Idempotence](https://en.wikipedia.org/wiki/
Idempotence) - certain operations can be applied <strong>multiple times</strong> without changing the result beyond the initial application.</li>
</ul>
<h3 id="installing-terraform-cli"><a class="header" href="#installing-terraform-cli">Installing Terraform CLI</a></h3>
<pre><code class="language-bash">brew tap hashicorp/tap
brew install hashicorp/tap/terraform

</code></pre>
<ul>
<li>Usage</li>
</ul>
<pre><code>% terraform
Usage: terraform [global options] &lt;subcommand&gt; [args]

The available commands for execution are listed below.
The primary workflow commands are given first, followed by
less common or more advanced commands.

Main commands:
  init          Prepare your working directory for other commands
  validate      Check whether the configuration is valid
  plan          Show changes required by the current configuration
  apply         Create or update infrastructure
  destroy       Destroy previously-created infrastructure

All other commands:
  console       Try Terraform expressions at an interactive command prompt
  fmt           Reformat your configuration in the standard style
  force-unlock  Release a stuck lock on the current workspace
  get           Install or upgrade remote Terraform modules
  graph         Generate a Graphviz graph of the steps in an operation
  import        Associate existing infrastructure with a Terraform resource
  login         Obtain and save credentials for a remote host
  logout        Remove locally-stored credentials for a remote host
  output        Show output values from your root module
  providers     Show the providers required for this configuration
  refresh       Update the state to match remote systems
  show          Show the current state or a saved plan
  state         Advanced state management
  taint         Mark a resource instance as not fully functional
  test          Experimental support for module integration testing
  untaint       Remove the 'tainted' state from a resource instance
  version       Show the current Terraform version
  workspace     Workspace management

Global options (use these before the subcommand, if any):
  -chdir=DIR    Switch to a different working directory before executing the
                given subcommand.
  -help         Show this help output, or the help for a specified subcommand.
  -version      An alias for the &quot;version&quot; subcommand.

</code></pre>
<ul>
<li>Demo
<video controls loop=""><source src="https://www.terraform.io/videos/oss-cli-demo.mp4" type="video/mp4"></video></li>
</ul>
<h2 id="videos-5"><a class="header" href="#videos-5">Videos</a></h2>
<iframe width="800" height="420" src="https://www.youtube.com/embed/Tkv49sTvKZY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script type="text/javascript">
            var socket = new WebSocket("ws://localhost:5000/__livereload");
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>



        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->

        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </body>
</html>
