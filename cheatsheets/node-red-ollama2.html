<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Node-RED and Ollama Integration Guide</title>
  <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/themes/prism.min.css" />
  <link rel="icon" type="image/x-icon" href="https://mohan-chinnappan-n5.github.io/dfv/img/mc_favIcon.ico">
</head>
<body class="bg-gray-100 font-sans leading-relaxed">

  <div class="container mx-auto p-4"> <!-- Main container -->
    <!-- Navbar -->
    <nav class="bg-blue-800 p-4 sticky top-0 z-50">
      <div class="flex justify-between items-center">
        <h1 class="text-white font-bold text-xl">Node-RED & Ollama - Demo</h1>
        <div>
          <a href="#about" class="text-gray-300 hover:text-white px-3">About</a>
          <a href="#setup" class="text-gray-300 hover:text-white px-3">Setup Guide</a>
          <a href="#demo" class="text-gray-300 hover:text-white px-3">Demo</a>
        </div>
      </div>
    </nav>

    <!-- About Section -->
    <section id="about" class="p-6 mt-6">
      <h2 class="text-2xl font-semibold text-gray-800 mb-4">About the Integration</h2>
      <p class="text-gray-700 leading-relaxed">
        This application demonstrates how to integrate <strong>Node-RED</strong> with <strong>Ollama</strong>, a Language Model API.
        Using Node-RED, we can create flows that send prompts to an LLM and receive responses, which are then displayed or further processed in the Node-RED environment.
      </p>
    </section>

    <!-- Setup Guide Section -->
    <section id="setup" class="p-6 mt-6 bg-white shadow rounded">
      <h2 class="text-2xl font-semibold text-gray-800 mb-4">Setup Guide</h2>
      <ol class="list-decimal list-inside text-gray-700 leading-relaxed">
        <li>
          <strong>Step 1:</strong> Create a Node-RED flow and add the custom <code>llm-query</code> node with the following code:
          <pre class="bg-gray-100 p-4 rounded mt-2 language-javascript">
<code class="language-javascript">
module.exports = function (RED) {
    function LLMQueryNode(config) {
        RED.nodes.createNode(this, config);
        const node = this;

        // Retrieve the configuration
        node.prompt = config.prompt;
        node.url = config.url || "http://localhost:11434/api/generate";  // default URL if not specified in the config

        // Handle incoming messages
        node.on('input', async function (msg) {
            // Set the prompt from msg or config
            const prompt = msg.prompt || node.prompt;
            if (!prompt) {
                node.error("No prompt provided.");
                return;
            }

            // Prepare the request payload
            const payload = { prompt: prompt , model: "llama3.2", stream: false};
            console.log(payload);
            console.log(JSON.stringify(payload));

            // Send the request
            try {
                console.log(node.url);
                const response = await fetch(node.url, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify(payload)
                });

                if (!response.ok) {
                    node.error(`Error in calling: ${response.statusText}`);
                    return;
                }

                const resultText = await response.text();  // Get the response as plain text
                //console.log("Raw response text:", resultText);  // Log the raw text

                // Try parsing if it's JSON
                try {
                    const result = JSON.parse(resultText);
                    console.log(result.response);

                    msg.payload = result.response || "No response";
                    node.send(msg);
    
                } catch (e) {
                    node.error("Response is not valid JSON: " + e.message);
                    return;
                }

            
            } catch (error) {
                node.error("Failed to connect to the LLM endpoint: " + error.message);
            }
        });
    }
    RED.nodes.registerType("llm-query", LLMQueryNode);
}
</code>
          </pre>
        </li>
        <li>
          <strong>Step 2:</strong> Configure the <code>llm-query</code> node with the correct endpoint URL (default: <code>http://localhost:11434/api/generate</code>).
        </li>
        <li>
          <strong>Step 3:</strong> Add a <code>Debug</code> node connected to <code>llm-query</code> to view the response in the Node-RED debug panel.
        </li>
        <li>
          <strong>Step 4:</strong> Deploy and test the flow, and observe the responses in the debug panel.
          <img src='img/node-red-llm-query-1.png' alt="Node-RED LLM Query Example">
        </li>
      </ol>
    </section>

    <!-- Node-RED Node Script in a Textarea -->
    <section class="p-6 mt-6 bg-white shadow rounded">
      <h2 class="text-2xl font-semibold text-gray-800 mb-4">Node-RED Node Script</h2>
       <textarea class="w-full h-48 p-4 border border-gray-300 rounded bg-black text-white  font-mono" style="font-family: 'Monaco', monospace;" readonly>
        // Node-RED custom node registration
        RED.nodes.registerType('llm-query', {
          category: 'function',
          color: '#a6bbcf',
          defaults: {
            name: { value: "" },
            prompt: { value: "" },
            url: { value: "http://localhost:11434/api/generate" }
          },
          inputs: 1,
          outputs: 1,
          icon: "font-awesome/fa-comments",
          label: function () {
            return this.name || "LLM Query";
          }
        });
      </textarea>
    </section>

    <section class="p-6 mt-6 bg-white shadow rounded">
      <h2 class="text-2xl font-semibold text-gray-800 mb-4">Node-RED Node Template</h2>
      <textarea class="w-full h-48 p-4 border border-gray-300 rounded bg-black text-white  font-mono" style="font-family: 'Monaco', monospace;" readonly>
        <script type="text/x-red" data-template-name="llm-query">
          <div class="form-row">
            <label for="node-input-name"><i class="fa fa-tag"></i> Name</label>
            <input type="text" id="node-input-name" placeholder="Node Name">
          </div>
          <div class="form-row">
            <label for="node-input-prompt"><i class="fa fa-terminal"></i> Prompt</label>
            <input type="text" id="node-input-prompt" placeholder="Enter prompt text">
          </div>
          <div class="form-row">
            <label for="node-input-url"><i class="fa fa-link"></i> API Endpoint</label>
            <input type="text" id="node-input-url" placeholder="http://localhost:11434/api/generate">
          </div>
        </script>
      </textarea>
    </section>

    <!-- Demo Section -->
    <section id="demo" class="p-6 mt-6 bg-white shadow rounded">
      <h2 class="text-2xl font-semibold text-gray-800 mb-4">Demo</h2>
      <p class="text-gray-700 mb-4">
        Watch the demo video below to see how the Node-RED flow works with Ollama:
      </p>
      <div class="flex justify-center">
        <video controls class="rounded-lg shadow-lg" style="max-width: 80%;">
          <source src="https://github.com/mohan-chinnappan-n5/compares/raw/refs/heads/main/demos/node-red-ollama.webm" type="video/webm">
          Your browser does not support the video tag.
        </video>
      </div>
    </section>

    <!-- Footer -->
    <footer class="bg-blue-600 text-white p-4 mt-12">
      <div class="text-center">
        <p>MCDocs - LLMs</p>
      </div>
    </footer>
  </div> <!-- End of main container -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/prism.min.js"></script>
</body>
</html>
