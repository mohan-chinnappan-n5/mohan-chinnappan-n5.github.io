<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>LLM Cheat Sheet</title>
  <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
  <link rel="icon" type="image/x-icon" href="https://mohan-chinnappan-n5.github.io/dfv/img/mc_favIcon.ico">
  <script type="module" src="https://unpkg.com/mermaid@10/dist/mermaid.esm.min.mjs"></script>
  <script>
    document.addEventListener("DOMContentLoaded", function () {
      mermaid.initialize({ startOnLoad: true });
    });
  </script>
</head>
<body class="bg-gray-100 font-sans leading-normal tracking-normal">

  <!-- Navbar -->
  <nav class="bg-blue-900 text-white p-4 fixed w-full z-10 top-0">
    <div class="container mx-auto flex justify-between items-center">
      <a href="#overview" class="text-lg font-semibold hover:text-gray-200">LLM Cheat Sheet</a>
      <div class="flex space-x-4">
        <a href="#overview" class="hover:text-gray-200">Overview</a>
        <a href="#tokenization" class="hover:text-gray-200">Tokenization</a>
        <a href="#embedding" class="hover:text-gray-200">Embedding</a>
        <a href="#transformers" class="hover:text-gray-200">Transformers</a>
        <a href="#attention" class="hover:text-gray-200">Attention Mechanism</a>
        <a href="#fine-tuning" class="hover:text-gray-200">Fine-Tuning</a>
        <a href="#usage" class="hover:text-gray-200">Usage</a>
        <a href="#tips" class="hover:text-gray-200">Tips</a>
      </div>
    </div>
  </nav>

  <!-- Main Content -->
  <div class="container mx-auto mt-16 px-6 py-8 space-y-12">

    <!-- Overview Section -->
    <section id="overview">
      <h2 class="text-2xl font-bold mb-4 text-blue-900">Overview</h2>
      <p class="text-gray-700">
        Large Language Models (LLMs) are machine learning models that can process and generate natural language text.
        They are trained on vast datasets of text, enabling them to understand context, syntax, semantics, and even nuanced meanings.
        Popular LLMs include GPT-4, BERT, and T5.
      </p>
    </section>

    <!-- Tokenization Section -->
    <section id="tokenization">
      <h2 class="text-2xl font-bold mb-4 text-blue-900">Tokenization</h2>
      <p class="text-gray-700">
        Tokenization is the process of breaking down text into smaller units, like words or sub-words, called tokens. LLMs use tokens to represent input and output text, allowing the model to understand and generate human language effectively.
      </p>
      <ul class="list-disc ml-6 text-gray-700 space-y-2">
        <li>
          <span class="font-semibold">Word-based Tokenization:</span> Treats each word as a token. However, this method can be inefficient with large vocabularies.
        </li>
        <li>
          <span class="font-semibold">Sub-word Tokenization:</span> Breaks down words into sub-word units (like prefixes or suffixes), reducing vocabulary size while maintaining meaning.
        </li>
        <li>
          <span class="font-semibold">Byte-Pair Encoding (BPE):</span> A common technique in LLMs for creating sub-word units by merging frequent pairs of characters or words.
        </li>
      </ul>
    </section>

    <!-- Embedding Section -->
    <section id="embedding">
      <h2 class="text-2xl font-bold mb-4 text-blue-900">Embedding</h2>
      <p class="text-gray-700">
        Embedding is a representation of text in a continuous vector space, capturing the semantic meaning of words or phrases. In LLMs, embeddings serve as inputs that allow the model to understand relationships between tokens.
      </p>
      <ul class="list-disc ml-6 text-gray-700 space-y-2">
        <li>
          <span class="font-semibold">Word Embeddings:</span> Traditional embeddings like Word2Vec and GloVe capture word relationships in fixed dimensions.
        </li>
        <li>
          <span class="font-semibold">Contextual Embeddings:</span> Advanced embeddings, like those generated by Transformers, change based on surrounding words, enabling dynamic context understanding.
        </li>
        <li>
          <span class="font-semibold">Positional Embedding:</span> Adds information on word positions, crucial in sequence models to maintain word order.
        </li>
      </ul>
    </section>

    <!-- Transformers Section -->
    <section id="transformers">
      <h2 class="text-2xl font-bold mb-4 text-blue-900">Transformers</h2>
      <p class="text-gray-700">
        Transformers are the foundational architecture behind modern LLMs. They use self-attention mechanisms to process input data in parallel, capturing complex relationships between tokens in a sentence.
      </p>
      <ul class="list-disc ml-6 text-gray-700 space-y-2">
        <li>
          <span class="font-semibold">Self-Attention:</span> Allows the model to focus on relevant words in a sentence, regardless of their position.
        </li>
        <li>
          <span class="font-semibold">Positional Encoding:</span> Adds information about the position of words, enabling Transformers to understand word order.
        </li>
        <li>
          <span class="font-semibold">Multi-Head Attention:</span> Uses multiple attention mechanisms to capture different relationships within the text.
        </li>
      </ul>
    </section>

    <!-- Attention Mechanism Section -->
    <section id="attention">
      <h2 class="text-2xl font-bold mb-4 text-blue-900">Attention Mechanism</h2>
      <p class="text-gray-700">
        The Attention Mechanism is a core feature of Transformers, allowing them to focus on relevant parts of the input sequence. By assigning "attention weights" to each token, the model can prioritize certain words when generating output, enabling nuanced and contextually relevant responses.
      </p>
      <ul class="list-disc ml-6 text-gray-700 space-y-2">
        <li>
          <span class="font-semibold">Scaled Dot-Product Attention:</span> Calculates attention weights based on the dot product of query and key vectors, scaled for numerical stability.
        </li>
        <li>
          <span class="font-semibold">Softmax Activation:</span> Applies the softmax function to normalize attention scores, making them interpretable as probabilities.
        </li>
        <li>
          <span class="font-semibold">Key-Value Pairs:</span> The attention mechanism computes weighted combinations of values based on the relevance of keys, determining which information is emphasized.
        </li>
      </ul>
    </section>
    <!-- Attention Mechanism with Query and Key Vectors -->
<section id="attention-mechanism" class="mb-8">
  <h2 class="text-2xl font-bold mb-4 text-blue-900">Attention Mechanism: Query and Key Vectors</h2>
  <p class="text-gray-700 mb-4">
    In the attention mechanism, each word in a sequence is represented by three vectors: 
    <span class="font-semibold">Query</span>, <span class="font-semibold">Key</span>, and <span class="font-semibold">Value</span>.
    These vectors enable the model to calculate the relevance of each word to others in the sequence.
  </p>
  <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
    <div class="p-4 bg-blue-100 rounded-lg">
      <h3 class="font-semibold text-blue-900">Query Vector</h3>
      <p class="text-gray-700">
        The <span class="font-semibold">Query vector</span> is used to determine the focus word's perspective in understanding related information within the sequence. Each word's query helps identify relevant connections with other words.
      </p>
    </div>
    <div class="p-4 bg-blue-100 rounded-lg">
      <h3 class="font-semibold text-blue-900">Key Vector</h3>
      <p class="text-gray-700">
        The <span class="font-semibold">Key vector</span> acts as the reference for relevance in the sequence. When comparing query and key vectors, the model calculates the attention score, determining how much focus a word should have on another.
      </p>
    </div>
  </div>
   <!-- Mermaid Diagram -->
    <div class="bg-white p-4 rounded shadow">
      <div class="mermaid">
      flowchart LR
          Q[Query Vector] -->|"Calculates Attention Score"| K[Key Vector]
          Q -->|Focuses on Relevant Words| Attention["Attention Mechanism"]
          K -->|Links Query to Context| Attention

      </div>
    </div>
</section>

    <!-- Fine-Tuning Section -->
    <section id="fine-tuning">
      <h2 class="text-2xl font-bold mb-4 text-blue-900">Fine-Tuning</h2>
      <p class="text-gray-700">
        Fine-tuning is the process of adapting a pre-trained LLM to specific tasks, domains, or applications. This process enhances the modelâ€™s accuracy and relevance for specific use cases.
      </p>
      <ul class="list-disc ml-6 text-gray-700 space-y-2">
        <li>
          <span class="font-semibold">Task-Specific Fine-Tuning:</span> Fine-tuning a model for tasks like summarization, translation, or question answering.
        </li>
        <li>
          <span class="font-semibold">Domain-Specific Fine-Tuning:</span> Adapting models to specialized fields, like healthcare or finance, by training on domain-specific datasets.
        </li>
        <li>
          <span class="font-semibold">Data Quality:</span> Using high-quality and relevant data for fine-tuning is essential for optimal model performance.
        </li>
      </ul>
    </section>


     <!-- GPUs Section -->
    <section id="gpu">

    <h2 class="text-2xl font-bold mb-4 text-blue-900">GPUs in LLM Training</h2>
    <p class="text-gray-700 mb-4">
      Training large language models (LLMs) requires immense computational power. GPUs, with their parallel processing capabilities, are essential for handling the vast matrix operations and data involved in LLM training, significantly speeding up the process compared to CPUs.
    </p>
    </section>


    <!-- Usage Section -->
    <section id="usage">
      <h2 class="text-2xl font-bold mb-4 text-blue-900">Usage</h2>
      <p class="text-gray-700 mb-4">
        Using LLMs effectively involves setting up prompt-based interactions or training custom models. Here are some general usage scenarios:
      </p>
      <div class="space-y-4">
        <div class="p-4 bg-blue-100 rounded-lg">
          <h3 class="font-semibold text-blue-900">Text Generation</h3>
          <p class="text-gray-700">Generate coherent, contextually relevant text based on a given prompt.</p>
        </div>
        <div class="p-4 bg-blue-100 rounded-lg">
          <h3 class="font-semibold text-blue-900">Summarization</h3>
          <p class="text-gray-700">LLMs can produce concise summaries of long-form text, preserving key details.</p>
        </div>
        <div class="p-4 bg-blue-100 rounded-lg">
		          <h3 class="font-semibold text-blue-900">Translation</h3>
          <p class="text-gray-700">Transform text between languages while maintaining contextual meaning.</p>
        </div>
        <div class="p-4 bg-blue-100 rounded-lg">
          <h3 class="font-semibold text-blue-900">Sentiment Analysis</h3>
          <p class="text-gray-700">Detect and interpret the sentiment in text, useful for customer feedback and social media analysis.</p>
        </div>
      </div>
    </section>

    <!-- Tips Section -->
    <section id="tips">
      <h2 class="text-2xl font-bold mb-4 text-blue-900">Tips</h2>
      <ul class="list-disc ml-6 text-gray-700 space-y-2">
        <li>
          <span class="font-semibold">Experiment with Prompts:</span> LLMs are sensitive to prompts; slight adjustments can lead to significantly different outputs.
        </li>
        <li>
          <span class="font-semibold">Use Temperature and Top-P:</span> For text generation, control creativity by adjusting temperature (higher values for more randomness) and top-p (probability mass for sampling).
        </li>
        <li>
          <span class="font-semibold">Limit Token Usage:</span> Models have token limits; ensure input is concise to avoid truncation.
        </li>
        <li>
          <span class="font-semibold">Fine-tune for Specific Tasks:</span> For specialized applications, fine-tuning LLMs on task-specific data can yield more accurate results.
        </li>
      </ul>
    </section>

 
      <!-- References Section -->
    <section id='references'>
    <h2 class="text-2xl font-bold mb-4 text-blue-900">References</h2>

    <!-- YouTube Video Grid -->
    <div class="grid grid-cols-1 md:grid-cols-3 gap-4">
      <iframe class="w-full aspect-video rounded shadow" src="https://www.youtube.com/embed/zjkBMFhNj_g" title="YouTube video" allowfullscreen></iframe>
      <iframe class="w-full aspect-video rounded shadow" src="https://www.youtube.com/embed/wjZofJX0v4M" title="YouTube video" allowfullscreen></iframe>
      <iframe class="w-full aspect-video rounded shadow" src="https://www.youtube.com/embed/eMlx5fFNoYc" title="YouTube video" allowfullscreen></iframe>
      
      <iframe class="w-full aspect-video rounded shadow" src="https://www.youtube.com/embed/9-Jl0dxWQs8" title="YouTube video" allowfullscreen></iframe>
      <iframe class="w-full aspect-video rounded shadow" src="https://www.youtube.com/embed/UPtG_38Oq8o" title="YouTube video" allowfullscreen></iframe>
      <iframe class="w-full aspect-video rounded shadow" src="https://www.youtube.com/embed/h9Z4oGN89MU" title="YouTube video" allowfullscreen></iframe>
    </div>
  </section>

     <!-- Research papers Section -->
    <section id='papers'>
     <h2 class="text-2xl font-bold mb-4 text-blue-900">Research Papers</h2>
    <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
      <iframe src="https://arxiv.org/pdf/1706.03762" title="Attention Is All You Need" class="w-full h-96 rounded shadow"></iframe>
      <iframe src="https://arxiv.org/pdf/2005.14165" title="Language Models are Few-Shot Learners" class="w-full h-96 rounded shadow"></iframe>
      <iframe src="https://arxiv.org/pdf/1301.3781" title="Word2Vec" class="w-full h-96 rounded shadow"></iframe>
      <iframe src="https://arxiv.org/pdf/1810.04805" title="BERT" class="w-full h-96 rounded shadow"></iframe>
    </div>
  </section>



   <section id='books'>
    <!-- Books Section -->
    <h2 class="text-2xl font-bold mb-4 text-blue-900">Books and code and links</h2>
    <ul class="list-disc list-inside mb-6 text-gray-700">
      <li><a class="text-blue-500 hover:underline" href="https://www.3blue1brown.com/lessons/eigenvalues" target="_blank">3Blue1Brown: Eigenvalues</a></li>
      <li><a class="text-blue-500 hover:underline" href="https://udlbook.github.io/udlbook/" target="_blank">Universal Design for Learning Book</a></li>
      <li><a class="text-blue-500 hover:underline" href="https://www.deeplearningbook.org/contents/linear_algebra.html" target="_blank">Deep Learning Book - Linear Algebra</a></li>
      <li><a class="text-blue-500 hover:underline" href="https://vgel.me/posts/handmade-transformer/" target="_blank">Handmade Transformer</a></li>
      <li><a class="text-blue-500 hover:underline" href="https://e2eml.school/transformers.html" target="_blank">E2E ML: Transformers</a></li>
      <li><a class="text-blue-500 hover:underline" href="https://transformersbook.com/" target="_blank">Transformers Book</a></li>
      <li><a class="text-blue-500 hover:underline" href="https://github.com/nlp-with-transformers/notebooks" target="_blank">Transformers Notebooks (Code)</a></li>
      <li><a class="text-blue-500 hover:underline" href="https://plainenglish.io/blog/building-and-training-a-transformer-from-scratch" target="_blank">Building and Training a Transformer from Scratch</a></li>
    </ul>
  </section>




  </div>
    <!-- Footer -->
    <footer class="bg-blue-600 text-white py-6 mt-10">
        <div class="container mx-auto text-center">
           MCDocs v0.7
        </div>
    </footer>
</body>
</html>
